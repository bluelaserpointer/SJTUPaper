\chapter{头部姿态在多模态级联架构中的引入}

\section{问题定义}
\label{sec:problem}

在整体系统中，FaceCapGes模块承担着从多模态输入信号到上半身骨骼姿态预测的核心任务。
为了明确模型的输入输出结构与学习目标，本节对该问题进行形式化定义。

\subsection{任务描述}

目标是在实时条件下，根据用户当前时刻的语音、面部表情与头部姿态信息，预测其对应的上半身骨骼姿态。模型需能够逐帧生成与语音节奏、面部动态和头部转动方向相协调的自然手势动作，而不依赖未来的输入帧或整句语音信息。

形式上，可以将该任务定义为一个多模态时序映射函数：
\begin{equation}
\hat{\bm{v}}_{t}^{B} = f_\theta\!\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big),
\end{equation}
其中$f_\theta$表示由参数$\theta$控制的生成模型，$N$为历史窗口长度。
%
各模态输入定义如下：
$\bm{v}_{t}^{A}$表示语音模态在时刻$t$的特征向量，由麦克风信号经特征提取模块得到；
$\bm{v}_{t}^{F}$表示面部模态的输入，为ARKit输出的52维标准化BlendShape系数；
$\bm{v}_{t}^{H}$表示头部模态的输入，为ARKit得出的头部旋转矩阵经Rot6D表示；
而$\hat{\bm{v}}_{t}^{B}$为生成模型在当前时刻预测的上半身骨骼姿态向量。
%
模型仅利用当前及过去$N$帧的输入信息估计$\hat{\bm{v}}_{t}^{B}$，
从而满足严格的实时推理约束。

\subsection{输入与输出模态}

FaceCapGes模型的输入由三种可同时实时获取的模态组成：语音特征、面部BlendShape权重及头部姿态参数；输出为当前帧的上半身骨骼旋转状态。各模态的符号与维度如表~\ref{tab:modalities}所示。

\begin{table}[h]
\bicaption{输入输出模态符号与维度}{Notations and Dimensions of Input/Output Modalities}
\centering
\label{tab:modalities}
\begin{tabular}{llll}
\toprule
\textbf{模态} & \textbf{符号} & \textbf{维度} & \textbf{描述} \\
\midrule
语音特征 & $\bm{v}_t^{A}$ & $\mathbb{R}^{1067}$ & 由音频信号提取的时序特征（Mel频谱、短时能量、基频等） \\
面部 BlendShape & $\bm{v}_t^{F}$ & $\mathbb{R}^{52}$ & ARKit输出的标准化表情权重向量 \\
头部姿态 & $\bm{v}_t^{H}$ & $\mathbb{R}^{6}$ & 采用Rot6D表示的头部旋转参数 \\
骨骼姿态（输出） & $\hat{\bm{v}}_t^{B}$ & $\mathbb{R}^{6 \times 47}$ & 上半身47个关节的旋转状态 \\
\bottomrule
\end{tabular}
\end{table}

输入序列$\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big)$描述了用户在过去$N$帧内的语音与表情动态信息。模型通过学习其时序变化规律，逐帧生成对应的骨骼姿态输出$\hat{\bm{v}}_t^{B}$。在推理阶段，模型仅访问至时刻$t$的输入序列，无法访问任何未来帧信息，保证了生成过程的因果性与实时性。

\subsection{学习目标与优化形式}
\label{sec:loss}
在训练阶段，给定来自多模态语音动作数据集（如 BEAT）的配对样本：
\begin{equation}
\big(\bm{v}_t^{A},\, \bm{v}_t^{F},\, \bm{v}_t^{H},\, \bm{v}_t^{B}\big),
\end{equation}
模型的学习目标是在不依赖未来帧的条件下，
最小化预测姿态 $\hat{\bm{v}}_t^{B}$ 与真实姿态 $\bm{v}_t^{B}$ 之间的差异，
从而生成自然、流畅且与语音节奏相匹配的上半身动作序列。

假设模型在每个时间窗口中输出连续的 $M$ 帧预测结果，
得到预测动作序列
$\hat{\bm{g}} \in \mathbb{R}^{M \times 6 \times 47}$。
综合考虑空间重构精度、时序平滑性以及动作分布一致性，
总体优化目标定义为：
\begin{equation}
\mathcal{L}_{total} =
\lambda_r  \mathcal{L}_{rec}
+ \lambda_v \mathcal{L}_{vel}
+ \lambda_a \mathcal{L}_{acc}
+ \lambda_{adv} \mathcal{L}_{adv}.
\label{eq:loss_total}
\end{equation}

其中 $\mathcal{L}_{rec}$ 衡量单帧姿态重构误差，
$\mathcal{L}_{vel}$ 与 $\mathcal{L}_{acc}$ 分别约束速度与加速度的连续性,
$\mathcal{L}_{adv}$ 表示对抗训练损失，将在第~\ref{sec:adv}中介绍。
该组合设计在自回归预测过程中能够有效缓解抖动与速度漂移问题。

\paragraph{姿态重构与时序平滑损失}
为同时保证空间重构精度与时间连续性，
我们采用基于 Huber 误差的重构损失形式，
并分别作用于姿态、速度与加速度信号。

给定任意预测序列 $\hat{\bm{x}}$ 及其对应的真实序列 $\bm{x}$，
基础误差项定义为：
\begin{equation}
\mathcal{L}_{Huber}(\bm{x}, \hat{\bm{x}})
=
\beta \cdot
\mathrm{SmoothL1}
\left(
\frac{\bm{x}}{\beta},
\frac{\hat{\bm{x}}}{\beta}
\right),
\end{equation}
其中 $\mathrm{SmoothL1}(\cdot)$ 表示平滑L1误差，$\beta$为平滑系数，本文中设为 $0.1$。

在此基础上，
姿态、速度与加速度重构损失分别定义为：
\begin{align}
\mathcal{L}_{rec} &= \mathcal{L}_{Huber}(\bm{g}, \hat{\bm{g}}), \\
\mathcal{L}_{vel} &= \mathcal{L}_{Huber}(\bm{g}', \hat{\bm{g}}'), \\
\mathcal{L}_{acc} &= \mathcal{L}_{Huber}(\bm{g}'', \hat{\bm{g}}''),
\end{align}
其中一阶与二阶时间差分 $\bm{g}'$、$\bm{g}''$ 定义为：
\begin{equation}
\bm{g}'_t = \bm{g}_t - \bm{g}_{t-1}, \quad
\bm{g}''_t = \bm{g}'_t - \bm{g}'_{t-1}.
\end{equation}

该多尺度重构约束在自回归预测过程中
能够有效缓解高频抖动与速度漂移问题，
在保证运动学精度的同时提升生成序列的时间稳定性。

\paragraph{对抗损失}
\label{sec:adv}
为进一步提升生成动作的自然度，
引入基于判别器的对抗损失：
\begin{equation}
\mathcal{L}_{adv} = -\mathbb{E}[\log(Dis(\hat{\bm{g}}))],
\end{equation}
其中判别器 $Dis$ 以完整动作序列为输入，
判断其是否来自真实数据分布。
该损失从整体动力学分布层面约束生成结果，
促进生成动作在节奏、加速度与能量变化等统计特性上
与真实表演者保持一致。
训练过程中通过交替优化生成器与判别器参数以维持稳定性。

各损失项的权重系数在实验中设定为
$\lambda_r = 5\times10^{2}$，
$\lambda_v = 10^{3}$，
$\lambda_a = 10^{3}$，
$\lambda_{adv} = 10^{-1}$。

\section{级联架构设计的继承}
\label{sec:cascade}

\subsection{级联架构的原理与理论背景}

现有语音驱动手势生成模型多采用多模态融合结构，其中以CaMN\cite{beatcamn}为代表的级联架构在设计理念上具有代表性。其核心思想是将语音、面部表情与身体动作视为语义表达的不同层级：语音模态承担语义与节奏驱动作用，面部模态反映情感与意图，身体动作则是语言与情绪的外化呈现。CaMN 采用自上而下的处理顺序，即依次对语音、面部和动作模态进行建模，从而以层次化结构保持模态间的语义依存关系。

这种设计符合人类交流中“语言、表情、动作”一体化的认知规律\cite{mcneill_1992_hand, kendon_2004_gesture}。语音先规划语义与节奏，面部表情作为情绪强化信号随后产生，最终通过身体动作完成完整的非语言表达。模型中，语音编码器输出的时间嵌入被输入至面部编码器，再与面部特征融合后驱动动作解码器，从而保持语义一致性并增强表现力。

然而，CaMN的原始设计面向离线整句生成任务，需要访问未来上下文以维持全局连贯性。在实时场景下，这种依赖将引入显著延迟并破坏因果性。FaceCapGes在继承其层次思想的同时，对输入模式、训练方式与模态选择进行了系统性重构，以满足帧级实时约束。

\subsection{说话人ID分支移除}
如图~\ref{fig:system_architecture}所示，用户配置层会设置说话人ID配置用于模型切换，但该模态在本模型中不属于网络输入。
在基线模型 CaMN 中，输入模态包含显式的说话人ID向量，用于在同一模型内区分不同演讲者的风格差异。
然而在实时交互场景下，该分支并非必要：用户身份通常固定，且说话风格的变化频率远低于帧级推理速度。
因此，FaceCapGes移除了ID输入分支，采用针对每个说话人独立训练模型参数的方法。
实验表明，该方式能在保持收敛稳定的同时提升动作的自然性与节奏一致性。
从系统使用角度看，不同模型可视为说话风格配置，用户仅在需要时切换对应参数，该操作发生频率低，不会影响实时推理性能。

\subsection{输入模态继承}
语音特征通过时间卷积网络（Temporal Convolutional Network, TCN）和多层感知机（Multilayer Perceptron, MLP）编码，以捕捉短时节奏模式；面部模态采用相似结构，并在中间层融合语音嵌入，从而增强语音与表情之间的语义关联。  
语音编码器$E_A$与面部编码器$E_F$的输出定义为：
\begin{equation}
\bm{z}^{A}_t = E_A(\bm{v}_{t-N:t}^{A}), \quad
\bm{z}^{F}_t = E_F(\bm{v}_{t-N:t}^{F}; \bm{z}^{A}_t)
\end{equation}
其中$\bm{z}^{A}_t \in \mathbb{R}^{128}$，$\bm{z}^{F}_t  \in \mathbb{R}^{32}$。
这两个编码器负责提取低层次语音节奏与表情动态信息，为后续模态融合提供稳定上下文表征。

此外，系统在此基础上引入头部姿态模态$\bm{v}_t^{H}$，用于补充空间方向与节奏信号。
其编码器$E_H$将Rot6D表示的头部旋转向量映射为紧凑潜在表征：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}_{t-N:t}^{H}),
\end{equation}
编码器结构将在第~\ref{sec:head_encoder}~节详细说明。

\subsection{输出模态继承（身体姿态解码）}

在输入模态经过编码与融合后，模型需将多模态特征映射至对应的身体姿态空间。
为实现层次化的动作生成与结构协调，本文将上半身的输出区域划分为两个互补分支：
躯干（Torso, T）与上肢（Upper limbs, U）。
躯干部分包含脊椎的三个主要控制关节，用于确定身体的姿态基准与运动节奏；
上肢部分包含双臂及手部关节，负责生成与语音节奏及情绪表达相呼应的细节动作。
最终的上半身姿态表示为两者的组合：
\begin{equation}
\bm{v}^B = \bm{v}^T \otimes \bm{v}^U,
\end{equation}
其中$\otimes$表示通道维度拼接操作。

该分层设计继承了CaMN的层次预测思路：
模型首先生成相对稳定的躯干姿态以确定整体方向，
再以此为条件预测上肢动作，从而在实时生成中保持整体协调性与自然度。

具体而言，来自语音、面部与头部编码器的特征
$\bm{z}_t^{A}$、$\bm{z}_t^{F}$、$\bm{z}_t^{H}$会与历史姿态序列$(\bm{v}_{t-N}^{B}, \ldots, \bm{v}_t^{B})$拼接，组成多模态隐向量：
\begin{equation}
\bm{z}_t^{fuse} = \bm{z}_t^{A} \otimes \bm{z}_t^{F} \otimes \bm{z}_t^{H} \otimes (\bm{v}_{t-N}^{B}, \ldots, \bm{v}_t^{B}),
\end{equation}
其中$\otimes$表示通道维度拼接操作，时间末帧采用零填充以对齐维度。

随后，$\{\bm{z}_0^{fuse}, \ldots, \bm{z}_N^{fuse}\}$经两个单向 LSTM 解码器，分别生成躯干与上肢的潜在特征：
\begin{equation}
\bm{z}^{T} = \mathrm{LSTM}_{T}(\bm{z}_0^{fuse}, \ldots, \bm{z}_N^{fuse}), \quad
\bm{z}^{U} = \mathrm{LSTM}_{U}(\bm{z}_0^{fuse}, \ldots, \bm{z}_N^{fuse}),
\end{equation}
并通过独立的 MLP 模块还原为旋转参数：
\begin{equation}
\hat{\bm{v}}^{T} = \mathrm{MLP}_{T}(\bm{z}^{T}), \quad
\hat{\bm{v}}^{U} = \mathrm{MLP}_{U}(\bm{z}^{U}).
\end{equation}
最终拼接得到当前帧的完整上半身姿态：
\begin{equation}
\hat{\bm{v}}^{B} = \hat{\bm{v}}^{T} \otimes \hat{\bm{v}}^{U}.
\end{equation}

在推理阶段，解码器隐状态在时间步之间保持连续，
与前述输入模态特征配合，使模型在保持因果性的同时具备自然的时间平滑性。
由于该部分结构沿用自基线模型，本文不再赘述。

\section{头部姿态模态的引入}
\label{sec:head_encoder}

\subsection{输入特征与表示}
本文仅使用头部旋转信息作为输入特征，不引入头部平移位移。
在 BEAT 数据集中，演讲者多为站姿录制，过程中存在一定幅度的身体移动与位移变化；
而目标应用场景下的用户运动模式可能不同，例如在坐姿交互中身体位移通常较小。
由于平移分量更易受到拍摄坐标系、相机距离与场景布置等因素影响，
直接建模头部位移可能引入额外的场景依赖性，从而削弱跨场景泛化能力。
因此，本文仅采用头部旋转作为输入，并使用 Rot6D~\cite{rot6d} 进行表示，
以获得连续且无奇异性的旋转特征表达。

\subsection{特征获取方法}
如第~\ref{sec:system} 所述，头部姿态可与面部表情共同由面部捕捉工具（如 ARKit）实时提取。
对于 BEAT 数据集，其原始数据中未单独提供头部姿态信号。
因此，我们在训练预处理中利用骨架层级关系，
沿骨架链路复合从根节点到头部关节的相对旋转，
从而得到头部在全局坐标系下的绝对旋转表示，并进一步转换为 Rot6D 形式作为模型输入。

\subsection{级联结构中的位置}
在模型结构设计中，我们考察了头部姿态特征与其他模态的多种组合方式。%
具体而言，分别尝试了：%
(1) 将头部姿态特征在编码阶段与语音或面部特征进行早期融合；%
(2) 在解码阶段以前两者的嵌入结果为条件，预测头部姿态特征作为辅助信号。%
实验结果显示，这两种交互方式均未带来显著性能提升，%
部分设置甚至出现训练收敛速度下降或动作节奏轻微错位的情况。%

这一现象与认知层面的规律相符。%
头部动作虽然与语音韵律在时间上存在同步性，%
但在认知层面并非由语音或表情直接驱动，%
也难以反向推导这些模态的动态变化。%
换言之，三者更可能属于并行协同关系，%
共享节奏与注意机制，但不构成单向的预测链。%

基于此观察，本文在最终架构中采用弱耦合的后级输入设计：%
头部姿态特征在语音与面部特征编码完成后，%
以独立通道的形式拼接至多模态隐向量 $\bm{z}^{fuse}_t$，%
而非在编码阶段进行显式交互。%
该处理方式在保持整体结构简洁性的同时，%
仍保留头部姿态在方向、节奏及注意焦点方面的补充作用。%

实验表明，%
在此配置下模型的整体自然度与时序稳定性得到改善，%
说明头部姿态虽非语音或表情的从属模态，%
但作为空间与节奏的辅助信号仍具有积极贡献。

\paragraph{编码器结构}
图~\ref{fig_headencoder} 所示为头部姿态编码器结构。该编码器由两层前馈网络组成，输入为Rot6D表示的6维向量：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}^H_{t-N:t}),
\end{equation}
其中 $E_H$ 的具体形式为：
\begin{align}
\bm{h}_1 &= \mathrm{ReLU}(W_1 \bm{v}^H_t + b_1), \\
\bm{z}^{H}_t &= W_2 \bm{h}_1 + b_2,
\end{align}
网络维度设置为：输入 $6$，中间层 $36$，输出 $12$。  
在特征层面，其输出与语音、面部嵌入拼接后输入解码器，形成从语义到反应的多层信号流。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.8\linewidth]{figures/Fig_headencoder.png}
\bicaption{头部姿态编码器结构示意图}{Architecture of the Head Pose Encoder}
\label{fig_headencoder}
\end{figure}

\section{模型整体结构}
\label{sec:architecture}
图~\ref{fig_architecture}展示了FaceCapGes从音频、面部、头部编码器分别提取模态特征后拼接，输入至 LSTM 解码器生成躯干与手部动作的过程。
其中，LSTM的输出仅保留最后一帧作为当前时刻预测，符合帧级实时推理设定。
实际训练中，训练阶段历史姿态序列比目标长度少一帧，需进行零填充进行对齐。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/Fig_NeuroArchitecture.png}
\bicaption{FaceCapGes 模型整体结构}{Overall Architecture of FaceCapGes}
\label{fig_architecture}
\end{figure*}

\section{本章小结}

本章围绕 FaceCapGes 的模型结构与训练策略，系统介绍了面向实时交互场景的语音驱动手势生成方法设计。
首先，本文在任务设定上明确了严格的因果约束与逐帧在线生成要求，
区别于依赖未来上下文的一次性离线生成方法，
为后续模型结构与训练策略的选择奠定了基础。

在模型设计方面，本文基于 CaMN 的级联框架，引入语音、面部表情与头部姿态三种模态的协同建模方式，
并详细说明了各模态在网络中的编码位置与作用。
其中，本文将头部姿态作为独立输入模态系统性地引入实时手势生成任务，
仅使用旋转信息，以提升跨场景泛化能力与时序连续性。
同时，针对不同数据来源，给出了从面捕系统与骨架数据中获取头部姿态特征的统一处理方法。

在时序建模与训练策略上，
本文采用滑动窗口的帧级自回归训练方式，
使模型在训练阶段即暴露于自身历史预测分布，
从而保证训练目标与在线推理过程的一致性。
该设计在不依赖未来信息的前提下，
增强了生成动作在时间维度上的连贯性与稳定性，
为实时交互场景中的持续动作生成提供了有效支持。

通过上述模型结构与训练策略的设计，
FaceCapGes 在满足低延迟与因果约束的同时，
为后续章节中的主观评估、定量分析与性能测试提供了清晰的方法基础。
\chapter{多模态在线实时数字人驱动模型架构}
\label{chap:model_architecture}

CaMN\cite{beatcamn}提供了语音驱动手势生成的基础级联框架，为实时数字人动作建模奠定了有效的结构基础。
然而，原始 CaMN 主要面向离线序列建模，其训练与推理过程默认可访问完整的时间上下文，难以直接满足在线实时场景下严格因果与逐帧生成的需求。
为此，本文在保留级联解码思想的同时，对输入模态组织方式与时序建模进行了适配，使模型能够在仅依赖当前与历史信息的条件下进行生成。
此外，为进一步增强空间方向感与节奏一致性，本文在语音与面部捕捉之外引入头部姿态模态作为额外输入，并设计对应编码器以提供补充表征。
本章将介绍上述多模态编码、融合以及身体姿态解码的结构。

\section{问题定义}
\label{sec:problem}

在整体系统中，FaceCapGes模块承担着从多模态输入信号到上半身骨骼姿态预测的核心任务。
为了明确模型的输入输出结构与学习目标，本节对该问题进行形式化定义。

\subsection{任务描述}

目标是在实时条件下，根据用户当前时刻的语音、面部表情与头部姿态信息，预测其对应的上半身骨骼姿态。模型需能够逐帧生成与语音节奏、面部动态和头部转动方向相协调的自然手势动作，而不依赖未来的输入帧或整句语音信息。

形式上，可以将该任务定义为一个多模态时序映射函数：
\begin{equation}
\hat{\bm{v}}_{t}^{B} = f_\theta\!\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big),
\end{equation}
其中$f_\theta$表示由参数$\theta$控制的生成模型，$N$为历史窗口长度。
%
各模态输入定义如下：
$\bm{v}_{t}^{A}$表示语音模态在时刻$t$的特征向量，由麦克风信号经特征提取模块得到；
$\bm{v}_{t}^{F}$表示面部模态的输入，为ARKit输出的52维标准化BlendShape系数；
$\bm{v}_{t}^{H}$表示头部模态的输入，为ARKit得出的头部旋转矩阵经Rot6D表示；
而$\hat{\bm{v}}_{t}^{B}$为生成模型在当前时刻预测的上半身骨骼姿态向量。
%
模型仅利用当前及过去$N$帧的输入信息估计$\hat{\bm{v}}_{t}^{B}$，
从而满足严格的实时推理约束。

\subsection{输入与输出模态}

FaceCapGes模型的输入由三种可同时实时获取的模态组成：语音特征、面部BlendShape权重及头部姿态参数；输出为当前帧的上半身骨骼旋转状态。各模态的符号与维度如表~\ref{tab:modalities}所示。

\begin{table}[h]
\bicaption{输入输出模态符号与维度}{Notations and Dimensions of Input/Output Modalities}
\centering
\label{tab:modalities}
\begin{tabular}{llll}
\toprule
\textbf{模态} & \textbf{符号} & \textbf{维度} & \textbf{描述} \\
\midrule
语音特征 & $\bm{v}_t^{A}$ & $\mathbb{R}^{1067}$ & 由音频信号提取的时序特征（Mel频谱、短时能量、基频等） \\
面部 BlendShape & $\bm{v}_t^{F}$ & $\mathbb{R}^{52}$ & ARKit输出的标准化表情权重向量 \\
头部姿态 & $\bm{v}_t^{H}$ & $\mathbb{R}^{6}$ & 采用Rot6D表示的头部旋转参数 \\
骨骼姿态（输出） & $\hat{\bm{v}}_t^{B}$ & $\mathbb{R}^{6 \times 47}$ & 上半身47个关节的旋转状态 \\
\bottomrule
\end{tabular}
\end{table}

输入序列$\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big)$描述了用户在过去$N$帧内的语音与表情动态信息。模型通过学习其时序变化规律，逐帧生成对应的骨骼姿态输出$\hat{\bm{v}}_t^{B}$。在推理阶段，模型仅访问至时刻$t$的输入序列，无法访问任何未来帧信息，保证了生成过程的因果性与实时性。

\section{级联架构设计的继承}
\label{sec:cascade}

\subsection{级联架构的原理与理论背景}

现有语音驱动手势生成模型多采用多模态融合结构，其中以CaMN\cite{beatcamn}为代表的级联架构在设计理念上具有代表性。其核心思想是将语音、面部表情与身体动作视为语义表达的不同层级：语音模态承担语义与节奏驱动作用，面部模态反映情感与意图，身体动作则是语言与情绪的外化呈现。CaMN 采用自上而下的处理顺序，即依次对语音、面部和动作模态进行建模，从而以层次化结构保持模态间的语义依存关系。

这种设计符合人类交流中“语言、表情、动作”一体化的认知规律\cite{mcneill_1992_hand, kendon_2004_gesture}。语音先规划语义与节奏，面部表情作为情绪强化信号随后产生，最终通过身体动作完成完整的非语言表达。模型中，语音编码器输出的时间嵌入被输入至面部编码器，再与面部特征融合后驱动动作解码器，从而保持语义一致性并增强表现力。

然而，CaMN的原始设计面向离线整句生成任务，需要访问未来上下文以维持全局连贯性。在实时场景下，这种依赖将引入显著延迟并破坏因果性。FaceCapGes在继承其层次思想的同时，对输入模式、训练方式与模态选择进行了系统性重构，以满足帧级实时约束。

\subsection{说话人ID分支移除}
如图~\ref{fig:system_architecture}所示，用户配置层会设置说话人ID配置用于模型切换，但该模态在本模型中不属于网络输入。
在基线模型 CaMN 中，输入模态包含显式的说话人ID向量，用于在同一模型内区分不同演讲者的风格差异。
然而在实时交互场景下，该分支并非必要：用户身份通常固定，且说话风格的变化频率远低于帧级推理速度。
因此，FaceCapGes移除了ID输入分支，采用针对每个说话人独立训练模型参数的方法。
实验表明，该方式能在保持收敛稳定的同时提升动作的自然性与节奏一致性。
从系统使用角度看，不同模型可视为说话风格配置，用户仅在需要时切换对应参数，该操作发生频率低，不会影响实时推理性能。

\subsection{输入模态的解码}
语音特征通过时间卷积网络（Temporal Convolutional Network, TCN）和多层感知机（Multilayer Perceptron, MLP）编码，以捕捉短时节奏模式；面部模态采用相似结构，并在中间层融合语音嵌入，从而增强语音与表情之间的语义关联。两者都延续 CaMN\cite{beatcamn}的结构设计。

具体而言，语音编码器 $E_A$，采用 $12$ 层TCN建模局部时间依赖，并通过跳跃连接（skip connection）增强深层特征的传递稳定性；在第 $12$ 层提取的语音时序特征后接入两层 MLP，用于进一步特征精炼与维度压缩，最终输出语音潜在表示 $\bm{z}^{A}_t \in \mathbb{R}^{128}$。
面部编码器 $E_F$ 采用较浅的 $8$ 层 TCN 结构以捕捉面部表情的短时动态变化，并在第 $8$ 层将语音嵌入与面部特征进行通道维拼接融合，以增强面部表情变化与语音节奏之间的对应关系；编码器末端同样使用两层 MLP 进行特征映射与输出压缩，最终得到面部潜在表示 $\bm{z}^{F}_t \in \mathbb{R}^{32}$。

语音编码器 $E_A$ 与面部编码器 $E_F$ 的输出定义为：
\begin{equation}
\bm{z}^{A}_t = E_A(\bm{v}_{t-N:t}^{A}), \quad
\bm{z}^{F}_t = E_F(\bm{v}_{t-N:t}^{F}; \bm{z}^{A}_t).
\end{equation}
这两个编码器负责提取低层次语音节奏与表情动态信息，为后续模态融合提供稳定上下文表征。

此外，系统在此基础上引入头部姿态模态$\bm{v}_t^{H}$，用于补充空间方向与节奏信号。
其编码器$E_H$将Rot6D表示的头部旋转向量映射为紧凑潜在表征：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}_{t-N:t}^{H}),
\end{equation}
编码器结构将在第~\ref{sec:head_encoder}~节详细说明。

\subsection{身体姿态的解码}
\label{subsec:output_modality_decoding}

在输入模态经过编码与融合后，模型需将多模态特征映射至对应的身体姿态空间。
为实现层次化的动作生成与结构协调，本文将上半身的输出区域划分为两个互补分支：
躯干（Torso, T）与上肢（Upper limbs, U）。
躯干部分包含脊椎的三个主要控制关节，用于确定身体的姿态基准与运动节奏；
上肢部分包含双臂及手部关节，负责生成与语音节奏及情绪表达相呼应的细节动作。
最终的上半身姿态表示为两者的组合：
\begin{equation}
\bm{v}_t^{B} = \bm{v}_t^{T} \otimes \bm{v}_t^{U},
\end{equation}
其中$\otimes$表示通道维度拼接操作。

该分层设计继承了CaMN的层次预测思路：
模型首先生成相对稳定的躯干姿态以确定整体方向，
再以此为条件预测上肢动作，从而在实时生成中保持整体协调性与自然度。

\paragraph{融合输入的序列化表示}
在时刻$t$，来自语音、面部与头部编码器的特征
$\bm{z}_t^{A}$、$\bm{z}_t^{F}$、$\bm{z}_t^{H}$
与历史动作上下文共同构成LSTM解码器的输入。
为保持严格因果性，我们显式提供最近$N$帧的历史上半身姿态$\bm{v}_{t-N:t-1}^{B}$作为条件信息，并对当前时刻的动作输入使用占位符进行对齐。
具体而言，定义历史动作对齐序列$\bm{s}_\tau$为：
\begin{equation}
\bm{s}_\tau =
\begin{cases}
\bm{v}_\tau^{B}, & \tau \le t-1, \\
\mathbf{0}, & \tau = t,
\end{cases}
\quad \tau \in \{t-N,\ldots,t\},
\label{eq:history_padding}
\end{equation}
则窗口内每一帧的融合输入向量可写为：
\begin{equation}
\bm{z}_\tau^{fuse} = \bm{z}_\tau^{A} \otimes \bm{z}_\tau^{F} \otimes \bm{z}_\tau^{H} \otimes \bm{s}_\tau,
\quad \tau \in \{t-N,\ldots,t\},
\label{eq:fuse_per_frame}
\end{equation}
并将其按时间维堆叠得到长度为$N{+}1$的因果上下文输入序列：
\begin{equation}
\bm{Z}_t^{fuse} = (\bm{z}_{t-N}^{fuse}, \ldots, \bm{z}_{t}^{fuse}).
\label{eq:fuse_sequence}
\end{equation}

\paragraph{躯干与上肢的级联解码}
将输入序列$\bm{Z}_t^{fuse}$分别送入躯干与上肢两路单向LSTM解码器，得到窗口内的输出序列：
\begin{equation}
\bm{O}_t^{T} = \mathrm{LSTM}_{T}(\bm{Z}_t^{fuse}), \quad
\bm{O}_t^{U} = \mathrm{LSTM}_{U}(\bm{Z}_t^{fuse}),
\label{eq:lstm_outputs_seq}
\end{equation}
其中$\bm{O}_t^{T} = (\bm{o}_{t-N}^{T},\ldots,\bm{o}_{t}^{T})$，
$\bm{O}_t^{U} = (\bm{o}_{t-N}^{U},\ldots,\bm{o}_{t}^{U})$。
由于本文在时刻$t$的目标是预测当前帧动作，我们仅取序列末端输出作为当前帧的潜在表征：
\begin{equation}
\bm{z}_t^{T} = \bm{o}_t^{T}, \quad
\bm{z}_t^{U} = \bm{o}_t^{U}.
\label{eq:last_step_repr}
\end{equation}
最后，通过两路独立的MLP模块将潜在表征还原为旋转参数：
\begin{equation}
\hat{\bm{v}}_t^{T} = \mathrm{MLP}_{T}(\bm{z}_t^{T}), \quad
\hat{\bm{v}}_t^{U} = \mathrm{MLP}_{U}(\bm{z}_t^{U}),
\label{eq:decode_tu}
\end{equation}
并拼接得到当前帧的完整上半身动作预测：
\begin{equation}
\hat{\bm{v}}_t^{B} = \hat{\bm{v}}_t^{T} \otimes \hat{\bm{v}}_t^{U}.
\label{eq:decode_body}
\end{equation}

上述内容描述了模型在一个因果上下文输入序列内完成对当前帧动作的解码过程。

需要说明的是，LSTM 在实现中维护隐藏状态与记忆单元状态以编码时间依赖，其形式化表达见第\ref{sec:one_step_lstm}节。

\section{头部姿态模态的引入}
\label{sec:head_encoder}

\subsection{输入特征的定义}
本文仅使用头部旋转信息作为输入特征，不引入头部位置或位移。

这是因为，头部位置相对旋转易受到身体姿态的变化。
以BEAT\cite{beatcamn}为例的演讲数据集中，演讲者多为站姿录制，单次演讲时长偏长（约1分钟），有轻微的重心移动等站姿调整；
而目标应用场景下的用户交互姿态有可能包含坐姿，此时的身体活动方式与站姿存在差异，例如不存在长时间的站姿带来的重心移动。
由此可见，直接建模头部位置或位移可能引入额外的场景依赖性，从而削弱跨场景泛化能力。
因此，本文仅采用头部旋转作为输入。

\subsection{特征获取方法}
如第~\ref{sec:system}节所述，头部姿态在推理阶段可从面部捕捉工具（如 ARKit\cite{ARKitDocumentation}）实时提取。
但在训练阶段中，演讲数据集通常不单独提供头部姿态信号。
此时，我们可以在预处理中利用骨架层级关系，
沿骨架链路从根节点到头部关节的相对旋转进行旋转姿态的叠加，
从而得到头部在全局坐标系下的绝对旋转表示。

\subsection{级联架构中的位置}
在级联架构设计中，我们考察了头部姿态特征与其他模态的多种组合方式。%
具体而言，分别尝试了：%
(1) 将头部姿态特征在编码阶段与语音或面部特征进行早期融合；%
(2) 在解码阶段以前两者的嵌入结果为条件，预测头部姿态特征作为辅助信号。%
实验结果显示，这两种交互方式均未带来显著性能提升，%
观察到了训练收敛速度的下降。%

本文推测，头部动作虽然与语音韵律在时间上存在同步性，%
但头部动作与语音或表情之间的驱动关系不强，%
因为头部动作可以包含演讲人自然面向不同方向的听众等，与语音韵律或情感难以找到关联性的信息。%

基于此观察，本文在最终架构中采用了弱耦合的设计：%
头部姿态特征在语音与面部特征编码完成后，%
以独立通道的形式拼接至多模态隐向量 $\bm{z}^{fuse}_t$，%
同时头部姿态特征不参与其它输入模态的编码过程。%
我们在此配置下得到了更快的训练收敛。

\paragraph{编码器结构}
图~\ref{fig_headencoder} 所示为头部姿态编码器结构。该编码器由两层前馈网络组成，输入为Rot6D表示的6维向量：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}^H_{t-N:t}),
\end{equation}
其中 $E_H$ 的具体形式为：
\begin{align}
\bm{h}_1 &= \mathrm{ReLU}(W_1 \bm{v}^H_t + b_1), \\
\bm{z}^{H}_t &= W_2 \bm{h}_1 + b_2,
\end{align}
网络维度设置为：输入 $6$，中间层 $36$，输出 $12$。  
在特征层面，其输出与语音、面部嵌入拼接后输入解码器，形成从语义到反应的多层信号流。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.8\linewidth]{figures/Fig_headencoder.png}
\bicaption{头部姿态编码器结构示意图}{Architecture of the Head Pose Encoder}
\label{fig_headencoder}
\end{figure}

\section{模型整体结构}
\label{sec:architecture}
图~\ref{fig_architecture}展示了FaceCapGes从音频、面部、头部编码器分别提取模态特征后拼接，输入至 LSTM 解码器生成躯干与手部动作的过程。
其中，训练阶段历史姿态序列比目标长度少一帧，需进行零填充进行对齐，如式~\eqref{eq:fuse_per_frame}所示。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/Fig_NeuroArchitecture.png}
\bicaption{FaceCapGes 模型整体结构}{Overall Architecture of FaceCapGes}
\label{fig_architecture}
\end{figure*}

\section{本章小结}
\label{sec:chapter3_summary}

本章围绕 FaceCapGes 的模型结构设计，系统介绍了面向严格实时交互场景的多模态级联架构与关键输入输出模态设定。
首先，本文在任务设定上明确了逐帧在线生成与严格因果约束：模型在任意时刻仅利用当前及过去帧的语音、面部与头部信息进行动作预测，从而区别于依赖未来上下文的一次性离线生成方法，并为后续结构设计提供了统一前提。

在模型结构方面，本文继承 CaMN 的级联框架，将语音、面部表情与身体动作视为具有层次关联的表达信号，并进一步系统性地引入头部姿态作为独立输入模态，以补充空间方向与节奏信息。
同时，本文仅使用头部旋转信息进行建模，从而提升跨场景泛化能力，并给出了分别适用于训练与推理阶段的特征获取方式。

在解码端，本章说明了面向上半身骨骼姿态的层次化输出设计：将动作划分为躯干与上肢两路分支，并通过级联解码保持整体协调性与自然度。
上述模型结构为后续章节提出的滑动窗口自回归展开策略与片段级优化目标提供了清晰的结构基础。
下一章将进一步介绍模型在训练与推理阶段的统一展开过程，并证明该策略在严格因果约束下能够实现稳定的实时动作生成。

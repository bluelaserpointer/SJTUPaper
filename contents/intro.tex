% !TEX root = ../main.tex

\chapter{引言}

\section{研究背景和意义}

近年来，随着元宇宙、虚拟社交与直播等领域的相关技术日趋成熟，用户已能够使用任意外观的虚拟人作为交互载体，在虚拟空间中与异地用户进行交流。虚拟人 3D 模型的姿态由其内部骨架关节的旋转参数（如欧拉角、四元数等）定义，最终通过蒙皮渲染技术完成可视化。得益于自动骨骼绑定技术，骨骼动画的生成可消除不同 3D 模型间的骨架拓扑差异，实现跨模型复用。

在虚拟人交互中，穿戴式动作捕捉设备是实时驱动手势的传统方案，其可将用户肢体动作实时转换为骨骼动画，提供直观且沉浸的操控体验。尽管其精度较高，但对于大多数用户而言，此类设备存在功能用途单一、硬件成本昂贵、便携性差等问题，限制了使用频率。因此，在当前的元宇宙社交中，仅少数专业用户会使用此类设备，多数普通用户无法在交互中使用手势，导致了用户体验的不一致。

针对普通用户对低门槛虚拟人交互的需求，基于相机的动作捕捉技术\cite{mediapipefacemesh,AppleARKitTrackingGuide}成为主流替代方案。该技术无需额外硬件，仅通过手机、电脑的内置相机（部分依赖深度相机）即可实时捕捉用户动作。但该方案仍存在两种局限：一是需用户面向相机主动做出手势，导致交互过程中难以同步操作键盘、鼠标，且受相机视野范围限制；二是持续手势动作会产生体力消耗，在直播等长时间场景中，用户疲劳问题尤为突出。”

因此，我们提出一种新的需求：用户无需实际执行手势，而是由计算机结合其实时语音、面部表情与头部姿态，自动生成与之语义和情感相匹配的手势动画。该方法旨在降低使用门槛，并解决操作冲突与体力消耗问题。

然而，现有研究尚未提供成熟的解决方案来满足这一需求。当前多数手势生成方法依赖于完整的语音或文本输入，即在获取整个句子的语义信息后方可开始生成，缺乏仅基于历史与当前信息的实时生成研究。此外，从用户环境中可实时获取的模态主要包括语音、面部表情与头部姿态三种。头部姿态对手势的节奏与朝向具有明确的关系，但利用该模态来增强生成手势的自然度的相关研究尚不充分。

为此，本文提出了一种新颖的实时手势生成模型。该模型以帧为单位，输入语音、面部表情与头部姿态数据，并逐帧输出对应的骨骼动画。本文首次在实时手势生成中，将头部姿态作为一种新的模态引入，利用其与自然手势节奏与朝向的高度相关性，结合语音和面部信息共同提升生成动作的自然度。我们采用级联多模态架构与自回归训练来融合这些模态，以学习其联合表征，从而在严格的实时约束下增强手势的表现力。

本文的主要贡献如下：

\begin{enumerate}
\item 提出了 FaceCapGes，一种帧级实时手势生成模型，使用户无需动作捕捉设备或实际做出手势，仅通过语音等常见输入即可驱动虚拟人的手势动画；
\item 将头部姿态作为新模态引入多模态级联架构，在实时生成约束下有效提升了手势的自然性与表现力；
\item 通过实验结果表明，该模型在手势自然性、语音-手势对齐度与实时响应方面展示了良好的性能。其框架适用于所有兼容 ARKit 的设备。
\end{enumerate}

\section{研究内容}

本文的研究目标是构建一种基于语音、面部捕捉与头部姿态的实时数字人手势生成模型，实现无需动作捕捉设备即可驱动虚拟角色自然表达的实时动画系统。该研究旨在解决现有手势生成方法对未来上下文的依赖及实时性不足的问题，从而为虚拟人交互提供更低门槛、更高沉浸度的解决方案。

为实现上述目标，本文的主要研究内容如下：

其一，设计一种帧级手势生成架构。
为实现帧级实时推理，本架构在基线模型 \cite {beatcamn} 的基础上做两种调整：一方面，保留其语音、面部表情等可实时获取的输入模态，移除非实时模态的输入分支与特征处理模块；另一方面，采用滑动窗口式自回归训练方式，确保模型仅依赖历史与当前帧信息进行推理（不依赖未来上下文），同时通过窗口内时序依赖建模保持动作的时间连续性。由此使架构可处理逐帧输入的实时流数据。

其二，提出一种引入头部姿态的新型模态融合策略。
在现有语音、面部表情生成手势的模型基础上，将头部姿态作为终端模态引入，设计头部姿态编码器并提取特征，以增强生成手势的朝向的自然性。

其三，搭建实验系统与用户测试环境。
本文基于主流渲染引擎构建了虚拟人驱动与手势动画的可视化系统。该系统作为评估平台，为后续的主观实验提供了统一环境。

综上所述，本文引入头部姿态的新模态，来辅助手势生成模型从历史与现在信息推理手势的能力，并且提供了一个实验场景验证模型的推理质量与实时性能。

\section{论文组织架构}

本文共分为六个章节，内容安排如下：

第一章为绪论，介绍本研究的背景与意义，阐述研究目标、主要内容及核心贡献，并说明论文的整体组织结构。

第二章为相关工作， 回顾了国内外在语音驱动手势生成、多模态学习及实时面部捕捉技术等方面的研究现状，分析了现有方法的不足，并明确了本文的研究定位与创新点。

第三章为方法， 详细介绍了本文提出的 FaceCapGes 模型的整体架构与算法流程，包括多模态输入编码、姿态解码机制、滑动窗口自回归训练策略及对抗优化过程。

第四章为评估， 阐述了模型在 BEAT 数据集上的实验设置与性能评估，涵盖客观指标、用户主观实验及消融研究；同时分析了模型在实时交互场景中的表现与应用潜力。

第五章为结论， 总结了本文的主要研究成果与贡献，讨论了当前系统的局限性，并对未来在语义手势生成与跨平台实时驱动方向上的研究进行了展望。

第六章为附录与致谢部分， 包含用户实验的平衡拉丁方设计、附加实验结果、参考文献及致谢内容。

%近年来，已提出多种手势生成方法，包括多层感知机（MLP）\cite{gesticulator2020,beatcamn}、循环神经网络（RNN）\cite{beatcamn}、图神经网络\cite{gesturemaster2022}、扩散模型\cite{diffsheg,diffgesture,DiffTED2024}、VQ-VAE\cite{emage}以及自监督学习方法\cite{diffusion-self-supervised2023}。这些模型在生成质量上表现优异，但大多数依赖离线处理或未来上下文信息，难以满足实时应用需求。部分在线模型已被提出\cite{towards_realtime_co_speech_gesture_generation,realtime_gesture_animation_generation}，但往往仍需短时未来窗口，并忽略了常见设备中可用的重要模态信息。

%为解决上述问题，本文提出FaceCapGes，一种无需动作捕捉设备即可实现用户驱动的虚拟人控制的帧级实时手势生成模型。本模型支持在无未来上下文的情况下进行在线推理，能从实时输入中持续预测手势。我们首次引入“头部姿态”作为新模态，与语音和面部捕捉结合，利用其与自然手势节奏的高度相关性，提升生成动作的自然度。各模态通过级联多模态架构进行处理，以学习其联合关系，从而在实时约束下提升手势表达力。

%本方法可部署于轻量设备，具体实现基于配备 Apple ARKit 面部捕捉系统的 iPhone。尽管模型本身计算效率高，具备实时推理能力，但目前仍依赖 BEAT 数据集提供的 ARKit 专用面部数据格式，从中提取面部表情与头部动作。因此，跨平台部署能力仍受限，对通用摄像头、桌面环境或 VR 头显的支持仍是一个开放问题。

%通过在 BEAT 数据集上的实验表明，引入头部姿态显著提升了手势自然性，同时保持了低延迟的实时性能。实验结果说明，即使在无动作捕捉设备与未来输入的情况下，用户也可仅通过语音与头部动作驱动富有表现力的虚拟角色，只要输入符合 ARKit 格式即可。
%为实现实时推理，模型采用滑动窗口的自回归训练策略，并在不依赖未来信息的情况下处理各输入模态。此外，我们使用六维连续旋转表示（Rot6d）\cite{rot6d} 替代传统的欧拉角表示，从而提升动作多样性。后续章节将详细介绍模型架构、训练策略及在 BEAT 数据集上的评估结果，并在主观质量、时序对齐与推理延迟方面与现有手势生成模型进行对比（见图~\ref{fig3}，表~\ref{tab1}，表~\ref{tab3}）。

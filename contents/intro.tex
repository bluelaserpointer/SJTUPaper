% !TEX root = ../main.tex

\chapter{引言}

\section{研究背景和意义}

近年来，随着元宇宙、虚拟社交与直播等领域的相关技术日趋成熟，用户已能够使用任意外观的虚拟人作为交互载体，在虚拟空间中与异地用户进行交流。虚拟人3D模型的姿态由其内部骨架关节的旋转参数（如欧拉角、四元数等）定义，最终通过蒙皮渲染技术完成可视化。得益于自动骨骼绑定技术，骨骼动画的生成可消除不同3D模型间的骨架拓扑差异，实现跨模型复用。

在虚拟人交互中，穿戴式动作捕捉设备是实时驱动手势的传统方案，贴合肢体的标记点可将肢体运动实时转换为相同的骨骼动画，提供直观、准确的操控。尽管其精度较高，但对于大多数用户而言，此类设备存在功能用途单一、硬件成本昂贵、便携性差等问题，限制了使用频率。因此，在当前的虚拟人交互应用中，仅少数专业用户会使用此类设备，而多数用户在设备限制下无法简单控制虚拟人肢体，导致了两种用户体验之间的不一致。

针对普通用户对低门槛虚拟人交互的需求，基于相机的动作捕捉技术\cite{mediapipefacemesh,AppleARKitTrackingGuide}成为主流替代方案。该技术无需额外硬件，仅通过手机或电脑的内置相机即可实时捕捉用户动作，用于转化为骨骼动画。然而，该方案仍存在三种局限：一是这种方法要求用户在空中面向相机做出手势，过程中难以同步操作键盘、鼠标，造成操作冲突；二是手势活动范围受相机视野限制，在自然使用距离通过手机或电脑的内置相机拍摄用户，将严重限制手势的捕捉范围；三是持续的手势动作会产生体力消耗，在直播等长时间使用场景中，用户疲劳问题将变得显著。

因此，我们提出一种新的需求：一种无需用户实际做出手势，仅通过用户的实时语音、面部捕捉与头部姿态，实时生成与语义和情感相匹配的手势骨骼动画。该方法旨在降低使用门槛，并解决操作冲突与体力消耗问题。

然而，现有研究尚未提供成熟的解决方案满足上述需求。首先，当前多数手势生成方法依赖完整的语音或文本输入。由于用户的语音输入是逐字进行的，计算机需要等待用户的未来输入才能解析当前的语义，造成动画生成的延迟。其次，现有研究未对用户的头部姿态模态做深入研究。头部姿态对手势的节奏与朝向具有明确的关系，且可以通过常规相机实时捕捉，但利用该模态来增强生成手势的自然度的相关研究尚不充分。

为此，本文提出了一种新颖的实时手势生成模型。该模型以帧为单位，输入语音、面部表情与头部姿态数据，并逐帧输出对应的骨骼动画。本文首次在实时手势生成中，将头部姿态作为一种新的模态引入，利用其与自然手势节奏与朝向的高度相关性，结合语音和面部信息共同提升生成动作的自然度。我们采用级联多模态架构与自回归训练来融合这些模态，以学习其联合表征，从而在严格的实时约束下增强手势的表现力。

\section{手势生成质量评估点}
语音驱动手势生成的目标并非仅复现关节轨迹，而是在交互语境中生成“看起来像人说话时自然会出现”的动作表现。

生成手势通常应满足以下直观属性：
\begin{enumerate}
\item 动作应具有自然的运动学规律与视觉连贯性，避免抖动、漂移与非人体的突然加速；
\item 手势的节奏与语音韵律应在时间上协调一致，使动作的起势、峰值与回落与重音或语调变化相呼应；
\item 生成结果应具有一定的变化性与表达张力，避免长时间静止或重复单一模式；
\item 在具备语义表达能力的任务中，动作还应与语义内容或情绪倾向保持一致；
\item 此外，面向实时交互系统的研究还必须兼顾低延迟与稳定性，使动作能够在连续输入流下平滑输出并保持可部署性。
\end{enumerate}
基于上述目标，现有研究形成了较为体系化的评价维度，可概括为以下几类：

\begin{enumerate}
    \item \textbf{自然性（Naturalness）} —— 衡量生成动作在运动平滑性、速度变化及能量分布上的合理性，常采用 FGD（Fréchet Gesture Distance）\cite{ginosar2019speech2gesture}、
          运动速度统计、或主观“自然度”评分等指标。
    \item \textbf{同步性（Synchronization）} —— 评估手势在时间上与语音重音或韵律事件的对齐程度，
          常用BA（Beat Alignment）\cite{kucherenko2021predictability}等方法，
          以及基于重读检测的主观同步性评价。
    \item \textbf{多样性（Diversity）} —— 衡量模型在不同语音输入下生成的动作变化程度，
          通常以轨迹分布的方差、速度曲线差异或L1范数等指标度量，
          以防止模型陷入单一模式或过度平滑。
    \item \textbf{语义相关性（Semantic Relevance）} —— 反映生成动作与语义关键词或情绪类别的一致性，
          可通过 SRGR（Semantic Relevance to Gesture Ratio）\cite{beatcamn} 等指标或人工标注语义标签对齐评估。
    \item \textbf{实时性与稳定性（Latency \& Robustness）} —— 在面向交互系统的研究中，
          还需评估帧级推理延迟与输出平滑性，以确保动作流连续且系统响应及时。
\end{enumerate}

\section{国内外研究现状}

近年来，语音驱动手势生成经历了从规则设计到数据驱动模型、再到多模态扩展与实时生成的持续演变。
这一过程不仅体现了算法架构的更新，也反映了研究目标与应用场景的变化：
从基于语言规则的行为映射，到学习语音—动作关系的深度生成模型，再到面向交互的多模态实时系统。

\subsection{规则驱动阶段}

早期的手势生成系统主要依赖语言学规则与专家知识构建\cite{behavior_expression_animation_toolkit,robot_behavior_toolkit,gesture_generation_by_imitation,gesture_and_speech_in_interaction}。
这类方法通过语义分类或韵律规则将语音片段映射为预定义的手势模板（如指示、肯定、节奏性动作），并以有限的动作库组合出手势序列。
它们可在虚拟代理或机器人中实现基于语音的同步动作。
然而，手势词典与语法规则的人工设计成本较高，难以覆盖自然语音中的多样变化，导致生成结果缺乏自然性与个体差异。

\subsection{数据驱动阶段}

随着大规模语音与动作配对数据的出现，研究者开始采用统计学习和深度神经网络模型学习语音—手势映射关系。
在此阶段，语音通常作为唯一输入模态，模型通过长短期记忆网络（Long Short-Term Memory, LSTM）、多层感知机（Multilayer Perceptron, MLP）等结构预测连续手势序列。
典型代表如CaMN模型\cite{beatcamn}，其基于BEAT数据集\cite{beatcamn}训练级联网络，将LSTM、全连接网络与GAN结构相结合，实现从语音到动作的端到端预测。

然而，该类模型多使用欧拉角或离散旋转参数作为手势表示，生成结果容易出现抖动与不连续。
后续工作引入更平滑的表示方式，如Rot6D\cite{rot6d,emage,Ao2023GestureDiffuCLIP, AMUSE2024}或Axis-Angle\cite{diffsheg}，显著提升了动作流畅性。
与此同时，为解决语音与手势间的多对多映射问题，研究者引入了向量量化变分自编码器（Vector Quantized Variational AutoEncoder，VQ-VAE）\cite{emage,zhang2024SemanticGesticulator}与扩散模型\cite{tamingDiffgesture,diffsheg,diffstylegesture,DiffTED2024,diffusion-self-supervised2023}，
在保持自然性的同时提升了生成多样性与表现力。

尽管这些方法在客观指标与视觉效果上优于传统模型，但通常依赖完整语句级上下文。在用户语音下的流式逐字输入场景中，为获取未来上下文以进行语义判别与韵律对齐，需引入缓冲机制，因此即使推理较快的模型\cite{diffsheg}，整体延迟也因上下文缓冲造成端到端的显著延迟。

\subsection{多模态扩展阶段}

为进一步提升动作表现力与语音理解能力，部分研究引入视觉模态或语言语义特征。
例如，CaMN\cite{beatcamn}在语音输入的基础上融合面部捕捉信息以增强表现；
EMAGE\cite{emage}与DiffSHEG\cite{diffsheg}同时生成手势与面部动作；
DiffTED\cite{DiffTED2024}实现了端到端的视频合成。

这些多模态生成方法在提升虚拟智能体的自然感与沉浸感方面表现优异，但其任务假设仍基于整句输入，因此主要用于AI虚拟形象生成或离线内容创作场景，而非实时用户交互。

\subsection{国内外研究现状总结}
本节关注近年来代表性生成范式及其在线化需求；其中部分方法在离线设定下效果突出，但本文受严格因果与低延迟约束未予实现，仅作为后续工作参考与对照基线。

\paragraph{技术趋势}
近年来的手势生成研究在建模策略上呈现两类趋势；

\begin{enumerate}
  \item \textbf{扩散模型（Diffusion-based generation）}：
  扩散模型在手势生成中常带来较高的动作自然度与多样性\cite{diffsheg,diffstylegesture,DiffTED2024,tamingDiffgesture,alexanderson2023diffgesture}。
  现有工作多以固定长度的语音/文本片段作为条件，在迭代去噪采样中生成整段手势序列，
  因而在直接迁移到在线场景时会引入片段缓冲的延迟。
  另一方面，也已有研究开始探索离线实时或任意长度生成的采样与拼接策略（如基于扩展/外延采样的设计），
  表明扩散式框架具备向流式化演进的潜力\cite{diffsheg}。

  \item \textbf{语义增强方向（Semantic-aware generation）}：
  为覆盖iconic、metaphoric等更依赖语义的手势，
  引入语义表征作为额外条件\cite{yoon2020speechgesturebert,alexanderson2023diffgesture}，
  以提升动作与语义的一致性与表现力。
  在在线场景下，该方向的主要挑战通常来自语义信号的获得方式：
  现有工作使用整句/整段文本嵌入，基于更充分的上下文以稳定语义对齐，
  因而需要通过有限前瞻或缓冲策略来权衡延迟与语义质量。
\end{enumerate}

总体而言，上述方法多在离线或半离线设定下达到最佳效果；
若面向严格低延迟的在线实时生成，需配合流式条件建模、有限前瞻与快速采样等机制。
本次将作为后续工作的技术储备与对照基线。

\section{在线实时手势生成与离线生成的差异}
现有手势生成研究可从面向用户的在线实时生成、与面向AI的离线生成分为两类，两者的差异在表~\ref{tab:task_constraint_comparison}进行了对比：

\begin{table}[h]
\bicaption{离线与在线手势生成任务在约束与可利用信息上的对比}{Comparison of Constraints and Available Information between Offline and Online Gesture Generation}
\centering
\label{tab:task_constraint_comparison}
\begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
\toprule
\textbf{对比维度} & \textbf{AI 虚拟形象生成} & \textbf{用户虚拟人实时生成} \\ 
\midrule
输入信息 & 完整句级语音或文本（可使用未来信息） & 实时语音流，仅使用过去与当前帧 \\ 
输出目标 & 整句手势序列（离线生成） & 连续流式手势（逐帧生成） \\ 
时间约束 & 不必要实时 & 帧级实时性（$<$50\,ms 推理延迟） \\ 
评价重点 & 整体语义一致性与美学自然度 & 瞬时同步性、动作平滑与交互稳定性 \\ 
应用场景 & 离线动画、内容合成、AI虚拟直播 & 实时虚拟人、视频会议、用户虚拟直播 \\ 
\midrule
语音模态 & 作为输入或由文本生成（TTS 输出） & 作为实时输入特征（语音流） \\ 
手部手势 & 生成目标（输出） & 生成目标（输出） \\ 
面部表情 & 通常为生成目标（输出） & 可通过设备实时采集，作为输入辅助推理 \\ 
头部姿态 & 通常为生成目标（输出） & 可实时采集并作为输入特征，用于同步推理 \\ 
\bottomrule
\end{tabular}
\end{table}

\subsection{面向AI虚拟形象的整句手势生成}

这一类研究的目标是让AI驱动的文本对话系统的虚拟形象具备手势表现力。

模型可以一次性生成下一句语音或文本，因此可以访问完整的未来信息，包括整句音频、文本和语义上下文。

典型方法通过编码完整句子的节奏与语义，预测整段动作轨迹，以最大化动作与语义的一致性和整体流畅性。

这类方法适合AI驱动的系统或离线生成的应用场景，如合成视频。

\subsection{面向用户交互的在线实时手势生成}

本文所聚焦的目标类型属于此类。

在用户实时说话的过程中，系统需根据当前语音流（以及可选的面部表情与头部姿态）即时生成同步手势。

此任务具有严格的实时性约束与因果性限制：模型在每一时刻只能使用当前及过去的信息，而不能访问未来语音或文本内容。

该任务更接近实时交互系统，而非内容生成系统。

\section{本文研究目标}

本文研究的目标是设计一种能够在实时条件下运行的语音驱动手势生成模型，使用户无需动作捕捉设备或特定硬件，仅通过语音输入与相机前的面部表情、头部动作即可驱动虚拟人的上肢与头部动作。与以往主要面向离线生成或预先制作型虚拟形象生成的研究不同，本文关注的任务场景为用户实时交互，因此系统必须在严格因果的条件下运行，即仅利用当前与过去的输入帧信息进行动作预测，避免依赖未来语音或文本内容。此外，实时运行对推理延迟与计算效率也提出了更高要求。

为满足上述需求，本文进一步引入用户的头部姿态作为辅助输入模态，为手势生成过程提供额外的非语言信号。头部姿态能反映注意方向与交互焦点，并在语音节奏发生变化时为动作的时序组织提供参考。通过将语音、面部与头部信号联合输入模型，系统能够在实时条件下获得更丰富的上下文线索，从而支撑连续动作的稳定生成。

本文以 CaMN 模型\cite{beatcamn}为基础进行扩展。CaMN 原为离线级联结构，其输入包括语音与面部捕捉特征，输出包含手部上肢动作与头部姿态。本文将其输入机制改写为逐帧输入的流式推理形式，并在此基础上引入头部姿态特征分析模块，将头部姿态作为独立通道输入至级联网络的后级层，以实现语音、面部与头部信号的联合驱动。最终，系统能够在实时语音流输入条件下逐帧生成骨骼动画输出，满足实时交互场景对因果性与低延迟的要求。

因此，本文的研究内容汇总如下：
\begin{enumerate}
    \item 多模态在线实时数字人手势生成系统设计：
    构建从设备平台采集语音、面部捕捉与头部姿态等输入模态，并以逐帧流式推理方式生成实时上半身动作的端到端系统流程，提出可部署的在线数字人驱动系统。

    \item 融合头部姿态的多模态级联手势生成模型架构：
    将头部旋转姿态作为新的实时输入模态，引入适用于在线场景的特征表示与编码器设计，并与语音、面部表情特征进行联合建模与融合，形成继承CaMN级联解码策略的多模态动作生成结构。

    \item 基于滑动窗口的自回归训练与在线推理一致性策略：
    针对严格因果的实时生成约束，提出片段切割与滑动窗口展开的自回归训练流程，结合单向时序解码器与历史动作缓冲机制，使模型在仅依赖当前及历史信息的条件下生成连续稳定的动作序列。

    \item 实验与评估平台实现：
    搭建统一的数据处理、推理与渲染流程，支持不同模型在相同输入条件下进行动作生成与公平对比，执行客观评价指标与主观用户实验的完整评估流程。
\end{enumerate}

\section{论文组织架构}
本文章节安排如下：

第一章陈述在线实时数字人驱动场景下，语音驱动手势生成的研究背景与意义，概述生成质量的主要评价维度，总结国内外研究现状并对比离线生成与在线生成任务的差异，在此基础上明确本文的研究目标与主要贡献；

第二章给出在线实时手势生成任务的定义与系统设计原则，明确输入、输出模态的参数化表示，并介绍端到端实时数字人驱动系统的整体框架与模块划分；

第三章介绍本文多模态级联手势生成模型 FaceCapGes，包括头部姿态模态的引入方式、编码器结构设计以及身体姿态的层次化解码过程；

第四章介绍本文模型的训练方法，提出滑动窗口自回归训练与推理一致性策略，给出片段切割、窗口展开、监督损失与对抗训练目标的定义；

第五章介绍本文模型的主观与客观评估，并从定性分析、消融实验与性能测试等角度验证本文方法在生成质量与实时性方面的优势；

第六章总结全文工作并讨论未来研究方向。
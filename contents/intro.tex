% !TEX root = ../main.tex

\chapter{引言}

\section{研究背景和意义}

近年来，随着元宇宙、虚拟社交与直播等领域的相关技术日趋成熟，用户已能够使用任意外观的虚拟人作为交互载体，在虚拟空间中与异地用户进行交流。虚拟人3D模型的姿态由其内部骨架关节的旋转参数（如欧拉角、四元数等）定义，最终通过蒙皮渲染技术完成可视化。得益于自动骨骼绑定技术，骨骼动画的生成可消除不同3D模型间的骨架拓扑差异，实现跨模型复用。

在虚拟人交互中，穿戴式动作捕捉设备是实时驱动手势的传统方案，贴合肢体的标记点可将肢体运动实时转换为相同的骨骼动画，提供直观、准确的操控。尽管其精度较高，但对于大多数用户而言，此类设备存在功能用途单一、硬件成本昂贵、便携性差等问题，限制了使用频率。因此，在当前的虚拟人交互应用中，仅少数专业用户会使用此类设备，而多数用户在设备限制下无法简单控制虚拟人肢体，导致了两种用户体验之间的不一致。

针对普通用户对低门槛虚拟人交互的需求，基于相机的动作捕捉技术\cite{mediapipefacemesh,AppleARKitTrackingGuide}成为主流替代方案。该技术无需额外硬件，仅通过手机或电脑的内置相机即可实时捕捉用户动作，用于转化为骨骼动画。然而，该方案仍存在三种局限：一是这种方法要求用户在空中面向相机做出手势，过程中难以同步操作键盘、鼠标，造成操作冲突；二是手势活动范围受相机视野限制，在自然使用距离通过手机或电脑的内置相机拍摄用户，将严重限制手势的捕捉范围；三是持续的手势动作会产生体力消耗，在直播等长时间使用场景中，用户疲劳问题将变得显著。

因此，本文提出一种新的需求：一种无需用户实际做出手势，仅通过用户的实时语音、面部捕捉与头部姿态，实时生成与语义和情感相匹配的手势骨骼动画。该方法旨在降低使用门槛，并解决操作冲突与体力消耗问题。

然而，现有研究尚未提供成熟的解决方案满足上述需求。首先，当前多数手势生成方法依赖完整的语音或文本输入。由于用户的语音输入是逐字进行的，计算机需要等待用户的未来输入才能解析当前的语义，造成动画生成的延迟。其次，现有研究未对用户的头部姿态模态做深入研究。头部姿态对手势的节奏与朝向具有明确的关系，且可以通过常规相机实时捕捉，但利用该模态来增强生成手势的自然度的相关研究尚不充分。

为此，本文将提出一种新颖的实时手势生成模型。该模型以帧为单位，输入语音、面部表情与头部姿态数据，并逐帧输出对应的骨骼动画。此外，本文在实时手势生成中，尝试将头部姿态作为一种新的模态引入，利用其与自然手势节奏与朝向的高度相关性，结合语音和面部信息共同提升生成动作的自然度。我们采用级联多模态架构与自回归训练来融合这些模态，以学习其联合表征，从而在严格的实时约束下增强手势的表现力。

因此，下面将从语音驱动手势生成的发展脉络出发，总结现有方法的主要假设与局限，并引出在线实时手势生成仍待解决的问题。

\section{国内外研究现状}

近年来，语音驱动手势生成经历了从规则设计到数据驱动模型、再到多模态扩展与实时生成的持续演变。
这一过程不仅体现了算法架构的更新，也反映了研究目标与应用场景的变化：
从基于语言规则的行为映射，到学习语音—动作关系的深度生成模型，再到面向交互的多模态实时系统。

\subsection{规则驱动阶段}

早期的手势生成系统主要依赖语言学规则与专家知识构建\cite{behavior_expression_animation_toolkit,robot_behavior_toolkit,gesture_generation_by_imitation,gesture_and_speech_in_interaction}。
这类方法通过语义分类或韵律规则将语音片段映射为预定义的手势模板（如指示、肯定、节奏性动作），并以有限的动作库组合出手势序列。
它们可在虚拟代理或机器人中实现基于语音的同步动作。
然而，手势词典与语法规则的人工设计成本较高，难以覆盖自然语音中的多样变化，导致生成结果缺乏自然性与个体差异。

\subsection{数据驱动阶段}

随着大规模语音与动作配对数据的出现，研究者开始采用统计学习和深度神经网络模型学习语音—手势映射关系。
在此阶段，语音通常作为唯一输入模态，模型通过长短期记忆网络（Long Short-Term Memory, LSTM）、多层感知机（Multilayer Perceptron, MLP）等结构预测连续手势序列。
典型代表如CaMN模型\cite{beatcamn}，其基于BEAT数据集\cite{beatcamn}训练级联网络，将LSTM、全连接网络与GAN结构相结合，实现从语音到动作的端到端预测。

然而，该类模型多使用欧拉角或离散旋转参数作为手势表示，生成结果容易出现抖动与不连续。
后续工作引入更平滑的表示方式，如Rot6D\cite{rot6d,emage,Ao2023GestureDiffuCLIP, AMUSE2024}或Axis-Angle\cite{diffsheg}，显著提升了动作流畅性。
与此同时，为解决语音与手势间的多对多映射问题，研究者引入了向量量化变分自编码器（Vector Quantized Variational AutoEncoder，VQ-VAE）\cite{emage,zhang2024SemanticGesticulator}与扩散模型\cite{tamingDiffgesture,diffsheg,diffstylegesture,DiffTED2024,diffusion-self-supervised2023}，
在保持自然性的同时提升了生成多样性与表现力。

尽管这些方法在客观指标与视觉效果上优于传统模型，但通常依赖完整语句级上下文。在用户语音下的流式逐字输入场景中，为获取未来上下文以进行语义判别与韵律对齐，需引入缓冲机制，因此即使推理较快的模型\cite{diffsheg}，整体延迟也因上下文缓冲造成端到端的显著延迟。

\subsection{多模态扩展阶段}

为进一步提升动作表现力与语音理解能力，部分研究引入视觉模态或语言语义特征。
例如，CaMN\cite{beatcamn}在语音输入的基础上融合面部捕捉信息以增强表现；
EMAGE\cite{emage}与DiffSHEG\cite{diffsheg}同时生成手势与面部动作；
DiffTED\cite{DiffTED2024}实现了端到端的视频合成。

近年来的手势生成研究在建模策略上呈现两类趋势；

\begin{enumerate}
  \item \textbf{扩散模型（Diffusion-based generation）}：
  扩散模型在手势生成中常带来较高的动作自然度与多样性\cite{diffsheg,diffstylegesture,DiffTED2024,tamingDiffgesture,alexanderson2023diffgesture}。
  现有工作多以固定长度的语音/文本片段作为条件，在迭代去噪采样中生成整段手势序列，
  因而在直接迁移到在线场景时会引入片段缓冲的延迟。
  另一方面，也已有研究开始探索离线实时或任意长度生成的采样与拼接策略（如基于扩展/外延采样的设计），
  表明扩散式框架具备向流式化演进的潜力\cite{diffsheg}。

  \item \textbf{语义增强方向（Semantic-aware generation）}：
  为覆盖iconic、metaphoric等更依赖语义的手势，
  引入语义表征作为额外条件\cite{yoon2020speechgesturebert,alexanderson2023diffgesture}，
  以提升动作与语义的一致性与表现力。
  在在线场景下，该方向的主要挑战通常来自语义信号的获得方式：
  现有工作使用整句/整段文本嵌入，基于更充分的上下文以稳定语义对齐，
  因而需要通过有限前瞻或缓冲策略来权衡延迟与语义质量。
\end{enumerate}

这些多模态生成方法在提升虚拟智能体的自然感与沉浸感方面表现优异，
但大多在离线生成场景下验证，其对在线实时任务的适用性仍需进一步讨论。

\subsection{在线实时手势生成与离线生成的差异}
现有手势生成研究可从面向用户的在线实时生成、与面向AI的离线生成分为两类，两者的差异在表~\ref{tab:task_constraint_comparison}进行了对比：

\begin{table}[h]
\bicaption{离线与在线手势生成任务在约束与可利用信息上的对比}{Comparison of Constraints and Available Information between Offline and Online Gesture Generation}
\centering
\label{tab:task_constraint_comparison}
\begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
\toprule
\textbf{对比维度} & \textbf{AI 虚拟形象生成} & \textbf{用户虚拟人实时生成} \\ 
\midrule
输入信息 & 完整句级语音或文本（可使用未来信息） & 实时语音流，仅使用过去与当前帧 \\ 
输出目标 & 整句手势序列（离线生成） & 连续流式手势（逐帧生成） \\ 
时间约束 & 不必要实时 & 帧级实时性（$<$50\,ms 推理延迟） \\ 
评价重点 & 整体语义一致性与美学自然度 & 瞬时同步性、动作平滑与交互稳定性 \\ 
应用场景 & 离线动画、内容合成、AI虚拟直播 & 实时虚拟人、视频会议、用户虚拟直播 \\ 
\midrule
语音模态 & 作为输入或由文本生成（TTS 输出） & 作为实时输入特征（语音流） \\ 
手部手势 & 生成目标（输出） & 生成目标（输出） \\ 
面部表情 & 通常为生成目标（输出） & 可通过设备实时采集，作为输入辅助推理 \\ 
头部姿态 & 通常为生成目标（输出） & 可实时采集并作为输入特征，用于同步推理 \\ 
\bottomrule
\end{tabular}
\end{table}

\subsubsection{面向AI虚拟形象的整句手势生成}

这一类研究的目标是让AI驱动的文本对话系统的虚拟形象具备手势表现力。

模型可以一次性生成下一句语音或文本，因此可以访问完整的未来信息，包括整句音频、文本和语义上下文。

典型方法通过编码完整句子的节奏与语义，预测整段动作轨迹，以最大化动作与语义的一致性和整体流畅性。

这类方法适合AI驱动的系统或离线生成的应用场景，如合成视频。

\subsubsection{面向用户交互的在线实时手势生成}

本文所聚焦的目标类型属于此类。

在用户实时说话的过程中，系统需根据当前语音流（以及可选的面部表情与头部姿态）即时生成同步手势。

此任务具有严格的实时性约束与因果性限制：模型在每一时刻只能使用当前及过去的信息，而不能访问未来语音或文本内容。

该任务更接近实时交互系统，而非内容生成系统。

鉴于在线实时任务更强调瞬时同步与稳定性，下一节将归纳现有研究中的主要评价维度，以明确本文实验的评估标准。

\subsection{手势生成质量评估点}
语音驱动手势生成的目标并非仅复现关节轨迹，而是在交互语境中生成“看起来像人说话时自然会出现”的动作表现。

生成手势通常应满足以下直观属性：
\begin{enumerate}
\item 动作应具有自然的运动学规律与视觉连贯性，避免抖动、漂移与非人体的突然加速；
\item 手势的节奏与语音韵律应在时间上协调一致，使动作的起势、峰值与回落与重音或语调变化相呼应；
\item 生成结果应具有一定的变化性与表达张力，避免长时间静止或重复单一模式；
\item 在具备语义表达能力的任务中，动作还应与语义内容或情绪倾向保持一致；
\item 此外，面向实时交互系统的研究还必须兼顾低延迟与稳定性，使动作能够在连续输入流下平滑输出并保持可部署性。
\end{enumerate}
基于上述目标，现有研究形成了较为体系化的评价维度，可概括为以下几类：

\begin{enumerate}
    \item \textbf{自然性（Naturalness）} —— 衡量生成动作在运动平滑性、速度变化及能量分布上的合理性，常采用 FGD（Fréchet Gesture Distance）\cite{ginosar2019speech2gesture}、
          运动速度统计、或主观“自然度”评分等指标。
    \item \textbf{同步性（Synchronization）} —— 评估手势在时间上与语音重音或韵律事件的对齐程度，
          常用BA（Beat Alignment）\cite{kucherenko2021predictability}等方法，
          以及基于重读检测的主观同步性评价。
    \item \textbf{多样性（Diversity）} —— 衡量模型在不同语音输入下生成的动作变化程度，
          通常以轨迹分布的方差、速度曲线差异或L1范数等指标度量，
          以防止模型陷入单一模式或过度平滑。
    \item \textbf{语义相关性（Semantic Relevance）} —— 反映生成动作与语义关键词或情绪类别的一致性，
          可通过 SRGR（Semantic Relevance to Gesture Ratio）\cite{beatcamn} 等指标或人工标注语义标签对齐评估。
    \item \textbf{实时性与稳定性（Latency \& Robustness）} —— 在面向交互系统的研究中，
          还需评估帧级推理延迟与输出平滑性，以确保动作流连续且系统响应及时。
\end{enumerate}

\section{本文研究目标与研究内容}

本文旨在面向实时数字人交互场景，研究一种可部署的在线随语手势生成方法。与离线生成任务不同，实时交互系统要求生成过程满足严格因果约束，即模型在任意时刻仅可利用当前及历史可观测信号进行推理，而无法依赖未来语音或动作片段。同时，系统还需具备低延迟、稳定连续输出等性能要求，以支持逐帧驱动虚拟人并保持自然的随语表达效果。

为满足上述在线实时生成需求，本文提出 FaceCapGes：一种仅依赖可在线采集信号的帧级多模态手势生成框架。该框架以语音、面部表情与头部姿态为输入，在严格因果条件下逐帧生成上半身 3D 骨骼动作，从而使用户无需真实做出手势即可驱动虚拟形象产生自然的随语表达。

围绕上述目标，本文的研究内容汇总如下：
\begin{enumerate}
    \item 提出面向实时交互的在线随语手势生成任务定义与系统框架，构建从多模态信号采集、流式推理到虚拟人驱动渲染的端到端流程，并实现可部署的实时数字人驱动系统。

    \item 设计基于语音、面部表情与头部姿态的在线多模态动作生成结构，引入适用于实时场景的头部姿态特征编码方法，并结合 CaMN 级联解码策略实现多模态融合建模。

    \item 提出满足严格因果约束的自回归训练策略，通过片段切割与滑动窗口展开实现帧级流式学习，并结合单向时序解码器与历史动作缓冲机制提升生成动作的连续性与稳定性。

    \item 构建统一的数据处理、推理与渲染评估平台，开展用户主观实验、客观指标测量与实时性能测试，并与代表性方法进行对比评估，以验证本文方法的有效性与性能。
\end{enumerate}


\section{论文组织架构}
本文章节安排如下：

第一章陈述在线实时数字人驱动场景下，语音驱动手势生成的研究背景与意义，概述生成质量的主要评价维度，总结国内外研究现状并对比离线生成与在线生成任务的差异，在此基础上明确本文的研究目标与主要贡献；

第二章给出在线实时手势生成任务的定义与系统设计原则，明确输入、输出模态的参数化表示，并介绍端到端实时数字人驱动系统的整体框架与模块划分；

第三章介绍本文多模态级联手势生成模型 FaceCapGes，包括头部姿态模态的引入方式、编码器结构设计以及身体姿态的层次化解码过程；

第四章介绍本文模型的训练方法，提出滑动窗口自回归训练与推理一致性策略，给出片段切割、窗口展开、监督损失与对抗训练目标的定义；

第五章介绍本文模型的主观与客观评估，并从定性分析、消融实验与性能测试等角度验证本文方法在生成质量与实时性方面的优势；

第六章总结全文工作并讨论未来研究方向。
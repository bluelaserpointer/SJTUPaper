% !TEX root = ../main.tex

\chapter{结论}

本文提出了 FaceCapGes，一种基于 CaMN 框架的帧级实时手势生成模型。与现有离线或依赖未来信息的方法不同，FaceCapGes 可使用户仅通过实时音频、面部 blendshape 权重与头部姿态，即可驱动虚拟角色生成自然手势，无需动作捕捉设备。

本模型融合 LSTM、MLP 以及对抗学习机制，采用级联架构与滑动窗口式自回归训练策略，支持低延迟的在线推理。我们首次将头部姿态作为终端模态引入，有效提升了手势的自然度与协调性。

综合评估表明，FaceCapGes 在生成质量上显著优于 CaMN，并在保持实时性的同时，在主观和客观指标上与主流离线方法表现相当。其模块化设计使其可部署于兼容 ARKit 的轻量设备上，具备良好的实用性与扩展性。

尽管当前系统尚未显式建模语义驱动的手势，未来工作将探索引入语言理解与意图识别机制，以丰富实时交互中的手势语义表达。此外，我们还计划扩展对通用摄像头与非 iOS 平台的支持，通过开发跨平台的面部与姿态捕捉接口，缓解当前对 ARKit 的依赖。

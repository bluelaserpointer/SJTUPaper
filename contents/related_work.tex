% !TEX root = ../main.tex

\chapter{相关工作}

\section{手势的定义与身体姿态的参数化表示}

\subsection{手势的定义与范围}

在人类交流中，手势是常与语音同时出现的身体动作信号，它承担语义补充、情感表达与互动调节等多重功能。这类动作通常呈现出明确的表达意图，使其区别于姿态平衡，移动等纯功能性动作\cite{kendon_2004_gesture}。因此，手势与语言并非两个相互独立的系统，而是源于共同的认知的表达机制。

在本研究的广义定义下，手势的运动形式不局限于手部运动，还可扩展至头部运动、躯干姿态等与表达相关的所有上半身动作。

\paragraph{Kendon连续体}
Kendon 连续体\cite{kendon_2004_gesture,mcneill_1992_hand}将与表达相关的手势行为，基于语言化程度做了以下分类：%
gesticulation（随语手势） %
$\rightarrow$ language-like gestures（语言样手势） %
$\rightarrow$ pantomime（拟态/哑剧式动作） %
$\rightarrow$ emblems（约定俗成的象征手势） %
$\rightarrow$ sign language（手语）。%
越靠左的类型通常更依赖当前的言语与语境、形式更即兴；%
越靠右则越接近离散的符号系统，规约化程度高，意义更稳定，可在缺少口语的情况下独立传达。%

当前人机交互、虚拟人/数字人驱动等方向的手势生成，%
多数工作聚焦于Kendon连续体最左侧的随语手势，%
即说话过程中自然出现、与语音节奏与语义强关联的上肢动作。其含义往往依赖当前的口语与语境，脱离它们时通常难以传达清晰含义。%

相对而言，连续体右侧的手势更接近文化中约定俗成的符号，可以脱离口语独立传达含义。%
例如，表达称赞的拍手动作，或表示"请保持安静"的嘴前竖起食指的动作，这些被视为象征手势，通常不在随语手势生成领域的研究对象中。

本文的研究范围据此限定在随语手势的学习与生成。

\paragraph{随语手势的分类}
McNeill\cite{mcneill_1992_hand}将随语手势进一步划分为四种基本类型：%

\begin{enumerate}
  \item \textbf{Iconic gestures（形象性手势）}：以具象方式描绘事物的外形、空间路径或动作特征。例如，用手势勾勒一个物体的轮廓，或划出一道线表示移动的轨迹。此类手势与语言内容直接对应，表达具体语义。
  \item \textbf{Metaphoric gestures（隐喻性手势）}：表达抽象概念或思维结构的手势，%
比如用双手做出捧起一个物体放到一边的动作，表示“先把这个话题放一边”。%
这种手势并不描绘实体，而是以具象化的方式呈现抽象语义。
  \item \textbf{Deictic gestures（指示性手势）}：指向空间中的对象、人物或方向，常用于对话焦点的指明与注意引导。
  \item \textbf{Beat gestures（节奏型手势）}：与语音重音、韵律或节奏同步的节奏性动作，通常不承载具体语义，但可用于强调语音节奏，引起听众对说话内容的注意。
\end{enumerate}

这四类手势构成了随语手势在语义与语篇功能上的主要维度，并在自然交流中常以复合形式出现。%
在手势生成任务中，研究通常将其视为不同的可生成目标：其中节奏型手势由于与语音韵律高度同步、对齐与建模相对容易，长期以来在数据驱动方法中更常被优先刻画；%
而形象性与隐喻性等语义相关手势则对文本的语义推理有更高要求，因而是更具挑战性的方向。%

鉴于本文以低延迟在线驱动为目标，本文优先建模与韵律强耦合的节奏型手势；语义一致的形象性/隐喻性手势留作后续工作。

\paragraph{节奏型手势在随语手势中的重要性}
尽管节奏型手势通常不承载具体语义信息，%
已有研究表明，其在交流效果与听众感知层面仍具有独立的价值。%
Baars等的实验\cite{FlapThoseHands}比较了无手势、仅使用节奏型手势、以及包含了形象性、隐喻性、指示性的意义性手势的三种演讲条件，%
结果显示，相较于完全不做手势，仅使用节奏型手势即可显著提升听众对说话者自然度的主观评价，并一定程度上提升了听众对演讲内容的记忆表现。%
而包含意义性手势的演讲条件在自然度与听众的记忆保留等指标上并未显著优于节奏型手势条件。
这一发现表明，即便缺乏形象性或隐喻性的语义映射，节奏型手势仍能通过与语音韵律的同步，对交流过程产生积极影响。

从功能上看，节奏型手势主要服务于语篇结构与韵律组织，%
其作用并非传递附加语义，而是通过时间对齐、重音标记与注意力引导，%
增强语音信息的感知显著性与节奏感。%
在真实的人机交互与虚拟人系统中，
这类手势常被作为一种低语义依赖、但高度稳健的非语言表达形式加以采用。

鉴于本文面向低延迟、严格逐帧的在线驱动场景，%
系统在任一时间步均无法获取未来文本或完整语义结构，%
对语义一致性要求较高的形象性与隐喻性手势难以可靠生成。%
相比之下，节奏型手势主要依赖于当前及局部时间窗口内的语音韵律特征，%
更适合在实时条件下进行稳定建模与生成。%
因此，本文选择以节奏型手势作为主要研究对象，%
并将其视为一种在系统约束下具有明确交互价值的可行随语手势形式。

\paragraph{头部手势的分类}
除手部动作外，头部动作同样是手势的重要组成部分。%
头部的点动与摆动在时间结构上常与手势及语音节奏保持同步\cite{gesture_and_speech_in_interaction}，%
在语用功能上既能辅助语音韵律的组织，也能表达态度与指向信息。%

在不同研究中，头部动作被从多个维度加以分析，%
其主要功能可归纳为以下几方面：%
\begin{enumerate}
  \item \textbf{韵律相关（prosodic）}动作反映语音重音与句法节奏的对应关系\cite{hadar_1989_headmovement}；%
  \item \textbf{语义或态度相关（semantic/attitudinal）}动作表达说话者的情绪倾向与交际意图\cite{kendon_2004_gesture,mcneill_1992_hand}；%
  \item \textbf{指向相关（deictic）}动作通过转头或注视方向建立叙事空间的参照\cite{mcneill_1992_hand}。
\end{enumerate}

此外，研究表明头部动作的启动时间往往早于发声\cite{esteve2017timing}。
具体而言，头部动作存在启动与加速过程，若其峰值需与重读音节的时间对齐，则动作必须提前起势。%
因此，头部动作可能对即将到来的语音韵律具有前瞻性。%

这一特征揭示了头部动作与语音之间的时序关系，%
说明视觉模态中的运动信号有时可先于声学事件出现。%
本文研究也因此关注头部动作，并将其纳入输入模态。

\subsection{身体姿态的参数化表示}
手势作为身体运动的子集，其生成和识别依赖于身体姿态的连续建模。因此，在进一步讨论手势生成方法之前，在此明确身体姿态的参数化表示方式。

\paragraph{骨架结构的定义}
在计算机动画与动作捕捉领域，身体姿态通常由骨架结构和关节旋转参数共同定义。骨架结构描述了人体各关节的拓扑关系及层级依赖；而每个姿态帧由一组关节旋转参数所确定，这些参数定义了相对于父节点的旋转变换。在不同的系统与任务中，骨架结构的具体形式往往有所差异，这种差异直接影响姿态数据的表示与学习方式。

在不同的系统与任务中，骨架结构可以遵循各自的标准，因此，不同的数据集、3D模型或神经网络往往基于自身定义的关节层级与命名体系进行训练与标注。例如，AMASS数据集\cite{AMASS:2019}采用SMPL拓扑结构\cite{SMPL:2015}，BEAT数据集\cite{beatcamn}使用简化上半身骨架。近年来的自动骨骼绑定与骨架归一化方法，通过学习或优化关节对应关系，实现了不同拓扑之间的姿态重定向（pose retargeting）\cite{GleicherRetargetting1998,MartinelliSkeleton-AwareRetargeting2024}，从而消除了模型依赖于特定骨架结构的限制。

\paragraph {旋转参数的选取}
在确定骨架结构之后，具体的关节状态可通过多种旋转参数进行描述。
常见的旋转参数表示方法包括：
\begin {enumerate}
      \item 欧拉角（Euler Angles）：
      通过三个顺序旋转角表示姿态，直观且参数维度低（3 维），适合存储，但运算存在万向节锁（Gimbal Lock）问题，导致特定姿态下自由度丢失，且插值过程易产生非物理运动；
      \item 四元数（Quaternion）：
      以四维单位向量（\(q_0^2 + q_1^2 + q_2^2 + q_3^2 = 1\)）表示旋转，避免欧拉角的奇异性的同时，实现平滑插值。
      然而在神经网络回归中通常需要显式处理单位长度约束，且同一旋转存在双覆盖（$q$ 与 $-q$ 等价），优化不稳定。
      \item Axis-Angle：以旋转轴（三维单位向量）与旋转角度（标量）的乘积表示旋转，参数紧凑（3维），是参数化人体模型SMPL\cite{SMPL:2015}的主要姿态存储格式。
      但角度取值范围存在周期性（\([0, 2\pi)\)），导致参数空间不连续，影响神经网络学习；
      \item 旋转矩阵：
      以3x3的正交矩阵，表示坐标系或物体绕某个轴旋转一定角度后的新姿态。
      常用于计算机图形学、机器人学和物理仿真中，但正交约束（\(R^T R = I\)且\(\det(R) = 1\)）提高神经网络的学习难度。
      \item Rot6d\cite {rot6d}：将3×3旋转矩阵的前两列展开为六维向量，
      通过 Gram-Schmidt 正交化过程自动保持列向量的正交性与单位长度，无需额外约束。
      该表示既继承了旋转矩阵的数值稳定性，又解决了其参数冗余和正交性约束难题，同时具备连续性优势。
\end {enumerate}

在计算机图形与实时渲染中，通常采用四元数或旋转矩阵进行骨骼变换与插值，以保证数值稳定性和计算效率。
然而，在深度学习任务中，这些表示方式的约束会对训练效率造成影响：
四元数需满足单位长度约束；旋转矩阵则需满足元素的正交约束。
近年来的研究表明\cite {rot6d}，Rot6d既规避了上述两种表示的约束难题，又保持了姿态空间的连续性与物理合理性，
在动作生成与姿态预测任务中展现出更优的可学习性。

手势生成方法对旋转参数的选取经历了一个演变过程。早期工作采用的方法各异（如欧拉角、Axis-Angle），多受限于特定框架或历史因素；
而近期研究，出于对神经网络训练稳定性的追求，已显著趋向于采用Rot6d表示。
这一趋势的实例对比见表~\ref{fig:rotation_comparison}。

\begin{table}[htbp]
\bicaption{不同旋转表示方式的空间连续性与使用示例}{}
\centering
\label{fig:rotation_comparison}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{表示方式} & \textbf{维度} & \textbf{连续性} & \textbf{典型应用场景} & \textbf{训练使用示例} \\ \midrule
欧拉角 & 3 & 存在万向节锁 & 姿态存储 & CaMN\cite{beatcamn} \\
四元数 & 4 & 连续，但存在单位长度约束 & 图形学 & — \\
Axis-Angle & 3 & 角度部分不连续 & 神经网络 & DiffSHEG\cite{diffsheg} \\
旋转矩阵 & 9 & 连续，但存在正交约束 & 图形学、机器人学 & MambaGesture\cite{fu2024mambagesture} \\
Rot6d & 6 & 连续 & 神经网络 & 近期多数\cite{emage, Ao2023GestureDiffuCLIP,AMUSE2024} \\ \bottomrule
\end{tabular}
\end{table}

因此，本文在姿态生成模型中统一使用Rot6d表示每个关节的旋转状态，以确保训练阶段的平滑收敛与推理阶段的稳定性。

\section{面部表情的定义与参数化表示}

面部表情是非语言交流的重要组成部分，与语音、手势共同传递情感和态度信息。与身体动作不同，面部表情主要由皮肤形变和局部运动构成，无法通过骨骼旋转直接建模，因此需要专门的参数化表示方式。

目前常见的面部状态描述方法主要有两类：

\paragraph{BlendShape权重模型}
FACS（Facial Action Coding System）\cite{EkmanFriesenFACS1978}提出复杂表情可分解为若干可组合的基本动作单元（Action Units, AU），为表情的参数化表示提供了理论基础。
受此启发，BlendShape 模型将面部表情表示为一组可线性叠加的形变基，每个基形对应一个权重控制的局部表情变化。复杂表情由多个基形的组合生成，其思想与FACS的动作单元体系相呼应。

与基于骨骼的形变不同，BlendShape 直接在顶点层面定义几何偏移量，因此对网格拓扑结构高度依赖：每个基形的顶点偏移需与基础网格逐点对应。在具有相同网格结构的角色模型之间，BlendShape 集合可以直接复用；但若拓扑发生变化，偏移数据将无法一一对应，从而难以在不同模型间映射或重定向。这种拓扑依赖性限制了其跨模型的通用性。

尽管如此，BlendShape 在表现精细面部表情和软组织形变方面具有显著优势。其标准化权重接口、实时可驱动性与渲染兼容性，使其成为虚拟人和表情捕捉系统的主流表示形式，并被广泛应用于如ARKit\cite{ARKitDocumentation} 等实时动画框架，以及主流渲染引擎中。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Fig_AUSample.png}
\caption{示例图来自~\cite{ozel_arkit_facs_cheatsheet}，展示FACS AU45（blink）对应的ARKit中的两种BlendShape基形：eyeBlinkLeft（闭左眼）与eyeBlinkRight（闭右眼）。}
\label{fig:au_sample}
\end{figure}

图~\ref{fig:bs_eyeblink}展示了eyeBlinkLeft、eyeBlinkRight的ARKit BlendShape基形在权重$w\!\in\![0,1]$下的线性插值效果（从张眼到闭眼）。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Fig_blendshapeEyeBlink.png}
\caption{BlendShape线性插值效果示例：eyeBlink从 $w{=}0$（左）到 $w{=}1$（右）的连续变化，中图为中间值。BS权重可直接用于实时渲染驱动。}
\label{fig:bs_eyeblink}
\end{figure}

\paragraph{关键点坐标模型}
关键点模型通过检测面部若干语义特征点（2D 或 3D 坐标）来描述几何结构变化。典型实现包括MediaPipe Face Mesh\cite{mediapipefacemesh}与OpenFace\cite{amos2016openface}系列。

与基于网格形变的BlendShape不同，关键点模型并不依附于任何具体网格拓扑，而是在几何空间中以语义一致的特征点集合形式定义面部结构。这种表示方式并不描述模型的形变，而是对人脸几何的抽象建模，因此常用于表情识别、头部姿态估计等分析任务，而较少用于驱动渲染。

在多模态学习与特征分析中，BlendShape表示具有较高的统一性和可量化性，适合以固定维度向量作为模型输入，并易于应用于不同虚拟角色的动画驱动。因此，本文在系统设计中采用与ARKit兼容的BlendShape参数作为面部模态输入特征。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.4\linewidth]{figures/Fig_MediaPipeLandmark.png}
\caption{MediaPipe Face Mesh 关键点结构示意图，截取自\cite{mediapipefacemesh}。}
\label{fig_mediapipe_landmark}
\end{figure}

\section{国内外研究现状}

\subsection{手势生成的研究目标}

\subsubsection{研究目标类型的差异}

语音驱动的手势生成研究在总体目标上虽一致——即让虚拟角色的动作与语音内容、节奏协调一致——但在使用场景与系统角色上存在显著差异。

现有研究大体可分为两类：

\paragraph{为AI的虚拟形象生成手势}

这一类研究的目标是让AI驱动的文本对话系统的虚拟形象具备手势表现力。

模型可以一次性生成下一句语音或文本，因此可以访问完整的未来信息，包括整句音频、文本和语义上下文。

典型方法通过编码完整句子的节奏与语义，预测整段动作轨迹，以最大化动作与语义的一致性和整体流畅性。

这类方法适合AI驱动的系统或离线生成的应用场景，如合成视频。

\paragraph{为用户的虚拟人生成实时手势}

本文所聚焦的目标类型属于第二类。

在用户实时说话的过程中，系统需根据当前语音流（以及可选的面部表情与头部姿态）即时生成同步手势。

此任务具有严格的实时性约束与因果性限制：模型在每一时刻只能使用当前及过去的信息，而不能访问未来语音或文本内容。

该任务更接近实时交互系统，而非内容生成系统。

\subsubsection{任务约束与可利用条件的差异}

这两类研究目标在可利用的信息条件和评价重心上存在本质区别：

\begin{table}[h]
\caption{两类手势生成任务在约束与可利用条件上的对比}
\centering
\label{tab:task_constraint_comparison}
\begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
\toprule
\textbf{对比维度} & \textbf{AI 虚拟形象生成} & \textbf{用户虚拟人实时生成} \\ 
\midrule
输入信息 & 完整句级语音或文本（可使用未来信息） & 实时语音流，仅使用过去与当前帧 \\ 
输出目标 & 整句手势序列（离线生成） & 连续流式手势（逐帧生成） \\ 
时间约束 & 不必要实时 & 帧级实时性（$<$50\,ms 推理延迟） \\ 
评价重点 & 整体语义一致性与美学自然度 & 瞬时同步性、动作平滑与交互稳定性 \\ 
应用场景 & 离线动画、内容合成、AI虚拟直播 & 实时虚拟人、视频会议、用户虚拟直播 \\ 
\midrule
语音模态 & 作为输入或由文本生成（TTS 输出） & 作为实时输入特征（语音流） \\ 
手部手势 & 生成目标（输出） & 生成目标（输出） \\ 
面部表情 & 通常为生成目标（输出） & 可通过设备实时采集，作为输入辅助推理 \\ 
头部姿态 & 通常为生成目标（输出） & 可实时采集并作为输入特征，用于同步推理 \\ 
\bottomrule
\end{tabular}
\end{table}

前者可以在生成阶段规划动作节奏与语义对应，而后者需在无未来信息的条件下维持自然与同步，
并保证输出连续、平滑且无突跳，从而对模型结构、输入模态与延迟控制提出更高要求。

\subsubsection{评估方法}

语音驱动手势生成研究的评价体系通常涵盖生成质量、时序匹配与表达多样性等多个维度。
研究者既关注生成动作的自然性与视觉流畅度，也重视其与语音信号在节奏和语义层面的对应关系。
近年来，随着实时生成和多模态扩展任务的发展，相关评估方法也逐渐体系化，可概括为以下几类：

\begin{enumerate}
    \item \textbf{自然性（Naturalness）} —— 衡量生成动作在运动平滑性、速度变化及能量分布上的合理性，常采用 FGD（Fréchet Gesture Distance）\cite{ginosar2019speech2gesture}、
          运动速度统计、或主观“自然度”评分等指标。
    \item \textbf{同步性（Synchronization）} —— 评估手势在时间上与语音重音或韵律事件的对齐程度，
          常用BA（Beat Alignment）\cite{kucherenko2021predictability}等方法，
          以及基于重读检测的主观同步性评价。
    \item \textbf{多样性（Diversity）} —— 衡量模型在不同语音输入下生成的动作变化程度，
          通常以轨迹分布的方差、速度曲线差异或L1范数等指标度量，
          以防止模型陷入单一模式或过度平滑。
    \item \textbf{语义相关性（Semantic Relevance）} —— 反映生成动作与语义关键词或情绪类别的一致性，
          可通过 SRGR（Semantic Relevance to Gesture Ratio）\cite{beatcamn} 等指标或人工标注语义标签对齐评估。
    \item \textbf{实时性与稳定性（Latency \& Robustness）} —— 在面向交互系统的研究中，
          还需评估帧级推理延迟与输出平滑性，以确保动作流连续且系统响应及时。
\end{enumerate}

总体而言，现有评价体系既包含客观的运动学与统计指标，也结合主观感知评分，
在不同任务目标下可形成“自然性—同步性—多样性—相关性”的综合评估框架。

\subsection{手势生成的演变}

近年来，语音驱动手势生成经历了从规则设计到数据驱动模型、再到多模态扩展与实时生成的持续演变。
这一过程不仅体现了算法架构的更新，也反映了研究目标与应用场景的变化：
从基于语言规则的行为映射，到学习语音—动作关系的深度生成模型，再到面向交互的多模态实时系统。

\subsubsection{规则驱动阶段}

早期的手势生成系统主要依赖语言学规则与专家知识构建\cite{behavior_expression_animation_toolkit,robot_behavior_toolkit,gesture_generation_by_imitation,gesture_and_speech_in_interaction}。
这类方法通过语义分类或韵律规则将语音片段映射为预定义的手势模板（如指示、肯定、节奏性动作），并以有限的动作库组合出手势序列。
它们可在虚拟代理或机器人中实现基于语音的同步动作。
然而，手势词典与语法规则的人工设计成本较高，难以覆盖自然语音中的多样变化，导致生成结果缺乏自然性与个体差异。

\subsubsection{数据驱动阶段}

随着大规模语音与动作配对数据的出现，研究者开始采用统计学习和深度神经网络模型学习语音—手势映射关系。
在此阶段，语音通常作为唯一输入模态，模型通过LSTM、MLP等结构预测连续手势序列。
典型代表如CaMN模型\cite{beatcamn}，其基于BEAT数据集\cite{beatcamn}训练级联网络，将LSTM、全连接网络与GAN结构相结合，实现从语音到动作的端到端预测。

然而，该类模型多使用欧拉角或离散旋转参数作为手势表示，生成结果容易出现抖动与不连续。
后续工作引入更平滑的表示方式，如Rot6d\cite{rot6d,emage,Ao2023GestureDiffuCLIP, AMUSE2024}或Axis-Angle\cite{diffsheg}，显著提升了动作流畅性。
与此同时，为解决语音与手势间的多对多映射问题，研究者引入了VQ-VAE\cite{emage,zhang2024SemanticGesticulator}与扩散模型\cite{tamingDiffgesture,diffsheg,diffstylegesture,DiffTED2024,diffusion-self-supervised2023}，
在保持自然性的同时提升了生成多样性与表现力。

尽管这些方法在客观指标与视觉效果上优于传统模型，但通常依赖完整语句级上下文。在用户语音下的流式逐字输入场景中，为获取未来上下文以进行语义判别与韵律对齐，需引入缓冲机制，因此即使推理较快的模型\cite{diffsheg}，整体延迟也因上下文缓冲造成端到端的显著延迟。

\subsubsection{多模态扩展阶段}

为进一步提升动作表现力与语音理解能力，部分研究引入视觉模态或语言语义特征。
例如，CaMN\cite{beatcamn}在语音输入的基础上融合面部捕捉信息以增强表现；
EMAGE\cite{emage}与DiffSHEG\cite{diffsheg}同时生成手势与面部动作；
DiffTED\cite{DiffTED2024}实现了端到端的视频合成。

这些多模态生成方法在提升虚拟智能体的自然感与沉浸感方面表现优异，但其任务假设仍基于整句输入，因此主要用于AI虚拟形象生成或离线内容创作场景，而非实时用户交互。

\subsection{当代生成研究的策略趋势}
本节关注近年来代表性生成范式及其在线化需求；其中部分方法在离线设定下效果突出，但本文受严格因果与低延迟约束未予实现，仅作为后续工作参考与对照基线。
近年来的手势生成研究在建模策略上呈现两类趋势；

\begin{enumerate}
  \item \textbf{扩散模型（Diffusion-based generation）}：
  扩散模型在手势生成中常带来较高的动作自然度与多样性\cite{diffsheg,diffstylegesture,DiffTED2024,tamingDiffgesture,alexanderson2023diffgesture}。
  现有工作多以固定长度的语音/文本片段作为条件，在迭代去噪采样中生成整段手势序列，
  因而在直接迁移到在线场景时会引入片段缓冲的延迟。
  另一方面，也已有研究开始探索离线实时或任意长度生成的采样与拼接策略（如基于扩展/外延采样的设计），
  表明扩散式框架具备向流式化演进的潜力\cite{diffsheg}。

  \item \textbf{语义增强方向（Semantic-aware generation）}：
  为覆盖iconic、metaphoric等更依赖语义的手势，
  引入语义表征作为额外条件\cite{yoon2020speechgesturebert,alexanderson2023diffgesture}，
  以提升动作与语义的一致性与表现力。
  在在线场景下，该方向的主要挑战通常来自语义信号的获得方式：
  现有工作使用整句/整段文本嵌入，基于更充分的上下文以稳定语义对齐，
  因而需要通过有限前瞻或缓冲策略来权衡延迟与语义质量。
\end{enumerate}

总体而言，上述方法多在离线或半离线设定下达到最佳效果；
若面向严格低延迟的在线实时生成，需配合流式条件建模、有限前瞻与快速采样等机制.
本次将作为后续工作的技术储备与对照基线。

\section{实时生成的理论基础与可行性分析}
从生成可行性的角度，现有研究普遍认为节奏型手势可在无语义理解的条件下由语音韵律直接驱动生成。
多数语音驱动手势研究证实，仅凭语音的能量、时长与音高变化即可合成自然的节奏性上肢动作\cite{ginosar2019speech2gesture,alexanderson2020stylegestures,kucherenko2021movingfastslow}。
这些研究所生成的动作在时间结构上与语音重音同步，体现了语音与手势共享的时间规划机制。

相比之下，iconic（形象性）、metaphoric（隐喻性）与deictic（指向性）手势均依赖语义或指向关系，
需要从上下文分析语义与语境，难以在严格实时的因果条件下生成。
而节奏相关特征在音频中则具有更高的可预测性。\cite{kucherenko2021predictability}
这表明，在缺乏未来语义与全局上下文的实时场景中，仅凭语音模态，模型只能稳定生成节奏层面的动作。

为突破这一限制，本文引入头部姿态模态作为补充输入信号。头部姿态能在实时因果条件下提供部分空间与时间线索：其转头与注视方向反映互动焦点，点头与抬头与语音重读共现，能够在不依赖未来语义信息的前提下，为手势生成提供弱先验约束。
这种模态扩展为实时系统提供了理论上的可行性基础，使模型能够在语音之外获得关于节奏、方向与视角的附加信息。

\paragraph{头部姿态对手势预测的贡献}
头部动作在自然语音中常呈现出一定的时间前瞻性~\cite{esteve2017timing}：%
其启动往往早于对应韵律词的发声，%
这意味着视觉模态可能比声学信号更早反映语音节奏的变化趋势。%
这种时序特性为实时生成任务提供了潜在的预测窗口，%
使系统能够在语音节奏变化尚未显现前，就提前捕获相关的动态线索。%
因此，头部姿态在实时生成中不仅提供同步参考，也可能在时间上形成前驱信号，为手势节奏的自然启动提供时序优势。%

\paragraph{头部姿态对空间锚定与视角一致性的贡献}
头部姿态模态为实时语音驱动的手势生成提供了关键的空间参照信号。%
其与语音韵律在时间组织上高度耦合。%
即使在无未来语义信息的条件下，头部的转向与注视变化仍能反映说话者的注意焦点与叙述方向，%
从而帮助模型在动作生成中保持空间的连贯性与方向一致性。%
这一机制使系统能够在时间与空间两个维度上同步对齐语音与动作，%
让生成的手势在视觉上更具互动感与表达意图。

在McNeill的四类手势体系中，头部姿态的引入主要强化了两类动作的生成：  
(1) 对beat手势而言，它为语音重读和节奏段落提供显式的时间协同信号，使手部与头部动作在韵律层面更加一致；  
(2) 对iconic手势而言，它在具有路径与方向特征的动作中提供空间参考，使模型能够在叙事空间中更稳定地确定动作的方位与轨迹方向。  
通过这两方面的强化，系统在保持实时性的同时获得了更自然的节奏衔接与空间表达。

与此同时，本文明确头部姿态模态的作用边界：其核心优势在于捕捉方向、焦点与时序节奏，而非手型语义或复杂形态描摹等细粒度语义特征。换言之，它主要改善手势的位置、方向与视角依附，而非手势的形状描绘或语义内容。对于依赖抽象语义或外指参照的metaphoric与deictic手势，仍需语言或上下文模态的补充。

总体而言，头部姿态为实时生成提供了介于韵律与语义之间的关键中层约束。%
其时间上的前瞻性与空间上的指向性共同帮助模型在低延迟条件下保持自然、连贯且空间协调的动作表现，%
从而在因果生成框架内有效拓展了语音驱动手势的可表达范围，并为节奏主导型动作的实时生成提供了结构的支持。

\section{本文研究目标}

本文研究的目标是设计一种能够在实时条件下运行的语音驱动手势生成模型，使用户无需动作捕捉设备或特定硬件，仅通过语音输入与相机前的面部表情、头部动作即可驱动虚拟人的上肢与头部动作。与以往主要面向离线生成或预先制作型虚拟形象生成的研究不同，本文关注的任务场景为用户实时交互，因此系统必须在严格因果的条件下运行，即仅利用当前与过去的输入帧信息进行动作预测，避免依赖未来语音或文本内容。此外，实时运行对推理延迟与计算效率也提出了更高要求。

为满足上述需求，本文进一步引入用户的头部姿态作为辅助输入模态，为手势生成过程提供额外的非语言信号。头部姿态能反映注意方向与交互焦点，并在语音节奏发生变化时为动作的时序组织提供参考。通过将语音、面部与头部信号联合输入模型，系统能够在实时条件下获得更丰富的上下文线索，从而支撑连续动作的稳定生成。

本文以 CaMN 模型\cite{beatcamn}为基础进行扩展。CaMN 原为离线级联结构，其输入包括语音与面部捕捉特征，输出包含手部上肢动作与头部姿态。本文将其输入机制改写为逐帧输入的流式推理形式，并在此基础上引入头部姿态特征分析模块，将头部姿态作为独立通道输入至级联网络的后级层，以实现语音、面部与头部信号的联合驱动。最终，系统能够在实时语音流输入条件下逐帧生成骨骼动画输出，满足实时交互场景对因果性与低延迟的要求。

因此，本文的研究内容汇总如下：
\begin{enumerate}
    \item 面向实时交互的任务定义与系统流程设计：
    提出严格因果、帧级推理的语音驱动手势生成任务设定，并构建从语音、面部与头部实时采集到骨骼动画输出的端到端驱动流程。

    \item 基线模型的实时适配与训练策略：
    以 CaMN 级联架构为基础，将其离线输入机制改写为逐帧流式推理形式，并结合单向时序建模与滑动窗口自回归训练策略，使模型在仅依赖历史与当前帧信息的条件下生成连续动作。

    \item 头部姿态输入模态的建模与融合结构设计：
    将头部旋转姿态作为新的实时输入模态，设计头部姿态特征表示与编码器，并探索其与语音、面部表情的联合输入方式，形成后级融合的多模态级联手势生成结构。

    \item 实验与评估平台实现：
    实现统一的数据处理、推理与渲染流程，使不同模型能够在相同输入条件下进行动作生成与对比，并支持客观指标与主观实验的评估流程。
\end{enumerate}

\section{本章小结}
本章综述了语音驱动手势生成领域的相关研究现状与发展脉络。  
首先，对手势的概念与在计算机中的参数化表示进行了阐述，说明了手势与面部表情、头部姿态在虚拟人交互中的角色与差异。  
随后，从研究目标的角度分析了不同任务设定之间的区别，指出现有大多数工作聚焦于为 AI 虚拟形象生成整句级动作，而缺乏面向用户实时交互的研究。  
在此基础上，回顾了手势生成方法从规则驱动到数据驱动、从单模态到多模态的演变过程，  
总结了现有模型虽在生成质量上取得显著进展，但在实时性与因果性方面仍存在局限。

最后，结合本文的研究目标，提出了面向实时交互的语音驱动手势生成方案。


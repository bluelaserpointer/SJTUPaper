\chapter{多模态在线实时手势生成框架}
本章围绕本文面向在线实时数字人驱动的手势生成任务，
给出必要的概念定义、输入模态表示以及系统层面的总体设计框架。
首先，我们明确本文研究所涉及的手势的定义与分类，并限定任务范围以满足严格低延迟场景的可行性。
随后，本章介绍身体姿态与面部表情在计算机动画与深度学习中的参数化表示方式，为后续模型结构设计提供统一的输入输出形式。
最后，本章给出端到端实时系统的整体架构与模块职责划分，说明 FaceCapGes 在实际部署系统中的位置与数据流组织方式。

\section{手势的定义}
在人类交流中，手势是常与语音同时出现的身体动作信号，它承担语义补充、情感表达与互动调节等多重功能。这类动作通常呈现出明确的表达意图，使其区别于姿态平衡，移动等纯功能性动作\cite{kendon_2004_gesture}。因此，手势与语言并非两个相互独立的系统，而是源于共同的认知的表达机制。

在本研究的广义定义下，手势的运动形式不局限于手部运动，还可扩展至头部运动、躯干姿态等与表达相关的所有上半身动作。

\subsection{Kendon连续体}
Kendon 连续体\cite{kendon_2004_gesture,mcneill_1992_hand}将与表达相关的手势行为，基于语言化程度做了以下分类：%
gesticulation（随语手势） %
$\rightarrow$ language-like gestures（语言样手势） %
$\rightarrow$ pantomime（拟态/哑剧式动作） %
$\rightarrow$ emblems（约定俗成的象征手势） %
$\rightarrow$ sign language（手语）。%
越靠左的类型通常更依赖当前的言语与语境、形式更即兴；%
越靠右则越接近离散的符号系统，规约化程度高，意义更稳定，可在缺少口语的情况下独立传达。%

当前人机交互、虚拟人/数字人驱动等方向的手势生成，%
多数工作聚焦于Kendon连续体最左侧的随语手势，%
即说话过程中自然出现、与语音节奏与语义强关联的上肢动作。其含义往往依赖当前的口语与语境，脱离它们时通常难以传达清晰含义。%

相对而言，连续体右侧的手势更接近文化中约定俗成的符号，可以脱离口语独立传达含义。%
例如，表达称赞的拍手动作，或表示"请保持安静"的嘴前竖起食指的动作，这些被视为象征手势，通常不在随语手势生成领域的研究对象中。

本文的研究范围据此限定在随语手势的学习与生成。

\subsection{随语手势的分类}
McNeill\cite{mcneill_1992_hand}将随语手势进一步划分为四种基本类型：%

\begin{enumerate}
  \item \textbf{Iconic gestures（形象性手势）}：以具象方式描绘事物的外形、空间路径或动作特征。例如，用手势勾勒一个物体的轮廓，或划出一道线表示移动的轨迹。此类手势与语言内容直接对应，表达具体语义。
  \item \textbf{Metaphoric gestures（隐喻性手势）}：表达抽象概念或思维结构的手势，%
比如用双手做出捧起一个物体放到一边的动作，表示“先把这个话题放一边”。%
这种手势并不描绘实体，而是以具象化的方式呈现抽象语义。
  \item \textbf{Deictic gestures（指示性手势）}：指向空间中的对象、人物或方向，常用于对话焦点的指明与注意引导。
  \item \textbf{Beat gestures（节奏型手势）}：与语音重音、韵律或节奏同步的节奏性动作，通常不承载具体语义，但可用于强调语音节奏，引起听众对说话内容的注意。
\end{enumerate}

这四类手势构成了随语手势在语义与语篇功能上的主要维度，并在自然交流中常以复合形式出现。%
在手势生成任务中，研究通常将其视为不同的可生成目标：其中节奏型手势由于与语音韵律高度同步、对齐与建模相对容易，长期以来在数据驱动方法中更常被优先刻画；%
而形象性与隐喻性等语义相关手势则对文本的语义推理有更高要求，因而是更具挑战性的方向。%

鉴于本文以低延迟在线驱动为目标，本文优先建模与韵律强耦合的节奏型手势；语义一致的形象性/隐喻性手势留作后续工作。

\paragraph{节奏型手势在随语手势中的重要性}
尽管节奏型手势通常不承载具体语义信息，%
已有研究表明，其在交流效果与听众感知层面仍具有独立的价值。%
Baars等的实验\cite{FlapThoseHands}比较了无手势、仅使用节奏型手势、以及包含了形象性、隐喻性、指示性的意义性手势的三种演讲条件，%
结果显示，相较于完全不做手势，仅使用节奏型手势即可显著提升听众对说话者自然度的主观评价，并一定程度上提升了听众对演讲内容的记忆表现。%
而包含意义性手势的演讲条件在自然度与听众的记忆保留等指标上并未显著优于节奏型手势条件。
这一发现表明，即便缺乏形象性或隐喻性的语义映射，节奏型手势仍能通过与语音韵律的同步，对交流过程产生积极影响。

从功能上看，节奏型手势主要服务于语篇结构与韵律组织，%
其作用并非传递附加语义，而是通过时间对齐、重音标记与注意力引导，%
增强语音信息的感知显著性与节奏感。%
在真实的人机交互与虚拟人系统中，
这类手势常被作为一种低语义依赖、但高度稳健的非语言表达形式加以采用。

鉴于本文面向低延迟、严格逐帧的在线驱动场景，%
系统在任一时间步均无法获取未来文本或完整语义结构，%
对语义一致性要求较高的形象性与隐喻性手势难以可靠生成。%
相比之下，节奏型手势主要依赖于当前及局部时间窗口内的语音韵律特征，%
更适合在实时条件下进行稳定建模与生成。%
因此，本文选择以节奏型手势作为主要研究对象，%
并将其视为一种在系统约束下具有明确交互价值的可行随语手势形式。

\subsection{头部手势的分类}
除手部动作外，头部动作同样是手势的重要组成部分。%
头部的点动与摆动在时间结构上常与手势及语音节奏保持同步\cite{gesture_and_speech_in_interaction}，%
在语用功能上既能辅助语音韵律的组织，也能表达态度与指向信息。%

在不同研究中，头部动作被从多个维度加以分析，%
其主要功能可归纳为以下几方面：%
\begin{enumerate}
  \item \textbf{韵律相关（prosodic）}动作反映语音重音与句法节奏的对应关系\cite{hadar_1989_headmovement}；%
  \item \textbf{语义或态度相关（semantic/attitudinal）}动作表达说话者的情绪倾向与交际意图\cite{kendon_2004_gesture,mcneill_1992_hand}；%
  \item \textbf{指向相关（deictic）}动作通过转头或注视方向建立叙事空间的参照\cite{mcneill_1992_hand}。
\end{enumerate}

此外，研究表明头部动作的启动时间往往早于发声\cite{esteve2017timing}。
具体而言，头部动作存在启动与加速过程，若其峰值需与重读音节的时间对齐，则动作必须提前起势。%
因此，头部动作可能对即将到来的语音韵律具有前瞻性。%

这一特征揭示了头部动作与语音之间的时序关系，%
说明视觉模态中的运动信号有时可先于声学事件出现。%
本文研究也因此关注头部动作，并将其纳入输入模态。

\section{身体姿态的参数化表示}
手势作为身体运动的子集，其生成和识别依赖于身体姿态的连续建模。因此，在进一步讨论手势生成方法之前，在此明确身体姿态的参数化表示方式。

\paragraph{骨架结构的定义}
在计算机动画与动作捕捉领域，身体姿态通常由骨架结构和关节旋转参数共同定义。骨架结构描述了人体各关节的拓扑关系及层级依赖；而每个姿态帧由一组关节旋转参数所确定，这些参数定义了相对于父节点的旋转变换。在不同的系统与任务中，骨架结构的具体形式往往有所差异，这种差异直接影响姿态数据的表示与学习方式。

在不同的系统与任务中，骨架结构可以遵循各自的标准，因此，不同的数据集、3D模型或神经网络往往基于自身定义的关节层级与命名体系进行训练与标注。例如，AMASS数据集\cite{AMASS:2019}采用Skinned Multi-Person Linear model（SMPL）\cite{SMPL:2015}的拓扑结构，BEAT数据集\cite{beatcamn}使用自身定义的简化上半身骨架。近年来的自动骨骼绑定与骨架归一化方法，通过学习或优化关节对应关系，实现了不同拓扑之间的姿态重定向（pose retargeting）\cite{GleicherRetargetting1998,MartinelliSkeleton-AwareRetargeting2024}，从而消除了模型依赖于特定骨架结构的限制。

\paragraph {旋转参数的选取}
在确定骨架结构之后，具体的关节状态可通过多种旋转参数进行描述。
常见的旋转参数表示方法包括：
\begin {enumerate}
      \item 欧拉角（Euler Angles）：
      通过三个顺序旋转角表示姿态，直观且参数维度低（3 维），适合存储，但运算存在万向节锁（Gimbal Lock）问题，导致特定姿态下自由度丢失，且插值过程易产生非物理运动；
      \item 四元数（Quaternion）：
      以四维单位向量（\(q_0^2 + q_1^2 + q_2^2 + q_3^2 = 1\)）表示旋转，避免欧拉角的奇异性的同时，实现平滑插值。
      然而在神经网络回归中通常需要显式处理单位长度约束，且同一旋转存在双覆盖（$q$ 与 $-q$ 等价），优化不稳定。
      \item Axis-Angle：以旋转轴（三维单位向量）与旋转角度（标量）的乘积表示旋转，参数紧凑（3维），是参数化人体模型SMPL\cite{SMPL:2015}的主要姿态存储格式。
      但角度取值范围存在周期性（\([0, 2\pi)\)），导致参数空间不连续，影响神经网络学习；
      \item 旋转矩阵：
      以3x3的正交矩阵，表示坐标系或物体绕某个轴旋转一定角度后的新姿态。
      常用于计算机图形学、机器人学和物理仿真中，但正交约束（\(R^T R = I\)且\(\det(R) = 1\)）提高神经网络的学习难度。
      \item Rot6D\cite {rot6d}：将3×3旋转矩阵的前两列展开为六维向量，
      通过 Gram-Schmidt 正交化过程自动保持列向量的正交性与单位长度，无需额外约束。
      该表示既继承了旋转矩阵的数值稳定性，又解决了其参数冗余和正交性约束难题，同时具备连续性优势。
\end {enumerate}

在计算机图形与实时渲染中，通常采用四元数或旋转矩阵进行骨骼变换与插值，以保证数值稳定性和计算效率。
然而，在深度学习任务中，这些表示方式的约束会对训练效率造成影响：
四元数需满足单位长度约束；旋转矩阵则需满足元素的正交约束。
近年来的研究表明\cite {rot6d}，Rot6D既规避了上述两种表示的约束难题，又保持了姿态空间的连续性与物理合理性，
在动作生成与姿态预测任务中展现出更优的可学习性。

手势生成方法对旋转参数的选取经历了一个演变过程。早期工作采用的方法各异（如欧拉角、Axis-Angle），多受限于特定框架或历史因素；
而近期研究，出于对神经网络训练稳定性的追求，已显著趋向于采用Rot6D表示。
这一趋势的实例对比见表~\ref{fig:rotation_comparison}。

\begin{table}[htbp]
\bicaption{不同旋转表示方式的空间连续性与使用示例}{Spatial Continuity and Usage Examples of Different Rotation Representations}
\centering
\label{fig:rotation_comparison}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{表示方式} & \textbf{维度} & \textbf{连续性} & \textbf{典型应用场景} & \textbf{训练使用示例} \\ \midrule
欧拉角 & 3 & 存在万向节锁 & 姿态存储 & CaMN\cite{beatcamn} \\
四元数 & 4 & 连续，但存在单位长度约束 & 图形学 & — \\
Axis-Angle & 3 & 角度部分不连续 & 神经网络 & DiffSHEG\cite{diffsheg} \\
旋转矩阵 & 9 & 连续，但存在正交约束 & 图形学、机器人学 & MambaGesture\cite{fu2024mambagesture} \\
Rot6D & 6 & 连续 & 神经网络 & 近期多数\cite{emage, Ao2023GestureDiffuCLIP,AMUSE2024} \\ \bottomrule
\end{tabular}
\end{table}

因此，本文在姿态生成模型中统一使用Rot6D表示每个关节的旋转状态，以确保训练阶段的平滑收敛与推理阶段的稳定性。

\section{面部表情的定义与参数化表示}

面部表情是非语言交流的重要组成部分，与语音、手势共同传递情感和态度信息。与身体动作不同，面部表情主要由皮肤形变和局部运动构成，无法通过骨骼旋转直接建模，因此需要专门的参数化表示方式。

Facial Action Coding System（FACS）\cite{EkmanFriesenFACS1978}提出，复杂表情可以分解为若干可组合的基本动作单元（Action Units, AU），为将面部表情描述为一组可控参数提供了理论基础。
在计算机图形与动画领域，BlendShape 模型进一步将这一思想具体化：其通过一组可线性叠加的形变基来表示面部表情，
每个基形由对应的权重参数控制局部表情变化。通过调节多个基形的权重并进行组合，即可生成丰富的复杂表情，
这与 FACS 将表情拆分为动作单元并进行组合的思路相一致。

与基于骨骼的形变不同，BlendShape直接在顶点层面定义几何偏移量，因此对网格拓扑结构高度依赖：每个基形的顶点偏移需与基础网格逐点对应。在具有相同网格结构的角色模型之间，BlendShape集合可以直接复用；但若拓扑发生变化，偏移数据将无法一一对应，从而难以在不同模型间映射或重定向。这种拓扑依赖性限制了其跨模型的通用性。

尽管如此，BlendShape 在表现精细面部表情和软组织形变方面具有显著优势。其标准化权重接口、实时可驱动性与渲染兼容性，使其成为虚拟人和表情捕捉系统的主流表示形式，并被广泛应用于如Apple ARKit\cite{ARKitDocumentation} 等实时动画框架，以及主流渲染引擎中。

图~\ref{fig:au_sample}来自文献~\cite{ozel_arkit_facs_cheatsheet}，展示FACS对闭眼睛的动作单元AU45的定义。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Fig_AUSample.png}
\bicaption{FACS 中闭眼动作单元的定义\cite{ozel_arkit_facs_cheatsheet}}{Definition of Eye Closure Action Units in FACS\cite{ozel_arkit_facs_cheatsheet}}
\label{fig:au_sample}
\end{figure}

这在BlendShape中，一般对应两种基形：eyeBlinkLeft（闭左眼）与eyeBlinkRight（闭右眼）。图~\ref{fig:bs_eyeblink}展示了eyeBlinkLeft、eyeBlinkRight的ARKit BlendShape基形在权重$w\!\in\![0,1]$下的线性插值效果（从张眼到闭眼）。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Fig_blendshapeEyeBlink.png}
\bicaption{BlendShape 在闭眼形变上的线性插值效果}{Linear Interpolation of BlendShape Deformation for Eye Closure}
\label{fig:bs_eyeblink}
\end{figure}

\section{系统设计思路}

\subsection{手势生成目标的选择}
手势生成任务可根据其对语义上下文的依赖程度与时间结构的复杂度，
大致划分为以语义理解为核心的“语义驱动型生成”，
与以韵律对齐为核心的“韵律驱动型生成”。
本文面向用户实时数字人驱动场景，在严格因果与低延迟约束下，
模型无法访问未来语音或完整文本语义，因此难以稳定生成对语义推理要求较高的形象性、隐喻性与指示性手势。
基于该约束，本文将研究重点聚焦于与语音韵律高度同步的节奏型手势生成，并将其视为在实时交互条件下具有明确表达价值且可稳定建模的随语手势形式。

在此定位下，本文提出帧级多模态级联手势生成模型 FaceCapGes，输入为实时语音特征、面部 BlendShape 权重以及头部姿态旋转参数，并在不依赖未来帧信息的条件下逐帧输出上半身骨骼姿态。模型通过多模态信号融合弥补单一语音模态在实时场景下的信息不足：面部表情提供情绪与说话强度等辅助线索，头部姿态提供节奏前瞻与空间锚定信号，从而在保持可部署实时性的同时提升生成动作的自然度、同步性与方向一致性。

综合实时数字人驱动场景的交互需求与部署约束，本文系统设计遵循以下原则：

(1) 严格因果性： 模型仅使用当前与过去的多模态输入，不访问未来帧信息；

(2) 低延迟： 支持帧级在线推理，满足实时交互的时延要求；

(3) 可实时采集模态： 输入模态需能通过常见设备实时获取，包括语音、面部参数与头部姿态；

(4) 可稳定建模： 在上述约束下优先建模节奏型手势，并将其作为核心生成目标；

\subsection{引入头部姿态的动机与贡献}
从生成可行性的角度，现有研究普遍认为节奏型手势可在无语义理解的条件下由语音韵律直接驱动生成。
多数语音驱动手势研究证实，仅凭语音的能量、时长与音高变化即可合成自然的节奏性上肢动作\cite{ginosar2019speech2gesture,alexanderson2020stylegestures,kucherenko2021movingfastslow}。
这些研究所生成的动作在时间结构上与语音重音同步，体现了语音与手势共享的时间规划机制。

相比之下，iconic（形象性）、metaphoric（隐喻性）与deictic（指向性）手势均依赖语义或指向关系，
需要从上下文分析语义与语境，难以在严格实时的因果条件下生成。
而节奏相关特征在音频中则具有更高的可预测性。\cite{kucherenko2021predictability}
这表明，在缺乏未来语义与全局上下文的实时场景中，仅凭语音模态，模型只能稳定生成节奏层面的动作。

为突破这一限制，本文引入头部姿态模态作为补充输入信号。头部姿态能在实时因果条件下提供部分空间与时间线索：其转头与注视方向反映互动焦点，点头与抬头与语音重读共现，能够在不依赖未来语义信息的前提下，为手势生成提供弱先验约束。
这种模态扩展为实时系统提供了理论上的可行性基础，使模型能够在语音之外获得关于节奏、方向与视角的附加信息。

\subsubsection{头部姿态对手势预测的贡献}
头部动作在自然语音中常呈现出一定的时间前瞻性~\cite{esteve2017timing}：%
其启动往往早于对应韵律词的发声，%
这意味着视觉模态可能比声学信号更早反映语音节奏的变化趋势。%
这种时序特性为实时生成任务提供了潜在的预测窗口，%
使系统能够在语音节奏变化尚未显现前，就提前捕获相关的动态线索。%
因此，头部姿态在实时生成中不仅提供同步参考，也可能在时间上形成前驱信号，为手势节奏的自然启动提供时序优势。%

\subsubsection{头部姿态对空间锚定与视角一致性的贡献}
头部姿态模态为实时语音驱动的手势生成提供了关键的空间参照信号。%
其与语音韵律在时间组织上高度耦合。%
即使在无未来语义信息的条件下，头部的转向与注视变化仍能反映说话者的注意焦点与叙述方向，%
从而帮助模型在动作生成中保持空间的连贯性与方向一致性。%
这一机制使系统能够在时间与空间两个维度上同步对齐语音与动作，%
让生成的手势在视觉上更具互动感与表达意图。

在McNeill的四类手势体系中，头部姿态的引入主要强化了两类动作的生成：  

(1) 对beat手势而言，它为语音重读和节奏段落提供显式的时间协同信号，使手部与头部动作在韵律层面更加一致；  

(2) 对iconic手势而言，它在具有路径与方向特征的动作中提供空间参考，使模型能够在叙事空间中更稳定地确定动作的方位与轨迹方向。  

通过这两方面的强化，系统在保持实时性的同时获得了更自然的节奏衔接与空间表达。

与此同时，本文明确头部姿态模态的作用边界：其核心优势在于捕捉方向、焦点与时序节奏，而非手型语义或复杂形态描摹等细粒度语义特征。换言之，它主要改善手势的位置、方向与视角依附，而非手势的形状描绘或语义内容。对于依赖抽象语义或外指参照的metaphoric与deictic手势，仍需语言或上下文模态的补充。

总体而言，头部姿态为实时生成提供了介于韵律与语义之间的关键中层约束。%
其时间上的前瞻性与空间上的指向性共同帮助模型在低延迟条件下保持自然、连贯且空间协调的动作表现，%
从而在因果生成框架内有效拓展了语音驱动手势的可表达范围，并为节奏主导型动作的实时生成提供了结构的支持。

基于上述任务范围与模态设计原则，下一节将进一步给出本文系统的总体架构，并说明各模块在实时生成流程中的功能定位。

\section{在线实时手势生成系统整体框架}
\label{sec:system}
本节介绍整个系统的端到端驱动流程及模块职责划分。如图~\ref{fig:system_architecture}所示，系统整体架构由五个层级组成：用户配置层、设备层、中间件层、手势生成模型层以及渲染与驱动层。各层之间通过多模态信号接口进行连接，实现从信号采集到虚拟人动作生成的端到端实时处理。

FaceCapGes模型位于中间层，承担多模态输入到上半身姿态输出的核心推理任务，而输入采集与渲染模块分别负责信号获取与结果展示。

为实现基于语音、面部捕捉与头部姿态的实时数字人驱动系统，本文构建了完整的信号采集、动作生成与渲染展示的处理管线。FaceCapGes模型作为该系统的核心计算模块，负责在实时约束下从多模态输入推理出当前帧的上半身骨骼姿态。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/SystemArchitecture.png}
\bicaption{系统整体架构与数据流示意图}{System Overview and Data Flow Diagram}
\label{fig:system_architecture}
\end{figure*}

\subsection{信号采集与系统配置层}
该层位于系统整体架构的输入端，用于从用户端设备实时获取多模态信号，并在系统初始化阶段完成运行参数的配置。整体结构可划分为设备层、中间件层与用户配置层三个部分，如图~\ref{fig:system_architecture} 所示。

\paragraph{设备层}
设备层负责采集语音与视觉模态信号。语音信号由麦克风实时录制，采样率与帧移可根据运行设备性能调整；视觉信号由前置深度相机摄像头获取面部深度图与视频流，并作为ARKit面部追踪模块的输入。

\paragraph{中间件层}
中间件层通过Apple提供的ARKit框架\cite{ARKitDocumentation}，将设备层的原始图像流与深度图转化为结构化特征。ARKit输出两类主要数据：  
(1) \textbf{面部表情特征}
ARKit提供52维BlendShape系数向量，用于描述关键肌肉群的局部形变状态。该特征能够反映用户的表情、口型与情感变化，并以帧级形式同步输出。  
(2) \textbf{头部姿态特征} 
ARKit在ARFaceAnchor中提供一个齐次变换矩阵$\mathbf{T}\in\mathbb{R}^{4\times4}$，
用于描述人脸锚点相对会话世界坐标系的位姿：
\begin{equation}
\mathbf{T} =
\begin{bmatrix}
\mathbf{R} & \mathbf{t} \\
\mathbf{0}^{\top} & 1
\end{bmatrix},
\quad
\mathbf{R}\in\mathbb{R}^{3\times3},\ 
\mathbf{t}\in\mathbb{R}^{3\times1}.
\end{equation}
其中左上角的$\mathbf{R}$为旋转矩阵，右上角的$\mathbf{t}$为平移向量。
本研究从矩阵中提取旋转部分 $\mathbf{R}$，
并将其转换为Rot6D~\cite{rot6d}表示形式，
以提升旋转空间的连续性与模型训练的稳定性。

同时，音频流在中间件层中被传入特征提取模块以生成时间序列特征。模型训练阶段使用Librosa库离线提取Mel频谱、短时能量与基频$F_0$等声学特征，以保证特征精度与一致性。系统运行阶段可由等价的实时特征提取模块（如torchaudio或TensorFlow Audio）逐帧生成对应特征，以实现端到端的低延迟运行。

\paragraph{用户配置层}
用户配置层负责系统初始化阶段的模型与参数设定。用户可在应用中选择说话风格，对应加载不同说话人ID配置下的模型权重。该配置仅在系统启动时生效，不参与实时推理过程。

本层提供的多模态信号经中间件处理后，以统一的数据接口传递至手势生成模型，实现语音、表情与头部姿态的实时融合输入。

\subsection{手势生成模型层}

FaceCapGes 模块作为系统的核心推理单元，接收来自信号采集与系统配置层的三类输入特征：语音特征、面部BlendShape系数以及头部姿态参数，并在不依赖未来帧的条件下，逐帧预测用户当前时刻的上半身骨骼姿态。

生成的骨骼姿态采用Rot6D\cite{rot6d}连续旋转表示形式，覆盖上半身47个关节的旋转参数。模型内部通过级联多模态编码结构提取时序相关特征，并利用单向LSTM解码器完成时间依赖建模，从而在保持实时性的同时，生成与语音节奏、表情变化及头部朝向高度一致的自然手势。

FaceCapGes输出的姿态数据通过统一接口传递至渲染与驱动模块，与实时面部捕捉信号共同驱动虚拟角色的整体动作。由于模型仅依赖当前与历史帧输入，可与输入层以固定帧率并行运行，实现端到端的低延迟推理。

\subsection{渲染驱动层}

该模块位于系统输出端，负责将手势生成模型与面部捕捉结果共同转化为虚拟人的实时动作表现。
系统将FaceCapGes模型输出的上半身骨骼姿态与ARKit实时检测的52维面部BlendShape系数传递至渲染引擎，
由引擎内的模块解析并映射至目标虚拟人的骨骼与表情控制接口，从而实现多模态动作驱动。

渲染模块采用基于GPU的蒙皮计算与实时光照模型，以确保动画的平滑性和视觉一致性。
最终，系统能够在实时流式输入条件下稳定运行，
同步呈现语音、表情与身体动作，
以自然流畅的数字人形象实现从多模态信号输入到可视化输出的完整驱动流程。

\section{本章小结}
本章围绕本文面向在线实时数字人驱动场景的多模态手势生成任务，
给出了必要的概念定义、输入模态表示与系统层面的总体框架设计。
首先，本章基于 Kendon 连续体与 McNeill 的随语手势分类体系，
对本文研究对象的手势类型进行界定，并结合严格因果与低延迟约束，
明确以节奏型手势作为可稳定建模的核心生成目标，为后续模型设计限定任务范围。
随后，本章介绍了身体姿态与面部表情在动作生成任务中的参数化表示方式：
姿态部分统一采用连续旋转表示 Rot6D，以保证训练与推理过程中的数值稳定性；
面部表情部分采用 BlendShape 权重表示，并说明其与 ARKit 标准接口的对应关系。

在系统设计思路方面，本章进一步论证了引入头部姿态模态的必要性与贡献：
头部姿态在实时因果条件下可提供节奏前瞻与空间锚定信号，
从而弥补仅依赖语音模态时对方向一致性与互动焦点感知不足的问题，
并在保持可部署实时性的前提下拓展手势生成的可表达范围。
最后，本章给出了端到端在线实时系统的整体架构与模块职责划分，
说明从多模态信号采集、特征转换到 FaceCapGes 推理与渲染驱动的完整数据流组织方式，
为后续章节的模型结构设计与训练策略提供系统背景。

下一章将进一步详细介绍 FaceCapGes 的多模态级联模型架构，包括语音、面部与头部姿态特征的编码方式、融合策略以及面向上半身姿态的层次化解码过程。

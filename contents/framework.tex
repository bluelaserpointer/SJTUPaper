\chapter{基于语音、面部、头部姿态的在线实时手势生成系统}

\section{研究定位与总体设计思路}
手势可根据语义依赖性与时间结构复杂度区分为可语义生成与可韵律生成两大类。
本文的研究聚焦于严格实时的语音驱动任务，在此条件下模型无法访问未来语音或完整语义，
因此重点生成与语音韵律同步的节奏型手势，
并通过面部与头部模态的联合输入进一步增强表达性与自然度。

基于上述定位，本文提出了一种基于音频、面部BlendShape权重和头部姿态输入的
帧级多模态级联手势生成模型FaceCapGes。
模型旨在在实时条件下实现自然、同步且具有一定指向性的上半身动作生成，
在不依赖语义理解或未来上下文的前提下，
通过多模态输入弥补语音模态预测能力的不足。

为避免与系统级说明混淆，本文在第~\ref{sec:system}~节给出端到端系统框架与各模块的功能划分；
在第~\ref{sec:problem}~节形式化定义任务目标、输入输出模态与符号体系；
在第~\ref{sec:cascade}~节详细介绍模型的级联结构、模态编码方式；
在第~\ref{sec:realtime}~节详细介绍滑动窗口、自回归训练等实时适配策略；
在第~\ref{sec:head_encoder}~节介绍头部姿态的引入方法与编码器；
在第~\ref{sec:architecture}~节展示模型的整体结构图；
最后在第~\ref{sec:implementation}~节说明实现细节与训练配置。

\section{系统整体框架与模块定位}
\label{sec:system}
本节介绍整个系统的端到端驱动流程及模块职责划分。如图~\ref{fig:system_architecture}所示，系统整体架构由五个层级组成：用户配置层、设备层、中间件层、手势生成模型层以及渲染与驱动层。各层之间通过多模态信号接口进行连接，实现从信号采集到虚拟人动作生成的端到端实时处理。

FaceCapGes模型位于中间层，承担多模态输入到上半身姿态输出的核心推理任务，而输入采集与渲染模块分别负责信号获取与结果展示。

为实现基于语音、面部捕捉与头部姿态的实时数字人驱动系统，本文构建了完整的信号采集、动作生成与渲染展示的处理管线。FaceCapGes模型作为该系统的核心计算模块，负责在实时约束下从多模态输入推理出当前帧的上半身骨骼姿态。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/SystemArchitecture.png}
\bicaption{系统整体架构与数据流示意图}{System Overview and Data Flow Diagram}
\label{fig:system_architecture}
\end{figure*}

\subsection{信号采集与系统配置层}
该层位于系统整体架构的输入端，用于从用户端设备实时获取多模态信号，并在系统初始化阶段完成运行参数的配置。整体结构可划分为设备层、中间件层与用户配置层三个部分，如图~\ref{fig:system_architecture} 所示。

\paragraph{设备层}
设备层负责采集语音与视觉模态信号。语音信号由麦克风实时录制，采样率与帧移可根据运行设备性能调整；视觉信号由前置深度相机摄像头获取面部深度图与视频流，并作为ARKit面部追踪模块的输入。

\paragraph{中间件层}
中间件层通过Apple提供的ARKit框架\cite{ARKitDocumentation}，将设备层的原始图像流与深度图转化为结构化特征。ARKit输出两类主要数据：  
(1) \textbf{面部表情特征}
ARKit提供52维BlendShape系数向量，用于描述关键肌肉群的局部形变状态。该特征能够反映用户的表情、口型与情感变化，并以帧级形式同步输出。  
(2) \textbf{头部姿态特征} 
ARKit在ARFaceAnchor中提供一个齐次变换矩阵$\mathbf{T}\in\mathbb{R}^{4\times4}$，
用于描述人脸锚点相对会话世界坐标系的位姿：
\begin{equation}
\mathbf{T} =
\begin{bmatrix}
\mathbf{R} & \mathbf{t} \\
\mathbf{0}^{\top} & 1
\end{bmatrix},
\quad
\mathbf{R}\in\mathbb{R}^{3\times3},\ 
\mathbf{t}\in\mathbb{R}^{3\times1}.
\end{equation}
其中左上角的$\mathbf{R}$为旋转矩阵，右上角的$\mathbf{t}$为平移向量。
本研究从矩阵中提取旋转部分 $\mathbf{R}$，
并将其转换为Rot6D~\cite{rot6d}表示形式，
以提升旋转空间的连续性与模型训练的稳定性。

同时，音频流在中间件层中被传入特征提取模块以生成时间序列特征。模型训练阶段使用Librosa库离线提取Mel频谱、短时能量与基频$F_0$等声学特征，以保证特征精度与一致性。系统运行阶段可由等价的实时特征提取模块（如torchaudio或TensorFlow Audio）逐帧生成对应特征，以实现端到端的低延迟运行。

\paragraph{用户配置层}
用户配置层负责系统初始化阶段的模型与参数设定。用户可在应用中选择说话风格，对应加载不同说话人ID配置下的模型权重。该配置仅在系统启动时生效，不参与实时推理过程。

本层提供的多模态信号经中间件处理后，以统一的数据接口传递至手势生成模型，实现语音、表情与头部姿态的实时融合输入。

\subsection{手势生成模型层（FaceCapGes）}

FaceCapGes模块位于系统的中间层，是本文提出的核心计算单元。该模块接收来自信号采集与系统配置层的三类输入特征：语音特征、面部BlendShape系数以及头部姿态参数，并在不依赖未来帧的条件下，逐帧预测用户当前时刻的上半身骨骼姿态。

生成的骨骼姿态采用Rot6D\cite{rot6d}连续旋转表示形式，覆盖上半身47个关节的旋转参数。模型内部通过级联多模态编码结构提取时序相关特征，并利用单向LSTM解码器完成时间依赖建模，从而在保持实时性的同时，生成与语音节奏、表情变化及头部朝向高度一致的自然手势。

FaceCapGes输出的姿态数据通过统一接口传递至渲染与驱动模块，与实时面部捕捉信号共同驱动虚拟角色的整体动作。由于模型仅依赖当前与历史帧输入，可与输入层以固定帧率并行运行，实现端到端的低延迟推理。

\subsection{渲染驱动层}

该模块位于系统输出端，负责将手势生成模型与面部捕捉结果共同转化为虚拟人的实时动作表现。
系统将FaceCapGes模型输出的上半身骨骼姿态与ARKit实时检测的52维面部BlendShape系数传递至渲染引擎，
由引擎内的模块解析并映射至目标虚拟人的骨骼与表情控制接口，从而实现多模态动作驱动。

渲染模块采用基于GPU的蒙皮计算与实时光照模型，以确保动画的平滑性和视觉一致性。
最终，系统能够在实时流式输入条件下稳定运行，
同步呈现语音、表情与身体动作，
以自然流畅的数字人形象实现从多模态信号输入到可视化输出的完整驱动流程。
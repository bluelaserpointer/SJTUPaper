\chapter{虚拟人驱动的多模态在线实时手势生成框架}
\label{chap:framework}

本文面向的应用场景是低门槛的单设备虚拟人驱动：
系统在用户侧仅依赖一台具备麦克风与前向摄像头的通用设备（如个人电脑或智能手机），
即可实时获取语音信号以及由面部视频解析得到的面部参数与头部姿态，
并进一步生成与语音表达一致的上半身手势序列用于虚拟人渲染。
围绕这一场景，本章首先给出需求分析，明确端到端框架在在线实时处理与多模态输入方面需要具备的能力，
并据此推导训练数据应满足的基本信息条件；随后将进入数据集结构、表示与预处理，以及端到端架构设计的具体分析。

\section{需求分析}
\label{sec:requirement_analysis_v2}

本文拟设计的端到端框架需要在在线环境中持续接收用户侧的多模态信号流，
并在低延迟约束下输出可驱动虚拟人的手势运动。
本文将输入侧约束为通用单设备条件，即系统仅依赖麦克风与前向摄像头即可完成所需信号的采集与解析。
在该设定下，系统能够稳定获得以下输入信息：  

（1）语音信号（Audio）：
通过设备麦克风实时采集，为后续的语音特征提取提供原始波形输入。

（2）面部参数（Expression Parameters）与头部姿态（Head Pose）：
通过设备前向摄像头获取面部视频序列，并借助面部捕捉工具对视频进行解析，输出结构化的面部表情参数以及头部三维姿态信息。

基于上述信号获取方式，本文端到端框架需要具备的核心能力可概括为：
（1）在线实时处理流：
系统应面向连续输入流工作，支持随时间推进的增量式推理与输出，并满足交互场景所需的低延迟要求。

（2）多模态条件生成：
模型需要同时接受语音、面部参数与头部姿态三种输入模态，并学习它们与上半身手势运动之间的映射关系，
从而生成与语音表达一致且具有非语言协同信息的动作序列。

（3）实时虚拟人渲染：
根据用户的面部参数与模型生成的实时动作，驱动虚拟人的姿态。

为训练满足上述能力的端到端模型，训练数据应在时间轴上提供与推理阶段一致的监督信号与条件信息。具体而言，数据集至少需要包含：

（1）语音信号：
用于提取语音内容与韵律特征，作为动作生成的主要时间驱动与语义线索。

（2）面部参数：
与推理阶段的面捕输出形式一致，用于向模型提供表情与语气相关的条件信息。

（3）人体动捕：
作为手势生成的监督目标，提供骨架运动。

此外，头部姿态不需要由数据集显式给出：
在许多动捕数据中，头部相关骨骼或关节的旋转信息已经包含在人体运动序列内，
因此头部姿态可以从动捕数据中计算得到，并与语音与面部参数对齐后作为模型输入条件。

综上，本节以单设备可获取信号为约束，明确了端到端实时框架的输入形式与能力要求，
并给出了训练数据所需包含的基本信息条件。
基于这些条件，后续小节将进一步分析训练数据的结构与表示方式。

\section{训练数据来源}
为在训练阶段引入面部模态监督，本文采用 BEAT 数据集~\cite{beatcamn}。
BEAT 是面向虚拟角色驱动的多模态动作数据集，核心优势在于同时提供语音、上半身动捕以及面部表情捕捉等信息，
并包含丰富的演讲人数据。

通常，模型的骨架拓扑与面部参数化方式需要与训练数据保持一致。
因此，本节将说明 BEAT 中使用的骨架拓扑结构与面部表情表示方式。

\subsection{骨架拓扑}
\label{subsec:skeleton_topology}

虚拟人身体姿态通常采用层次化骨骼结构表示，
其中每个关节节点通过父子关系构成一棵运动学树，
并通过关节的局部旋转描述其在三维空间中的姿态。
骨架拓扑决定了系统中可驱动的关节集合与其运动学约束方式。

具体而言，设骨架包含$J$个关节节点，
记关节集合为$\mathcal{J}=\{1,2,\dots,J\}$，
每个关节$j\in\mathcal{J}$具有唯一的父节点$p(j)$，
从而定义出一棵有根树结构。
系统在每一帧$t$输出该骨架上所有关节的局部旋转$\mathbf{r}^{(j)}_t$，从而构成虚拟人身体姿态的完整描述$\bm{v}^{B}_t$，形式化定义为：

\begin{equation}
\bm{v}^{B}_t = \left[\mathbf{r}^{(1)}_t,\mathbf{r}^{(2)}_t,\dots,\mathbf{r}^{(J)}_t\right]\in \mathbb{R}^{R \times J}
\label{eq:body_pose_vector}
\end{equation}

其中，$R$表示旋转表示的维度。

BEAT 数据集的标准骨架结构如图~\ref{fig_beatbones} 所示，共包含人体中的 47 个关节节点，即$J=47$，
包括上肢及躯干的三个主要控制点（蓝色区域所示），下肢关节则保持静态。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.15\textwidth]{figures/Fig_BEATBones.png}
\bicaption{BEAT 数据集的骨架拓扑结构与驱动范围}{Skeleton Topology and Actuated Joint Range in the BEAT Dataset}
\label{fig_beatbones}
\end{figure}

\subsection{头部姿态参数}

本文仅使用头部旋转信息作为头部姿态特征，不引入头部位置信息。

这是因为，头部位置更容易受到身体姿态变化的影响。
以 BEAT\cite{beatcamn} 为例，该演讲数据集中的演讲者多为站姿录制，单次演讲时长较长（约 1 分钟），
过程中可能出现轻微的重心移动等站姿调整；
而在目标应用场景中，用户交互姿态可能包含坐姿，其身体活动方式与站姿存在差异，例如不易产生长时间站立带来的重心偏移。

因此，直接建模头部位置或位移可能引入额外的场景依赖性，从而削弱跨场景的泛化能力。基于上述考虑，本文仅采用头部旋转作为头部姿态输入特征。

BEAT数据集没有单独的头部姿态参数。
我们可以在预处理中利用骨架层级关系，
沿骨架链路从根节点到头部关节的相对旋转进行旋转姿态的叠加，
从而得到头部在全局坐标系下的绝对旋转表示。

设头部关节索引为 $h$，根关节为 $r$。
记局部旋转 $\mathbf{r}_t^{(j)}$ 对应的旋转矩阵为
$\mathbf{R}_t^{(j)}=\Phi(\mathbf{r}_t^{(j)})\in SO(3)$，
其中 $\Phi(\cdot)$ 表示从所选旋转表示到旋转矩阵的映射。
则关节 $j$ 的全局旋转 $\mathbf{G}_t^{(j)}\in SO(3)$ 可递推定义为：
\begin{equation}
\mathbf{G}_t^{(r)} = \mathbf{R}_t^{(r)},\qquad
\mathbf{G}_t^{(j)} = \mathbf{G}_t^{(p(j))}\mathbf{R}_t^{(j)}
\label{eq:global_rotation_recursion}
\end{equation}
于是头部的绝对旋转为：
\begin{equation}
\mathbf{G}_t^{(h)} =
\prod_{k\in \mathcal{P}(r\rightarrow h)} \mathbf{R}_t^{(k)},
\label{eq:head_global_rotation_product}
\end{equation}
其中 $\mathcal{P}(r\rightarrow h)$ 表示从根关节到头部关节的有序关节序列（包含 $r$ 与 $h$）。

最后，将头部全局旋转再映射回与模型一致的旋转表示，得到头部姿态特征：
\begin{equation}
\bm{v}^{H}_t = \Psi\!\left(\mathbf{G}_t^{(h)}\right)\in\mathbb{R}^{R},
\label{eq:head_pose_vector}
\end{equation}
其中 $\Psi(\cdot)$ 为从旋转矩阵到所选旋转表示的映射，$R$ 为该表示的维度。

\subsection{面部参数}
\label{sec:facial_expression_representation}

为了在手势生成系统中实现对表情状态的采集、训练与渲染驱动，
通常需要将面部形变定义为一组可控且可实时更新的参数向量，
并使其能够稳定地映射到虚拟人网格模型的顶点形变上。

在 BEAT 数据集中，面部参数表示为一组基于iOS设备的面捕工具Apple ARKit\cite{ARKitDocumentation} 的 BlendShape 系数向量。
该向量的每一维对应一个 BlendShape 通道，并以标量权重刻画局部面部形变的强度，多个通道的线性组合可生成复杂表情。

其思想来源于Facial Action Coding System（FACS）\cite{EkmanFriesenFACS1978}，
将复杂表情分解为若干可组合的基本动作单元（Action Units, AU），
从而为将面部表情描述为一组可控参数。

BlendShape没有固定的规范，动捕软件、三维人物模型可自定义自己的形变基组合，
但通常只有基于相同规范的系统间才可以复用面部参数。

以闭眼动作为例，FACS将闭上双眼设为一个动作单元，如图~\ref{fig:au_sample}所示。
\begin{figure}[h!t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Fig_AUSample.png}
\bicaption{FACS 中闭眼动作单元的定义\cite{ozel_arkit_facs_cheatsheet}}{Definition of Eye Closure Action Units in FACS\cite{ozel_arkit_facs_cheatsheet}}
\label{fig:au_sample}
\end{figure}

在BEAT数据集使用的Apple ARkit\cite{ARKitDocumentation}定义的BlendShape标准中，上述动作对应两种基形：闭左眼与闭右眼。
图~\ref{fig:bs_eyeblink}展示了两种的ARKit BlendShape基形在权重$w\!\in\![0,1]$下的线性插值效果（从张眼到闭眼），
反映了BlendShape通过权重控制局部形变的基本机制。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Fig_blendshapeEyeBlink.png}
\bicaption{BlendShape 在闭眼形变上的线性插值效果}{Linear Interpolation of BlendShape Deformation for Eye Closure}
\label{fig:bs_eyeblink}
\end{figure}

设面部参数包含$K$个形变基，
记形变基集合为$\mathcal{K}=\{1,2,\dots,K\}$，
系统在每一帧$t$输出每个形变基权重$\mathbf{w}^{(k)}_t\in\![0,1]$，从而构成虚拟人面部的完整描述，形式化定义为：

\begin{equation}
\bm{v}^{F}_t = \left[\mathbf{w}^{(1)}_t,\mathbf{w}^{(2)}_t,\dots,\mathbf{w}^{(K)}_t\right]\in \mathbb{R}^{K}
\label{eq:face_vector}
\end{equation}

在Apple ARKit中，存在52个形变基，即$K=52$。

\section{旋转参数的选取}

身体姿态的表示形式在训练阶段应具备良好的可学习性，主要体现在：
（1）参数空间连续、不出现数值不连续或突变；
（2）表示本身约束简洁，避免依赖额外的归一化或复杂的后处理。

因此，我们需要将BEAT数据集的原始旋转表示（欧拉角）在预处理时进行转换。

常见的旋转表示包括：
欧拉角（Euler Angles）、轴角表示（Axis-Angle）、四元数（Quaternion）、旋转矩阵（Rotation Matrix），
以及近年来在神经网络回归任务中广泛使用的连续旋转表示，如$6$维旋转表示（Rot6D\cite{rot6d}）。
本文主要比较以下几种形式在工程系统中的适用性：

\begin{itemize}
    \item 欧拉角：
    表达直观、维度低，便于存储；
    但存在万向节锁（gimbal lock），且在角度接近边界时表现为不连续，
    容易导致网络输出在临界角附近产生跳变，影响实时动画的平滑性。
    \item 轴角表示：
    可用三维向量刻画旋转，但在接近零旋转时数值不稳定；
    同时角度的周期性仍会引入不连续，对回归学习不利。
    \item 四元数：
    可避免万向节锁，但需要满足单位范数约束，通常需在网络输出后进行归一化；
    此外四元数存在符号二义性（$\mathbf{q}$ 与 $-\mathbf{q}$ 表示同一旋转），可能造成监督目标不一致。
    \item 旋转矩阵：
    表达完备且无二义性，但输出必须满足正交约束与 $\det(\mathbf{R})=1$ 的 $\mathrm{SO}(3)$ 约束；
    直接回归时往往需要额外的投影或正交化步骤以保证有效性。
\end{itemize}

综合考虑网络学习的稳定性、输出连续性与工程部署便利性，
本文采用Rot6D\cite{rot6d}作为关节局部旋转的统一表示形式。
Rot6D通过取旋转矩阵的前两列向量（记为$\mathbf{a},\mathbf{b}\in\mathbb{R}^3$）
组成一个$6$维向量，并通过Gram-Schmidt正交化恢复完整的旋转矩阵，
从而避免了显式的正交约束回归问题。
具体而言，设模型输出的 Rot6D 为
$\mathbf{r}=[\mathbf{a};\mathbf{b}]\in\mathbb{R}^6$，
其中$[\mathbf{a};\mathbf{b}]$ 表示将 $\mathbf{a}$ 与 $\mathbf{b}$ 进行纵向拼接得到的六维向量。
则可以按如下方式恢复旋转矩阵 $\mathbf{R}\in\mathbb{R}^{3\times 3}$：

\begin{align}
\mathbf{u}_1 &= \frac{\mathbf{a}}{\|\mathbf{a}\|} \\
\mathbf{u}_2 &= \frac{\mathbf{b} - (\mathbf{u}_1^\top\mathbf{b})\mathbf{u}_1}{\left\|\mathbf{b} - (\mathbf{u}_1^\top\mathbf{b})\mathbf{u}_1\right\|} \\
\mathbf{u}_3 &= \mathbf{u}_1 \times \mathbf{u}_2 \\
\mathbf{R} &= [\mathbf{u}_1,\mathbf{u}_2,\mathbf{u}_3]
\label{eq:rot6d_conversion}
\end{align}

该表示的主要优势体现在：

(1) 连续性强，避免角度周期性造成的跳变；

(2) 无需显式单位范数或正交约束回归，优化过程更稳定；

(3) 可在推理后确定性地恢复旋转矩阵或转换为四元数，便于下游渲染引擎消费。

因此，本文采用Rot6D作为动作生成模型的训练目标。

根据式\eqref{eq:body_pose_vector}，训练集中的身体姿态表示为：$\bm{v}^{B}_t \in \mathbb{R}^{6 \times 47}$

又根据式\eqref{eq:head_pose_vector}，训练集中的头部姿态表示为：$\bm{v}^{H}_t \in \mathbb{R}^{6}$

\section{在线实时手势生成系统整体框架}
\label{sec:system}
本节介绍整个系统的端到端驱动流程及模块职责划分。如图~\ref{fig:system_architecture}所示，系统整体架构由五个层级组成：用户配置层、设备层、中间件层、手势生成模型层以及渲染与驱动层。各层之间通过多模态信号接口进行连接，实现从信号采集到虚拟人动作生成的端到端实时处理。

FaceCapGes模型位于中间层，承担多模态输入到上半身姿态输出的核心推理任务，而输入采集与渲染模块分别负责信号获取与结果展示。

为实现基于语音、面部捕捉与头部姿态的实时数字人驱动系统，本文构建了完整的信号采集、动作生成与渲染展示的处理管线。FaceCapGes模型作为该系统的核心计算模块，负责在实时约束下从多模态输入推理出当前帧的上半身骨骼姿态。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/SystemArchitecture.png}
\bicaption{系统整体架构与数据流示意图}{System Overview and Data Flow Diagram}
\label{fig:system_architecture}
\end{figure*}

\subsection{信号采集与系统配置层}
该层位于系统整体架构的输入端，用于从用户端设备实时获取多模态信号，并在系统初始化阶段完成运行参数的配置。整体结构可划分为设备层、中间件层与用户配置层三个部分，如图~\ref{fig:system_architecture} 所示。

\paragraph{设备层}
设备层负责采集语音与视觉模态信号。语音信号由麦克风实时录制，视觉信号由前置深度相机摄像头获取面部深度图与视频流，并作为ARKit面部追踪模块的输入。

\paragraph{中间件层}
中间件层通过Apple提供的ARKit框架\cite{ARKitDocumentation}，将设备层的原始图像流与深度图转化为结构化特征。ARKit输出两类主要数据：  
\begin{enumerate}
    \item 面部表情特征：
ARKit提供52维BlendShape系数向量，用于描述关键肌肉群的局部形变状态。该特征能够反映用户的表情、口型与情感变化，并以帧级形式同步输出。  
    \item 头部姿态特征：
ARKit在ARFaceAnchor中提供一个齐次变换矩阵$\mathbf{T}\in\mathbb{R}^{4\times4}$，
用于描述人脸锚点相对会话世界坐标系的位姿：
\begin{equation}
\mathbf{T} =
\begin{bmatrix}
\mathbf{R} & \mathbf{t} \\
\mathbf{0}^{\top} & 1
\end{bmatrix},
\quad
\mathbf{R}\in\mathbb{R}^{3\times3},\ 
\mathbf{t}\in\mathbb{R}^{3\times1}
\end{equation}
其中左上角的$\mathbf{R}$为旋转矩阵，右上角的$\mathbf{t}$为平移向量。
本研究从矩阵中提取旋转部分 $\mathbf{R}$，
并将其转换为Rot6D~\cite{rot6d}表示形式，
以提升旋转空间的连续性与模型训练的稳定性。
\end{enumerate}

同时，音频流在中间件层中被传入特征提取模块以生成时间序列特征。模型训练阶段使用Librosa库\cite{librosa}离线提取Mel频谱、短时能量与基频$F_0$等声学特征，以保证特征精度与一致性。系统运行阶段可由等价的实时特征提取模块（如PyAudio\cite{pyaudio}）逐帧生成对应特征，以实现端到端的低延迟运行。

\paragraph{用户配置层}
用户配置层负责系统初始化阶段的模型与参数设定。用户可在应用中选择说话风格，对应加载不同说话人ID配置下的模型权重。该配置仅在系统启动时生效，不参与实时推理过程。

本层提供的多模态信号经中间件处理后，以统一的数据接口传递至手势生成模型，实现语音、表情与头部姿态的实时融合输入。

\subsection{手势生成模型层}

FaceCapGes 模块作为系统的核心推理单元，接收来自信号采集与系统配置层的三类输入特征：语音特征、面部BlendShape系数以及头部姿态参数，并在不依赖未来帧的条件下，逐帧预测用户当前时刻的上半身骨骼姿态。

生成的骨骼姿态采用Rot6D\cite{rot6d}连续旋转表示形式，覆盖上半身47个关节的旋转参数。模型内部通过级联多模态编码结构提取时序相关特征，并利用单向LSTM解码器完成时间依赖建模，从而在保持实时性的同时，生成与语音节奏、表情变化及头部朝向高度一致的自然手势。

FaceCapGes输出的姿态数据通过统一接口传递至渲染与驱动模块，与实时面部捕捉信号共同驱动虚拟角色的整体动作。由于模型仅依赖当前与历史帧输入，可与输入层以固定帧率并行运行，实现端到端的低延迟推理。

\subsection{渲染驱动层}

该模块位于系统输出端，负责将手势生成模型与面部捕捉结果共同转化为虚拟人的实时动作表现。
首先，场景内有一个虚拟人3D模型，其顶点网络数据具有骨骼绑定与面部BlendShape绑定。
系统将FaceCapGes模型输出的上半身骨骼姿态与ARKit实时检测的52维面部BlendShape系数传递至渲染引擎，
由引擎内的模块解析并映射至目标虚拟人的骨骼与表情控制接口，从而实现多模态动作驱动。

\subsubsection{虚拟人骨骼蒙皮渲染}
\label{subsec:skinning_and_retargeting}

虚拟人角色一般由可渲染的三维网格$\mathcal{M}$与其绑定的骨骼结构$\mathcal{S}$组成。
骨骼结构定义了关节的层级与运动学关系，而网格则提供可见的表面几何形态。
在骨骼蒙皮渲染中，每个网格顶点$\mathbf{x}$会被赋予若干骨骼关节的影响权重，
从而使得关节运动能够以连续方式驱动角色表面产生形变。

需要指出的是，由于不同虚拟人模型在骨骼命名与比例结构上可能存在差异，
本文在Unity\cite{unity}渲染阶段使用Mecanim提供的人形骨架映射机制\cite{unitymechanim}，
将本文输出骨架与目标角色骨架进行自动匹配，从而实现动作的重定向与稳定播放。
该过程确保了本文定义的统一骨架拓扑能够在不同角色模型间保持一致的驱动效果，
并将动作生成模块与渲染资产解耦，提高了系统的可扩展性。

\subsubsection{虚拟人面部表情驱动}

与骨骼蒙皮渲染类似，虚拟人角色有与三维网络绑定的各形变基的顶点位移。
设虚拟人面部基础网格的顶点集合为$\bm{v}_0 \in \mathbb{R}^{P \times 3}$，
其中$P$为顶点数；给定$K$个BlendShape基形，其对应的顶点偏移为$\Delta \bm{v}_i \in \mathbb{R}^{P \times 3}$（$i=1,\dots,K$）。
则时间帧$t$下的面部表情网格可表示为：
\begin{equation}
\bm{v}(t) = \bm{v}_0 + \sum_{i=1}^{K} w_t^{(i)}\Delta \bm{v}_i,
\label{eq:blendshape}
\end{equation}
其中$w_t^{(i)}\in[0,1]$为第$i$个基形在帧$t$的权重系数。
式\eqref{eq:blendshape}表明，BlendShape可以视为对基础网格的一组线性形变叠加，
因此其具有良好的实时可驱动性与渲染兼容性，非常适合用于表情控制。

最终，系统能够在实时流式输入条件下稳定运行，
同步呈现语音、表情与身体动作，
以自然流畅的数字人形象实现从多模态信号输入到可视化输出的完整驱动流程。

\section{本章小结}
本章围绕在线实时数字人驱动场景的多模态手势生成任务，从系统约束出发给出了输入信号形式、数据来源与参数表示方式，并提出端到端实时系统的总体框架。首先，本章在单设备条件下明确系统需要持续接收语音信号与视觉解析得到的面部参数、头部姿态，并在低延迟约束下输出可驱动虚拟人的上半身骨骼动作。 
随后，本章基于 BEAT 数据集说明训练数据的模态构成与同步关系，并分别给出骨架拓扑与面部 BlendShape 参数化表示；同时补充头部姿态数据的获取方式。
在动作表示方面，本章选择 Rot6D 作为身体关节旋转的统一表示，以提升训练与推理的数值稳定性并便于下游渲染。 
最后，本章给出了端到端在线实时系统的分层架构与数据流组织方式，说明从多模态采集、特征转换到 FaceCapGes 推理与渲染驱动的完整链路。

下一章将进一步详细介绍 FaceCapGes 的多模态级联模型架构，包括语音、面部与头部姿态特征的编码方式、融合策略以及面向上半身姿态的层次化解码过程。

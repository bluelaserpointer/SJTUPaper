% !TEX root = ../main.tex

\begin{abstract}[zh]

手势是交流中重要的细节补充与情感表达载体。面向虚拟人/数字形象的肢体控制中，传统方案往往依赖穿戴式检测设备或其他专用传感器，以稳定获取人体运动信息，但这类设备带来较高的使用门槛与部署成本。

随着图像识别技术的发展，计算机能够通过相机识别用户动作并实现无穿戴的动捕式交互，在一定程度上降低了设备负担。然而，这类方案仍要求用户在镜头前实际表演手势动作；在远程交流或沉浸式场景中，手势作为情感表达工具往往伴随空间占用与体力消耗，人们需要投入较多精力活动肢体以增强说服力或表达效果，从而使虚拟世界中的手势表达依然显得繁琐。

近年来，手势与语言的关系被进一步分析，使得计算机能够从文本或语音生成与语义一致的手势动画，让数字人物在发言中融入更符合人类交流习惯的非语言表达。若将该技术用于用户的实时数字形象，则有望提供更轻便的手势控制方案：用户无需实际做出手势，只需在相机前说话即可，同时自然地产生语音、面部表情与头部旋转等信息。面部表情可补充情感线索，头部旋转可为手势朝向与叙事空间锚定提供指导，从而改进依赖空间方位与路径方向的手势推理；且这些信息的实时获取技术已相对成熟，具备良好的在线性能。

然而，当前协同语音手势生成技术通常以获取完整文本为前提，通过语义分析生成高精度手势，难以直接适配逐字输入的用户语音实时场景。因此，本论文提出FaceCapGes方法，基于语音、面部表情与头部旋转三种实时信息生成在线实时的3D手势骨骼动画。该方法可为用户的实时数字形象添加手势动画，无需用户实际做出手势动作；同时模型不依赖未来输入，能够在实时环境中提升虚拟形象的表达能力。

以往研究已对面部表情、语音与手势的关系进行分析，并提出成熟的学习方法。本模型在现有级联架构基础上，增加头部姿态特征分析模块，同时引入滑动窗口机制以实现架构的实时运行。本模型所依赖的框架及额外添加模块性能开销低，与面部捕捉和头部姿态计算任务同时运行时，仍具备良好的实时性能。

主观评价结果表明，在自然性方面，本方法与当前主流方法相当；在响应速度上，具有显著优势。生成的手势与语音高度对齐，且具备良好的实时交互表现。此外，本模型可部署在 iPhone 等轻量设备上，只要输入格式兼容 ARKit 面部捕捉标准，就能广泛应用于各类实时互动场景。

\end{abstract}

\begin{abstract}[en]

Gestures are a crucial channel for enriching communication with complementary details and emotional expression. In the control of virtual humans or digital avatars, traditional approaches often rely on wearable sensing devices or other dedicated sensors to reliably capture human motion; however, such hardware introduces substantial barriers to use and deployment costs.

With advances in visual recognition, camera-based systems can capture user movements and enable markerless, mocap-style interaction, partially reducing the dependence on wearables. Nevertheless, these approaches still require users to physically perform gestures in front of the camera. In remote communication or immersive scenarios, expressive gesturing often entails non-trivial spatial and physical effort: users must invest energy in moving their limbs to strengthen persuasion or convey emotions, which can make gestural expression in virtual worlds cumbersome.

Meanwhile, growing understanding of the relationship between gestures and language has enabled computers to generate semantically consistent gestures from text or speech, allowing digital characters to incorporate nonverbal behaviors that better match human conversational habits. If applied to a user’s real-time avatar, such techniques could provide a more lightweight control paradigm: instead of performing gestures, users only need to speak in front of a camera, naturally producing speech, facial expressions, and head rotations. Facial expressions can supplement affective cues, while head rotations can guide gesture orientation and narrative spatial anchoring, improving the inference of gestures that depend on spatial references and motion directions. Moreover, real-time capture of facial expressions and head pose is relatively mature and offers strong online performance.

However, most existing co-speech gesture generation methods assume access to complete text and rely on semantic analysis to generate high-quality gestures, making them difficult to adapt to real-time scenarios with incremental, word-by-word user speech. To address this, we propose FaceCapGes, a method that generates online, real-time 3D skeletal gesture animations from three streams of real-time signals: speech, facial expressions, and head rotations. FaceCapGes can augment a user’s live digital avatar with expressive gestures without requiring the user to physically gesture, and it does not depend on future inputs, thereby enhancing avatar expressiveness in live interactive settings.

Prior work has analyzed the relationships among facial expressions, speech, and gestures, and has established effective learning paradigms. Building on existing cascaded architectures, our model introduces an additional head-pose feature analysis module and incorporates a sliding-window mechanism to enable real-time operation. The underlying framework and added modules incur low computational overhead, and the system maintains robust real-time performance even when running concurrently with facial capture and head-pose estimation.

Subjective evaluations show that our method achieves naturalness comparable to current mainstream approaches, while providing a significant advantage in responsiveness. The generated gestures are tightly aligned with speech and demonstrate strong real-time interactive behavior. Furthermore, our model can be deployed on lightweight devices such as iPhones; as long as the input format is compatible with the ARKit facial capture standard, it can be broadly applied to a wide range of real-time interactive scenarios.

\end{abstract}

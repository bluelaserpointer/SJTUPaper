% !TEX root = ../main.tex

\begin{abstract}[zh]

手势是人类交流中用于补充语义细节与传递情绪的重要非语言信号。
在虚拟人/数字形象的实时控制中，传统动作捕捉往往依赖穿戴式设备，虽然精度高，但存在成本高、使用门槛高、便携性差等问题；相机动捕虽降低了硬件负担，却仍要求用户在镜头前持续表演手势，在远程交流、直播等长时间场景中会带来空间占用与体力消耗，并与键鼠操作产生冲突。
近年来语音驱动手势生成技术为低门槛驱动提供了可能，但现有主流方法通常假设可获取完整语句或未来上下文，难以直接应用于用户逐字语音输入的在线实时交互环境，因此需要一种低延迟、可部署的实时手势生成方案。

为解决上述问题，本论文提出 FaceCapGes：一种面向实时数字人驱动的帧级手势生成方法，仅使用可在线采集的三类信号：语音、面部表情与头部姿态，在不依赖未来输入的条件下逐帧生成上半身 3D 骨骼动作，从而让用户无需实际做出手势即可驱动虚拟形象获得自然的随语表达。本文的主要研究内容包括：

(1) 提出 FaceCapGes 在线实时多模态手势生成框架，给出从信号采集、帧级推理到虚拟人驱动渲染的整体流程设计，并在严格因果约束下明确其可部署的实时生成机制。

(2) 在级联多模态架构中引入头部姿态作为新的实时输入模态，设计并实现轻量姿态编码与弱耦合融合策略，为手势提供节奏前瞻与空间锚定线索，从而增强动作节奏与空间指向一致性。

(3) 提出并实现在线实时手势生成的滑动窗口自回归训练策略，通过片段切割与窗口展开实现严格因果的逐帧生成，并结合片段级监督与对抗训练目标，提升实时生成的稳定性与时间连续性，缓解自回归漂移与抖动问题。

(4) 开展用户主观实验、客观指标测量与实时性能测试，并与代表性方法进行对比评估。实验结果表明，在严格因果约束下，本文方法 FaceCapGes 在真实性上表现良好，与现有扩散模型相当；同时在语调变化较快与头部朝向变化明显的片段中，能够生成更平滑且指向一致的动作，并具备满足实时交互场景需求的推理性能。

\end{abstract}

\begin{abstract}[en]

Gestures are essential nonverbal signals in human communication, enriching semantic details and conveying affective states.
In real-time control of virtual humans or digital avatars, conventional motion capture systems typically rely on wearable devices.
Although accurate, such systems are costly, inconvenient to use, and lack portability.
Camera-based capture reduces hardware requirements but still forces users to continuously perform gestures in front of a camera, which occupies physical space, causes fatigue during long-term scenarios such as remote communication and live streaming, and conflicts with keyboard--mouse interaction.
Recently, speech-driven gesture generation has emerged as a low-barrier alternative; however, most existing approaches assume access to complete utterances or future context, making them difficult to deploy in online interactive settings where speech is streamed word by word.
Therefore, a low-latency and deployable real-time gesture generation solution is required.

To address these challenges, this thesis proposes FaceCapGes, a frame-level gesture generation method for real-time avatar driving.
FaceCapGes uses only three types of online-available signals---speech, facial expressions, and head pose---to generate upper-body 3D skeletal motions frame by frame without relying on future inputs, enabling users to drive avatars with natural co-speech gestures without physically performing them.
The main contributions of this thesis are summarized as follows:

(1) A real-time multimodal gesture generation framework, FaceCapGes, is proposed, providing an overall pipeline design from signal acquisition and frame-level inference to avatar driving and rendering, and clarifying a deployable real-time generation mechanism under strict causality constraints.

(2) Head pose is introduced as a new real-time input modality in a cascaded multimodal architecture.
A lightweight pose encoder and a weakly coupled fusion strategy are designed and implemented to provide rhythmic anticipation and spatial anchoring cues, thereby enhancing gesture timing and spatial pointing consistency.

(3) A sliding-window autoregressive training strategy for online real-time gesture generation is proposed and implemented.
By segmenting motion sequences and unfolding them with a sliding window, strictly causal frame-wise generation is achieved.
Combined with segment-level supervision and adversarial objectives, the proposed strategy improves temporal stability and continuity, alleviating autoregressive drift and jitter.

(4) Subjective user studies, objective metric evaluation, and real-time performance tests are conducted, with comparative assessments against representative methods. The results demonstrate that, under strict causality constraints, FaceCapGes achieves strong realism comparable to existing diffusion-based models; moreover, in segments with rapid prosodic variations and noticeable head orientation changes, it generates smoother motions with more consistent spatial pointing, while maintaining inference efficiency sufficient for real-time interactive applications.

\end{abstract}

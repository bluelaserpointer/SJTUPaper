% !TEX root = ../main.tex

\chapter{评估}

为评估生成手势的质量，我们将生成的手势动画与其它研究方法做对比，展开客观指标测量与主观评测。
此外，我们还对本文模型的性能进行了分析。

\section{客观评估指标与实现细节}
\label{sec:objective_metrics}

为全面评价模型在动作自然性、节奏同步性与多样性等方面的表现，
本文在客观指标层面采用四项度量：Fréchet Gesture Distance(FGD)\cite{speech_gesture_generation}、语义相关动作召回率(Semantic Relevance Gesture Recall, SRGR)\cite{beatcamn}、节奏对齐度(Beat Alignment, BA)\cite{beatcamn, beatalign}以及L1范数(L1DIV)。
这些指标分别对应生成动作在分布一致性、语音同步性与变化丰富性等不同维度，
共同构成对模型质量的综合评估体系。

\subsection{Fréchet Gesture Distance(FGD)}
\label{subsec:fgd}

FGD\cite{speech_gesture_generation}用于衡量生成手势分布与真实手势分布之间的统计距离，
灵感源自图像生成领域的Fréchet Inception Distance(FID)。
不同于图像任务直接利用Inception网络特征，
在动作生成领域，特征空间需由单独训练的动作自编码器定义。
该自编码器通过重构任务学习手势的潜在表示，使潜在空间具备对运动模式的压缩与区分能力。
在该潜在空间中，假设真实分布与生成分布的高维嵌入向量分别为
$\mathcal{N}(\bm{\mu}_r, \bm{\Sigma}_r)$ 与 $\mathcal{N}(\bm{\mu}_g, \bm{\Sigma}_g)$，
其中，$\mathcal{N}(\cdot)$为高斯分布，$\bm{\mu}$与$\bm{\Sigma}$分别为嵌入特征的均值向量与协方差矩阵。

此时，FGD定义为：
\begin{equation}
\mathrm{FGD} = 
\|\bm{\mu}_r - \bm{\mu}_g\|_2^2 +
\mathrm{Tr}(\bm{\Sigma}_r + \bm{\Sigma}_g - 2(\bm{\Sigma}_r \bm{\Sigma}_g)^{1/2}).
\end{equation}

其中，$\mathrm{Tr}(\cdot)$为矩阵迹运算。

较小的FGD值表示生成动作的统计分布更接近真实数据，
可反映动作的整体自然度与风格一致性。

\paragraph{评估模型结构与训练配置}
本文在每位说话人的训练集上分别训练一组评估用自编码器，以避免跨说话人分布差异对指标的干扰。
自编码器输入为以Rot6d表示的上半身骨架序列，
训练目标为最小化位置与速度的重构误差：
\begin{equation}
\mathcal{L}_{AE} = 
\lambda_r \|\hat{\bm{g}} - \bm{g}\|_2^2 +
\lambda_v \|\hat{\bm{g}}' - \bm{g}'\|_2^2,
\end{equation}
其中$\lambda_r = 1$, $\lambda_v = 0.1$，$\bm{g}'$为速度序列。

训练配置如下：
输入片段长度为32帧，批大小256，隐藏层维度128，学习率$1.2\times10^{-4}$，优化器为Adam；
共训练400轮。
训练片段步长设为10，以增加样本数量并保持时间连续性。

\paragraph{用于FGD评估的骨架感知自编码器}

在基线模型CaMN的FGD评估中，采用了一种基于时间卷积的嵌入式自编码器（embedding-based autoencoder）进行手势特征提取。
该结构将每一帧姿态平铺为高维向量输入，并对各关节分量进行独立建模。
在实际使用中我们观察到，这类嵌入空间对旋转表示的数值尺度较为敏感。
当采用Rot6d表示时，不同关节与分量之间的不均衡方差可能在潜在空间中被进一步放大，
从而导致协方差估计条件较差，并引发FGD数值不稳定的问题。

为提高FGD评估的鲁棒性，本文采用了一种骨架拓扑感知的自编码器作为特征提取器。
该模型在编码阶段显式引入骨架邻接矩阵$A$，并通过对相邻关节进行局部卷积实现结构约束。
具体而言，第$l$层中关节$i$的特征向量$\mathbf{h}_i^{(l)} \in \mathbb{R}^{d_l}$，
其中$d_l$表示第$l$层中每个关节节点的特征维度。
通过聚合其邻域$\mathcal{N}(i)$内相邻关节$j$的特征$\mathbf{h}_j^{(l)}$进行更新，
其中$A_{ij}$表示骨架邻接矩阵中节点$j$到节点$i$的连接权重，
即
\begin{equation}
A_{ij} =
\begin{cases}
1, & \text{若关节}i\text{与}j\text{在骨架拓扑中直接相连，或}i=j; \\
0, & \text{否则}.
\end{cases}
\end{equation}
此外，设$W^{(l)} \in \mathbb{R}^{d_{l+1}\times d_l}$与$b^{(l)} \in \mathbb{R}^{d_{l+1}}$
分别为第$l$层的可学习权重矩阵与偏置项。设$\sigma(\cdot)$为非线性激活函数，在本文实现中取为双曲正切函数$\tanh(\cdot)$。
上述局部邻域聚合过程可形式化表示为：
\begin{equation}
\mathbf{h}_i^{(l+1)} =
\sigma\left(
\sum_{j=1}^{J} A_{ij} W^{(l)} \mathbf{h}_j^{(l)} + b^{(l)}
\right),
\end{equation}
这种基于局部邻域的权重共享机制有助于保持人体运动的空间结构一致性，
并在特征提取过程中缓解由旋转表示差异带来的数值尺度放大问题。

实验结果表明，该骨架感知自编码器在Rot6d下产生更加稳定的潜在分布统计量。
在本文的实验设置中，该设计显著提升了FGD评估的数值稳定性，
并有效避免了在Rot6d表示条件下使用嵌入式自编码器时出现的极端FGD数值现象。

\paragraph{指标计算流程}
在获得评估模型后，
分别将生成序列与真实序列输入自编码器的编码器部分，
提取潜在特征$\bm{z}_g$与$\bm{z}_r$，
再计算两者的均值与协方差以求得FGD。
评估时按说话人独立计算，再取平均值作为总体指标。

\subsection{Semantic Relevance Gesture Recall (SRGR)}
\label{subsec:srgr}

SRGR指标（Semantic Relevance Gesture Recall）\cite{beatcamn}
用于衡量生成手势在语义相关时间段内与真实手势在数值层面的匹配程度。
该指标关注的是语音语义触发的关键手势是否被正确生成，
而非整体分布一致性或语音–手势节奏相关性。

与基于分布的评估指标（如 FGD）不同，
SRGR 属于基于阈值的逐帧召回率度量，
通过统计生成手势在允许误差范围内命中的比例，
反映模型在语义相关动作重现方面的准确性。

\paragraph{定义与原理}
在本文实现中，SRGR 作用于关节的旋转表示。
设真实手势序列与生成手势序列在第 $t$ 帧第 $j$ 个关节的旋转表示
分别为 $\mathbf{r}_{t,j}$ 与 $\hat{\mathbf{r}}_{t,j}$，
其中 $\mathbf{r}_{t,j} \in \mathbb{R}^{6}$ 为关节的 Rot6d 表示，
$T$ 为序列总帧数，$J$ 为关节数量。

在给定旋转表示误差阈值 $\delta$ 的条件下，
若生成关节旋转与真实关节旋转之间的表示差异满足
$\|\mathbf{r}_{t,j} - \hat{\mathbf{r}}_{t,j}\|_1 < \delta$，
则认为该关节在该时刻被成功召回。
在本文实验中，阈值固定设为 $\delta = 0.5$。

SRGR通过对所有时间帧与关节进行统计，
并引入语义相关性权重 $\lambda_t$，
定义为：
\begin{equation}
\mathrm{SRGR} =
\frac{1}{T J}
\sum_{t=1}^{T}
\sum_{j=1}^{J}
\lambda_t \,
\mathbb{I}
\left(
\|\mathbf{r}_{t,j} - \hat{\mathbf{r}}_{t,j}\| < \delta
\right),
\end{equation}
其中 $\mathbb{I}(\cdot)$ 为指示函数。

语义相关性权重 $\lambda_t$ 用于强调语音中语义显著时间片段
（如强调词或语义触发点）对应的手势匹配程度，由 BEAT 数据集提供的语义标注确定，
从而使 SRGR 更加关注语义相关手势的召回情况，
而非对所有时间片段进行均匀统计。

\paragraph{计算流程}
1. 对真实与生成手势序列进行时间对齐；
2. 在逐帧、逐关节层面计算 Rot6d 表示下的旋转误差；
3. 根据阈值 $\delta$ 判断是否成功召回；
4. 对所有关节与时间帧进行加权平均，
   得到最终的 SRGR 值。

\subsection{Beat Alignment (BA)}
\label{subsec:ba}

BA(Beat Alignment)指标用于衡量语音节拍事件与手势关键动作事件在时间轴上的对齐程度，
反映模型在语音–手势时序同步性（temporal synchronization）方面的性能。
本文采用 BEAT/CaMN 中使用的 BeatAlign 指标\cite{beatcamn}，其原始形式由 Li 等人提出于 AI Choreographer 工作中\cite{beatalign}，
并可视为音频节拍与动作节拍集合之间的单向 Chamfer 相似度度量。

与 SRGR 基于阈值的逐帧数值匹配不同，
BA 关注的是离散事件层面的时间对齐关系，
即手势关键动作是否在时间上合理地响应了语音中的重读节拍。

\paragraph{定义与原理}
设语音节拍事件集合为
$\mathcal{B}_s = \{ t^s_k \}$，
通过对语音信号的能量相关特征进行起始点检测（onset detection）得到。
具体而言，首先基于谱能量变化计算 onset strength 曲线，
并在该曲线上进行峰值检测以获得候选语音事件位置。
随后，引入 Root Mean Square(RMS)特征作为短时能量幅度的描述，
并在 RMS 曲线上对检测到的起始点进行 backtracking 校正，
从而将语音节拍事件定位至能量实际开始上升的位置，
以提高时间定位的稳定性与准确性。

手势关键动作事件集合为
$\mathcal{B}_g = \{ t^g_m \}$，
定义为关节运动速度的局部极小值点，
对应动作中的停顿或方向变化等显著运动事件。

对于每一个语音节拍事件$t^s_k$，
计算其与所有手势关键动作事件之间的最小时间偏差：
\begin{equation}
\Delta t_k = \min_{m} | t^s_k - t^g_m | .
\end{equation}
随后采用高斯核函数将时间偏差映射为相似度分数，
从而得到单个语音节拍的对齐得分。
最终BA指标定义为：
\begin{equation}
\mathrm{BA} =
\frac{1}{|\mathcal{B}_s|}
\sum_{k}
\exp\left(
-\frac{(\Delta t_k)^2}{2\sigma^2}
\right),
\end{equation}
其中$\sigma$为时间尺度参数，本文中取$\sigma=0.3$，用于控制对齐容忍范围。

该定义可视为从语音节拍集合到手势节拍集合的单向 Chamfer 相似度，
当语音节拍与手势关键动作在时间上高度对齐时，BA 值接近 1；
反之，当二者时间偏差较大时，BA 值趋近于 0。

\paragraph{计算流程}
1. 基于语音能量包络与 RMS 特征进行 onset detection，提取语音节拍事件；  
2. 计算关节旋转速度，并检测其局部极小值作为手势关键动作事件；  
3. 对每个语音节拍计算其到最近手势关键动作的时间偏差；  
4. 通过高斯核函数将时间偏差映射为相似度，并对所有语音节拍取平均，得到 BA 指标。

本文使用 BA 指标评估模型在语音重读节拍与手势关键动作之间的时间同步能力，
该指标与主观观察到的语音–手势同步自然度具有较好一致性。

\subsection{L1范数}
\label{subsec:l1div}

L1范数指标\cite{beatcamn}衡量生成手势序列的多样性，
即不同语音输入下模型生成结果的动作差异程度。
该指标反映模型在保持自然性的同时，
能否避免生成收敛到平均动作模式（mode collapse）的倾向。

\paragraph{定义与原理}
设模型对同一语音片段的 $N$ 次生成结果为
$\{\bm{g}^{(1)}, \bm{g}^{(2)}, \dots, \bm{g}^{(N)}\}$，
则L1范数（L1DIV）定义为任意两序列间平均L1距离：
\begin{equation}
\mathrm{L1DIV} = 
\frac{2}{N(N-1)} \sum_{i<j} 
\frac{1}{T} \sum_{t=1}^{T} 
\|\bm{g}^{(i)}_t - \bm{g}^{(j)}_t\|_1.
\end{equation}
当$N=2$ 时，该指标即为两次独立生成的平均动作差异。

\paragraph{计算流程}
1. 对同一语音输入重复生成多次（不同随机噪声或dropout路径）；  
2. 对每帧姿态计算关节旋转的L1差；  
3. 对时间与样本求平均得到整体L1范数值。  

较高的L1范数表明模型生成具有较强的多样性，
但过高可能意味着动作不稳定或噪声放大。
因此，L1范数通常与FGD联合分析：  
FGD 反映真实度，L1范数反映丰富度，
两者共同平衡模型在自然性—多样性维度上的表现。

\subsection{客观评估设置}
所有评估均在BEAT数据集上进行，选用说话人编号为2、4、6和8。生成的身体姿态与真实标签均采用相同骨架拓扑的BVH格式。
FGD训练方式由EMAGE\cite{emage}提取，L1范数、BA和SRGR的实现使用CaMN\cite{beatcamn}提供的算法。
各指标结果在四位说话人上取平均，以减少个体差异带来的偏差。

\section{模型对比概览}

表~\ref{tab:modalitycomparison}总结了各模型的输入输出模态特征。值得注意的是，我们的模型是唯一同时支持“头部姿态”输入且完全不依赖未来信息的方案。

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lccccc|c|c@{}}
\toprule
\multirow{2}{*}{模型} & \multicolumn{5}{c|}{输入模态} & \multirow{2}{*}{未来信息} & \multirow{2}{*}{输出} \\ 
\cmidrule(lr){2-6}
 & 音频 & 面部捕捉 & 头部姿态 & 说话人ID & 情绪 &  &  \\ 
\midrule
CaMN       & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 身体 \\
DiffSHEG   & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & 身体+面部 \\
本方法     & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\ast$ & $\times$ & $\times$ & 身体 \\ 
\bottomrule
\end{tabular}%
}
\caption{实验对比模型的输入输出模态对比}
\label{tab:modalitycomparison}
\end{table}

\section{用户调研}
\label{sec:userstudy}

为进一步验证模型在真实交互环境中的表现，
本文进行了用户主观评估实验，
比较FaceCapGes、DiffSHEG\cite{diffsheg}与CaMN\cite{beatcamn}三个模型在动作自然性、同步性与多样性方面的主观质量。
本节首先介绍用户研究系统与实验配置，
随后报告主观评价结果与分析。

\subsection{评估系统与实验配置}

\paragraph{实验材料与呈现方式}
用户评估所使用的手势动画均基于测试集语音片段生成，
并以BVH(Biovision Hierarchy)文件形式保存。
BVH是一种通用的动作捕捉数据格式，
通过层级化定义骨骼结构与帧级旋转参数，
可直接导入3D动画与虚拟人系统。

本文使用的BVH文件采用欧拉角旋转表示。
由于三种对比模型的输出旋转参数形式不同——
CaMN模型直接输出欧拉角，
DiffSHEG输出Axis-Angle，
而FaceCapGes输出Rot6d表示——
因此在导出BVH之前，
需将后两者统一转换为欧拉角表示，
以便在后续动画播放中使用统一渲染流程。

所有对比模型（FaceCapGes、CaMN、DiffSHEG）均使用相同的训练集与骨骼定义，
其中CaMN与DiffSHEG采用各自论文公开的原版参数。
虚拟人角色选自BEAT数据集提供的公开演讲者模型集合，
并经过Unity\cite{unity}的Mecanim\cite{unitymechanim}自动骨骼绑定系统匹配。
该系统会自动配对BVH文件中定义的骨骼层级与虚拟人模型的骨骼节点，
从而在不依赖手动权重绘制的情况下完成动作重定向。

本系统当前支持的虚拟人模型需同时具备骨骼绑定与ARKit\cite{ARKitDocumentation}兼容的BlendShape参数。
基于此约束，实验选用了CaMN论文公开的男女两名演讲者模型，二者均满足兼容要求，可实现身体与面部的联合驱动。

\paragraph{播放系统实现}
我们基于Unity自行编写播放脚本，
将各模型生成的BVH动画用于驱动虚拟人身体骨骼，
同时以面部捕捉序列驱动BlendShape表情参数，
并同步播放原始语音音频。
系统支持同时呈现三种模型生成的动画结果：
用户可在同一画面中（左、中、右）并行观察三种手势表现，
所有语音与面部表情完全一致，
唯一变量为身体动作。
该设计使参与者能够直接比较不同模型在动作风格、
节奏响应与语音同步性方面的差异。

为确保主观评价的公正性与可重复性，
系统在每次实验开始前会随机分配三种模型的位置（左、中、右），
界面上不会显示模型名称，
从而避免潜在偏向。
各测试片段的播放顺序在实验前统一设定，
以保证不同参与者之间的样本顺序均衡。
实验员在播放系统后台记录当前序列与模型对应关系，
以便后续结果统计。

\paragraph{实验界面与设备}
评估系统提供桌面端与VR端两种版本，功能完全一致。
VR版本基于PICO设备\cite{pico4}实现；
桌面版支持多窗口并行播放，方便用户同时对比。
如图~\ref{fig:userstudy_app}所示，
播放界面在两种设备上保持统一布局，
播放完成后参与者需通过交互界面对三个模型进行排序打分。
VR用户在沉浸式环境中逐一观看三段动画；
桌面端用户则可在单屏上同时观察全部模型。
因此前者注重细节感知与临场性，
后者更有利于整体风格与节奏的一致性对比。

\paragraph{实验流程与指导}
实验正式开始前，
研究人员向参与者说明了三项主观评价标准的含义，
确保所有被试对评分维度理解一致：
\begin{itemize}
    \item \textbf{真实感（Realism）}：整体动作是否自然流畅，是否存在明显的违和感，如朝向异常或突然抖动；
    \item \textbf{同步性（Synchronization）}：手势动作与语调、语音节奏是否协调一致；
    \item \textbf{多样性（Diversity）}：手势是否丰富多变，避免长时间静止或重复单一动作。
\end{itemize}

在实验过程中，
VR版本于线下环境进行，
桌面版通过线上远程环境执行。
两种形式均保持实时交流通道，
研究人员可在参与者提问时即时解释操作或澄清评分标准。
在正式评估阶段，
参与者可多次重播当前片段，
但不能返回查看先前内容，
以减少记忆偏差。
所有播放条件（Unity场景内的相机角度、光照参数、音量与分辨率设置）
在全部被试中保持一致，
以确保渲染输出的可比性。

需要说明的是，
对于VR实验，
所有测试均在相同的线下实验室环境中进行，
使用同一套PICO设备与照明条件；
而桌面端实验通过远程方式执行，
参与者在各自电脑上运行实验程序。
研究人员可通过实时屏幕共享观察其操作流程并保持语音沟通，
但无法严格控制其所在房间的光照或环境噪声条件。
因此，桌面端实验在“观看环境”上存在一定差异，
但由于任务内容与播放系统完全相同，
且实验员在测试中持续指导，
可认为该差异对结果的总体影响有限。

\paragraph{实验材料与任务设计}
评估样本来自BEAT数据集中四位演讲者（ID: 2、4、6、8），
其中2、4为男性，6、8为女性。
每位演讲者各选取两段平均长度约1分钟的语音片段，
演讲话题互不重复，共组成8段固定视频样本。
所有实验均使用相同的8段样本，
但其呈现顺序在不同被试间经过随机化或平衡化处理，
以避免顺序效应。
每段视频均包含三种模型生成的动作版本（FaceCapGes、CaMN、DiffSHEG），
并在播放时随机分配其在屏幕的左右位置。
参与者在观看每个片段后，
根据三项主观标准（真实感、同步性、多样性）
对三个模型的表现进行排序评估。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudyImage.png}
\caption{用户研究系统界面示意图。
左、中、右位置随机分配给三种模型，
音频与面部表情完全一致，仅身体动作不同。}
\label{fig:userstudy_app}
\end{figure}

\paragraph{实验参与者}
共邀请16名参与者（12名使用VR设备，4名使用桌面端），
涵盖不同性别与学术背景。
所有参与者在实验前均接受了操作说明与校准，
并在系统指导下完成评分练习。
为避免呈现顺序对主观印象造成偏差，
另设计了采用平衡拉丁方（Balanced Latin Square）顺序的实验版本，
使不同参与者观看样本的顺序均衡分布。

\subsection{平衡拉丁方排序设计}
\label{subsec:latin_square}

该版本实验共招募8位VR用户（4男4女），
排序顺序由HCI用户调研工具包\cite{LatinSquareToolkit}自动生成，
确保模型与演讲者组合的呈现顺序在全体被试间均匀分布。
所有条件保持一致，唯一变量为视频播放顺序。

\subsection{结果与分析}
\label{subsec:user_study_result}

本节综合分析两轮用户评估的统计结果与参与者反馈。
所有结果均基于相同的8段测试样本，
其中模型位置与播放顺序在不同被试间随机化或经平衡拉丁方控制，
以保证主观评价的公正性。

\paragraph{电脑用户测评结果}
如图~\ref{fig:userstudy}所示，
在16名参与者的总体评价中，
FaceCapGes在三个维度（真实感、同步性、多样性）上均优于基线模型CaMN，
并在“真实感”方面与离线扩散模型DiffSHEG持平。
这一结果表明，FaceCapGes虽在严格的实时因果约束下运行，
但仍能保持与非实时生成模型相近的动作自然度与流畅性。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudy_All.png}
\caption{16 名参与者对三种模型在“真实感”、“同步性”和“多样性”三个维度的主观排名结果。}
\label{fig:userstudy}
\end{figure}

\paragraph{平衡拉丁方实验结果}
图~\ref{fig:userstudy_latin_square}展示了平衡拉丁方实验版本中8位VR用户的独立结果，
该版本严格控制了模型与演讲者组合的呈现顺序。
结果与总体趋势一致，
FaceCapGes在“同步性”与“真实感”上显著优于CaMN，
并在“多样性”指标上与DiffSHEG接近。
这表明实验结果在不同顺序条件下保持稳定，
进一步验证了模型在多维度主观评价中的一致优势。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudy_LatinSquare.png}
\caption{平衡拉丁方实验版本中8位VR用户的主观排名结果，
评价维度包括“真实感”、“同步性”和“多样性”。}
\label{fig:userstudy_latin_square}
\end{figure}

\paragraph{用户反馈分析}
根据实验后访谈与自由评论汇总，
参与者普遍认为FaceCapGes的动作过渡自然、节奏感强，
手势响应与语音重音、语调变化更加一致。
部分VR用户指出，
在沉浸式视角下能够明显感受到手势的连贯性与时序协调，
而CaMN在头部与上身动作衔接处偶有“僵硬”或“转向延迟”现象。
约三分之一的参与者提到DiffSHEG的动作表现力最强，
但在某些片段中出现手部摆动幅度过大或抖动的情况，
导致同步性评分略低。

桌面端用户普遍认为三种模型的差异在节奏与流畅性上最明显；
VR用户则更容易察觉动作的空间一致性与临场协调。
两组被试的整体排序趋势一致，
说明模型差异具有跨设备一致性。

\paragraph{结果讨论}
CaMN得分较低的主要原因在于其采用欧拉角表示，
导致部分姿态序列在旋转空间中出现不连续，
尤其在头部动作上易产生跳变；
而DiffSHEG的高方差输出虽提升了动作多样性，
但表现出一定的突然颤抖，可能来源于Axis-Angle的不连续性。
相比之下，
FaceCapGes的因果式时间建模和头部姿态融合策略
有效提升了局部动作的平滑性与节奏协调，
使其在实时条件下同时兼顾自然度与稳定性。
此外，
实验的平衡拉丁方版本进一步证明结果在不同呈现顺序下的一致性，
排除了顺序偏差对主观评价的显著影响。

综上，
用户研究表明FaceCapGes在实时生成条件下
仍能维持与离线模型相当的主观表现，
在动作真实感、节奏同步性和长时间交互稳定性方面
均显著优于传统因果结构基线，
验证了本文提出的多模态融合与时间建模策略的有效性。

\section{定性分析}

我们强烈建议观看电子附录中的演示视频，内容包含GT、CaMN、DiffSHEG与本模型的并排展示，能够直观体现时间对齐性、手势响应性以及头-身协调性方面的差异。

如图~\ref{fig3} 所示，FaceCapGes能平滑且富有表现力地捕捉说话人动态。与CaMN相比，本模型避免了欧拉角带来的不连续问题，生成的身体动作可顺应头部运动趋势。与DiffSHEG相比，我们的模型在动作速度上略逊一筹，但能有效避免抖动或夸张姿态。头部姿态的引入也明显提升了手势方向与语义的一致性。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/GeneratedPoseOverview.png}
\caption{GT、CaMN、DiffSHEG与FaceCapGes的动作效果对比。本模型手势响应自然，方向与头部朝向一致。}
\label{fig3}
\end{figure*}

\section{定量分析}

\begin{table*}[h]
\centering
\begin{tabular}{@{}lclrrrr@{}}
\hline
区域 & 方法 & FGD\textdownarrow & SRGR\textuparrow & BA\textuparrow & L1DIV\textuparrow \\
\hline
\multirow{3}{*}{全身}
& CaMN       & 47.732 & 0.098 & 0.845 & 7.591 \\
& DiffSHEG   & \underline{26.846} & \textbf{0.109} & \underline{0.883} & \underline{11.028} \\
& 本方法     & \textbf{23.385} & \underline{0.107} & \textbf{0.913} & \textbf{13.284} \\
\hline
\multirow{3}{*}{不含头部}
& CaMN       & 49.437 & 0.103 & 0.845 & 7.327 \\
& DiffSHEG   & \underline{26.847} & \textbf{0.110} & \underline{0.883} & \underline{10.990} \\
& 本方法     & \textbf{23.384} & \underline{0.107} & \textbf{0.913} & \textbf{13.330} \\
\hline
\end{tabular}
\caption{在BEAT的四位说话人测试集上的定量评估结果。FaceCapGes在所有指标上优于CaMN，且与DiffSHEG表现相当。}
\label{tab1}
\end{table*}

表~\ref{tab1}显示，FaceCapGes在所有指标上均优于CaMN。与DiffSHEG相比，本模型FGD更低，SRGR相近，且在L1范数上表现最优，表明其生成动作具备良好多样性。但这一结论与用户主观评分存在一定出入：DiffSHEG在多样性上主观排名更高。

这个现象可能来自于L1范数的局限性：它主要衡量空间偏离程度，不能直接体现视觉表现力，或捕捉感知上的丰富性。虽然我们的模型在结构上更丰富，但用户普遍认为DiffSHEG的动作更活跃，在合理的时机做出了许多吸引注意的动作。

\section{消融实验分析}

\begin{table*}[h]
\centering
\begin{tabular}{@{}llrrrr@{}}
\hline
区域 & 变体 & FGD\textdownarrow & SRGR\textuparrow & BA\textuparrow & L1DIV\textuparrow \\
\hline
\multirow{4}{*}{全身}
& CaMN                    & 32.870  & 0.111  & 0.858  & 7.214  \\
& 移除头部姿态           & 19.591  & 0.123  & 0.916  & 10.642 \\
& 移除帧级生成           & 21.592  & \textbf{0.125}  & 0.892  & 10.456 \\
& FaceCapGes（本方法）   & \textbf{19.290}  & 0.123 & \textbf{0.918}  & \textbf{10.871} \\
\hline
\end{tabular}
\caption{说话人2的消融实验结果。头部姿态对提升手势自然性与表达力有显著作用。}
\label{tab2}
\end{table*}

表~\ref{tab2}展示了各模块对模型性能的影响。移除头部姿态会基本导致所有指标上的表现下降，证明其对表达性手势的重要性。

值得注意的是，非帧级版本使用未来上下文与双向LSTM，但其表现反而不及帧级版本。这可能是因为它采用一次性解码，对每段输入独立处理，无法保证时间连续性，与原CaMN的设计一致。而我们的方法通过滑动窗口训练和自回归推理，逐帧依赖历史信息，使动作更连贯、过渡更自然。即使不使用未来帧，也能获得更低的FGD和更自然的手势对齐。%需要更多验证

\section{性能评估}

\begin{table}[h]
\centering
\begin{tabular}{@{}lr@{}}
\hline
指标 & 时间 \\
\hline
测试帧数 & 93015 (f) \\
推理总时长 & 504 (s) \\
平均单帧时间 & 6.07E-03 (s/f) \\
\hline
\end{tabular}
\caption{在 RTX4090上的帧级推理速度评估结果。}
\label{tab3}
\end{table}

为评估实时性能，我们在BEAT测试集上使用batch size为1的配置进行推理测量。如表~\ref{tab3}所示，模型平均每帧处理时间小于7毫秒，满足实时响应需求。

尽管模型推理本身较快，但系统总延迟还受输入特征提取影响。在我们的原型中，音频特征通过Librosa离线提取，93,015帧总计耗时约61秒，单帧不足1毫秒。实际部署中可替换为如PyAudio或SoundDevice的实时提取方法，尽管本文未测量其具体延迟。

ARKit面部追踪的延迟由开发文档估算约为60FPS\cite{AppleARKitTrackingGuide}。结合模型推理时间，整体系统延迟预计不超过25ms，足以满足实时虚拟人交互的需求。

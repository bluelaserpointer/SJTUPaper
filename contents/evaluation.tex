% !TEX root = ../main.tex

\chapter{实验结果与分析}

为评估 FaceCapGes 的生成质量与实时性能，本文从用户评估、客观指标测量与推理效率三个方面展开实验，
并在生成质量上与代表性方法进行对比分析。

\section{训练配置}
\label{sec:implementation}

本文模型FaceCapGes基于PyTorch实现，
所有实验在单张 NVIDIA RTX 4090 GPU 上进行。

本文基于 BEAT 数据集\cite{beatcamn} 进行训练与评估。
该数据集包含多模态同步的语音、面部 blendshape 与全身动作信息，
以 15\,fps 记录多位专业表演者的演讲片段，覆盖多种语义与情绪场景。
其标准骨架结构如图~\ref{fig_beatbones} 所示，
BEAT 数据集共包含人体中的 47 个关节节点，
包括上肢及躯干的三个主要控制点（蓝色区域所示）。
下肢关节则保持静态。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.15\textwidth]{figures/Fig_BEATBones.png}
\bicaption{BEAT 数据集的骨架拓扑结构与驱动范围}{Skeleton Topology and Actuated Joint Range in the BEAT Dataset}
\label{fig_beatbones}
\end{figure}

本文选取表演者 ID~2、4、6、8 的数据进行训练与测试，
其中~2、4~为男性，~6、8~为女性，
确保在性别与说话风格上的分布均衡。
训练集与测试集均包含相同的表演者，
但使用不同的演讲片段，
在预处理阶段已进行严格划分以避免片段交叉。

\paragraph{训练配置}
\label{train_param_setting}
训练时输入窗口的前序帧数设为 $N=16$，预测长度为 $M=34$，训练片段的切割步长为 10 帧。
相邻片段因此存在部分重叠，从而在保证充分上下文信息的同时提升数据覆盖率与时间连续性。
批大小设为 256。

优化器采用随机梯度下降（Stochastic Gradient Descent, SGD）\cite{SGDBottou2010LargeScaleML}。
基础学习率设置为 $\mathrm{lr}_{base}=2.5\times10^{-4}$，
并根据批大小按线性规则缩放为
\begin{equation}
\mathrm{lr}_{g} = \mathrm{lr}_{base}\cdot \frac{\mathrm{batch\_size}}{128},
\end{equation}
其中 $\mathrm{lr}_{g}$ 为生成器（手势生成网络）的实际学习率。

在对抗训练阶段，判别器使用相同类型的 SGD 优化器，
其学习率按权重系数 $w_d$ 缩放为
\begin{equation}
\mathrm{lr}_{d} = w_d \cdot \mathrm{lr}_{g},
\end{equation}
本文中设定 $w_d = 0.2$。

为防止早期训练阶段的不稳定，对抗项在第 10 个 epoch 后引入，
即前 10 个 epoch 仅优化重构与时序平滑损失，
从第 11 个 epoch 起加入判别器并交替优化生成器与判别器参数。
整体训练共 374 个 epoch。

在损失计算中，前 $N$ 帧的历史窗口仅作为因果上下文输入，不参与重构与对抗项的误差回传。
损失函数采用第~\ref{sec:loss} 节所述的复合重构与对抗目标。

\paragraph{姿态表示}
所有身体动作均转换为连续可微的 Rot6D 表示，
使用 EMAGE\cite{emage} 中的实现方法，
以避免欧拉角奇异性与四元数的符号不确定性。

\paragraph{运行性能}
在实时推理阶段，FaceCapGes 以 15\,FPS 的速度驱动虚拟角色。

\section{实验配置}

\subsection{实验对比模型}

我们选取 FaceCapGes 的基线模型 CaMN\cite{beatcamn}，以及扩散模型方法中具有代表性的 DiffSHEG\cite{diffsheg} 作为对比模型。

表~\ref{tab:modalitycomparison} 总结了各模型的输入输出模态特征。其中，$\ast$表示模型结构未显式输入说话人ID，采用每位说话人独立训练的设置，说话人ID通过模型参数指定。

值得注意的是，FaceCapGes 是唯一满足严格因果约束的在线模型，
因此在评估时采用逐帧推理并将输出拼接为完整序列，以模拟实时输入流。

\begin{table}[h]
\bicaption{对比模型的输入输出模态}{Comparison of Input/Output Modalities of Evaluated Methods}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lccccc|c|c@{}}
\toprule
\multirow{2}{*}{模型} & \multicolumn{5}{c|}{输入模态} & \multirow{2}{*}{未来信息} & \multirow{2}{*}{输出} \\ 
\cmidrule(lr){2-6}
 & 音频 & 面部捕捉 & 头部姿态 & 说话人ID & 情绪 &  &  \\ 
\midrule
CaMN       & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 身体 \\
DiffSHEG   & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & 身体+面部 \\
本模型     & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\ast$ & $\times$ & $\times$ & 身体 \\ 
\bottomrule
\end{tabular}
}
\label{tab:modalitycomparison}
\end{table}

\subsection{跨模型评估设置}
本章所有实验均基于 BEAT 数据集进行，各模型使用一致的骨架拓扑与姿态表示，
从而保证输出格式可直接对齐与比较。
其中 CaMN 与 DiffSHEG 使用其官方公开的预训练模型；
FaceCapGes 则在相同的数据预处理与骨架设定下由本文训练得到。

对于离线模型（CaMN 与 DiffSHEG），我们将整段演讲作为输入并一次性生成完整动作序列；
对于在线模型（FaceCapGes），
我们在关闭批处理（Batch=1）的条件下模拟逐帧输入流，
并将逐帧输出按时间顺序拼接得到最终序列，以还原实时交互场景下的运行状态。

\section{用户评估}
\label{sec:userstudy}

为验证模型在交互环境中的表现，
本文进行了用户主观评估实验，
比较FaceCapGes、CaMN\cite{beatcamn}、DiffSHEG\cite{diffsheg}三个模型在动作自然性、同步性与多样性方面的主观质量。
本节首先介绍用户评估系统与实验配置，
随后报告主观评价结果与分析。

\subsection{用户评估系统与实验配置}

\paragraph{实验材料与呈现方式}
用户评估所使用的手势动画均基于BEAT数据集中的测试集语音片段生成，
并以Biovision Hierarchy（BVH）文件形式保存。
BVH是一种通用的动作捕捉数据格式，
通过层级化定义骨骼结构与帧级旋转参数，
可直接导入3D动画与虚拟人系统。

本文使用的BVH文件采用欧拉角旋转表示。
由于三种对比模型的输出旋转参数形式不同，
因此在导出BVH之前，
需将输出姿态统一转换为欧拉角表示，
以便在后续动画播放中使用统一渲染流程。

本系统当前支持的虚拟人三维模型，
需同时具备骨骼绑定，
和与ARKit~\cite{ARKitDocumentation}兼容的BlendShape参数。
基于此约束，本实验采用BEAT数据集提供的公开的男女两名演讲者三维模型，二者均满足兼容要求，可实现身体与面部的联合驱动。
此外，经过Unity\cite{unity}的Mecanim\cite{unitymechanim}自动骨骼绑定系统匹配，
可自动配对模型生成的BVH文件中定义的骨骼层级与虚拟人模型的骨骼节点，
从而在不依赖手动权重绘制的情况下完成动作重定向。

\paragraph{播放系统实现}
我们基于Unity自行编写播放脚本，
将各模型生成的BVH动画用于驱动虚拟人身体骨骼，
同时以面部捕捉序列驱动BlendShape表情参数，
并同步播放原始语音音频。
系统支持同时呈现三种模型生成的动画结果：
用户可在同一画面中（左、中、右）并行观察三种手势表现，
所有语音与面部表情完全一致，
唯一变量为身体动作。
该设计使参与者能够直接比较不同模型在动作风格、
节奏响应与语音同步性方面的差异。

为确保主观评价的公正性与可重复性，
系统在每次实验开始前会随机分配三种模型的位置（左、中、右），
界面上不会显示模型名称，
从而避免潜在偏向。
各测试片段的播放顺序在实验前统一设定，
以保证不同参与者之间的样本顺序均衡。
实验员在播放系统后台记录当前序列与模型对应关系，
以便后续结果统计。

\paragraph{实验界面与设备}
用户评估系统提供桌面端与虚拟现实（VR）端两种版本，功能完全一致。
VR版本基于PICO设备\cite{pico4}实现；
桌面版支持多窗口并行播放，方便用户同时对比。
如图~\ref{fig:userstudy_app}所示，
播放界面在两种设备上保持统一布局，
播放完成后参与者需通过交互界面对三个模型进行排序打分。

VR用户在沉浸式环境中逐一观看三段动画；
桌面端用户则可在单屏上同时观察全部模型。
因此前者注重细节感知与临场性，
后者更有利于整体风格与节奏的一致性对比。

\paragraph{实验流程与指导}
实验正式开始前，
研究人员向参与者说明了三项主观评价标准的含义，
确保所有被试对评分维度理解一致：
\begin{itemize}
    \item \textbf{真实性}：整体动作是否自然流畅，是否存在明显的违和感，如朝向异常或突然抖动；
    \item \textbf{同步性}：手势动作与语调、语音节奏是否协调一致；
    \item \textbf{多样性}：手势是否丰富多变，避免长时间静止或重复单一动作。
\end{itemize}

在实验过程中，
VR版本于线下环境进行，
桌面版通过线上远程环境执行。
两种形式均保持实时交流通道，
研究人员可在参与者提问时即时解释操作或澄清评分标准。
在正式评估阶段，
参与者可多次重播当前片段，
但不能返回查看先前内容，
以减少记忆偏差。
所有播放条件（Unity场景内的相机角度、光照参数、音量与分辨率设置）
在全部被试环境中保持一致，
以确保渲染输出的可比性。

需要说明的是，
对于VR实验，
所有测试均在相同的线下实验室环境中进行，
使用同一套PICO设备与照明条件；
而桌面端实验通过远程方式执行，
参与者在各自电脑上运行实验程序。
研究人员可通过实时屏幕共享观察其操作流程并保持语音沟通，
但无法严格控制其所在房间的光照或环境噪声条件。
因此，桌面端实验在观看环境上存在一定差异，
但由于任务内容与播放系统完全相同，
且实验员在测试中持续指导，
可认为该差异对结果的总体影响有限。

\paragraph{实验材料与任务设计}
评估样本来自BEAT数据集中四位演讲者（ID: 2、4、6、8），
其中2、4为男性，6、8为女性。
每位演讲者各选取两段平均长度约1分钟的语音片段，
演讲话题互不重复，共组成8段固定视频样本。
所有实验均使用相同的8段样本，
但其呈现顺序在不同被试间经过随机化或平衡化处理，
以避免顺序效应。
每段视频均包含三种模型生成的动作版本（FaceCapGes、CaMN、DiffSHEG），
并在播放时随机分配每个模型的动画在屏幕中的排序。
参与者在观看每一片演讲音频后，
根据三项主观标准（真实性、同步性、多样性）
对三个模型的手势动画表现进行排名评估。

图~\ref{fig:userstudy_app}中为用户评估工具的实机界面。
画面中共有3个虚拟人模型水平分布，在每一片演讲音频播放时，将3个生成模型的动画随机分配给3个虚拟人的身体骨骼。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudyImage.png}
\bicaption{用户评估工具实机界面}{User Study Interface on the Evaluation Device}
\label{fig:userstudy_app}
\end{figure}

\paragraph{实验参与者}
本实验共邀请16名参与者（12名使用VR设备，4名使用桌面端），
涵盖不同性别。
所有参与者在实验前均接受了操作说明与校准，
并在系统指导下完成评分练习。

为避免呈现顺序对主观印象造成偏差，
另设计了采用平衡拉丁方（Balanced Latin Square）顺序的实验版本，
使不同参与者观看样本的顺序均衡分布。
该版本实验共招募8位VR用户（4男4女），
排序顺序由HCI用户评估工具包\cite{LatinSquareToolkit}自动生成，
确保模型与演讲者组合的呈现顺序在全体被试间均匀分布。
所有条件保持一致，唯一变量为视频播放顺序。

\paragraph{实验环境说明}
为全面验证模型在不同交互场景下的表现稳定性，
本次主观评估设置了桌面端与VR端两种实验环境，
确保覆盖常规屏幕交互与沉浸式交互两类典型应用场景，
具体环境配置如下：

\begin{itemize}
    \item \textbf{桌面端环境}：参与者通过个人电脑或实验室台式机进行评估，
实验界面采用三窗口并行布局，参与者可同时观察左、中、右三个区域的虚拟人动作，
聚焦于整体动作风格、节奏同步性的直观对比，注意力分布于整个屏幕的动作全局表现。
    \item \textbf{VR端环境}：基于 PICO VR 设备\cite{pico4}搭建沉浸式评估场景，参与者佩戴 VR 头显后进入虚拟观测空间，
虚拟人以1:1比例呈现在眼前，观看距离模拟真实人际交流（约 1.2m）。
该环境下参与者注意力更易聚焦于虚拟人上半身细节动作，对空间一致性、动作协调感的感知更敏锐。
\end{itemize}

两种环境的实验流程、评估指标定义及测试样本完全一致，仅通过设备差异构建不同的观察视角与注意力聚焦模式，以验证模型表现的跨设备适配性。

\subsection{结果与分析}
\label{subsec:user_study_result}

本节综合分析两轮用户评估的统计结果与参与者反馈。
所有结果基于 BEAT 测试集中 4 位演讲者（ID:2、4、6、8；2男2女）各 2 段语音片段，共 8 段固定样本。

\paragraph{总体测评结果}
如图~\ref{fig:userstudy}所示，
在 16 名参与者的总体评价中，
FaceCapGes 在三个维度（真实性、同步性、多样性）上均优于基线模型 CaMN，
并在“真实性”维度上略优于离线扩散模型 DiffSHEG。
这一结果表明，FaceCapGes 虽在严格的实时因果约束下运行，
但仍能保持与非实时生成模型相近的动作自然度与流畅性。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudy_All.png}
\bicaption{用户评估总体主观排名结果}{Overall Subjective Ranking Results}
\label{fig:userstudy}
\end{figure}

\paragraph{平衡拉丁方实验结果}
图~\ref{fig:userstudy_latin_square} 展示了平衡拉丁方设置下 8 位 VR 用户的独立结果，
该版本严格控制了模型与演讲者组合的呈现顺序。
结果与总体趋势一致，
且 FaceCapGes 在“真实性”与“同步性”得到了更好的评价。
这表明实验结论在不同顺序条件下保持稳定，
进一步验证了模型主观评价结果的鲁棒性。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudy_LatinSquare.png}
\bicaption{平衡拉丁方设置下的主观排名结果}{Subjective Ranking Results under Balanced Latin Square Design}
\label{fig:userstudy_latin_square}
\end{figure}

\paragraph{用户反馈分析}
根据实验后访谈汇总，
参与者普遍认为 FaceCapGes 的动作过渡自然、节奏感强，
手势响应与语音重音、语调变化更为一致。
我们认为，FaceCapGes 融入的头部姿态信息使手势与身体朝向更贴近真实动作，
可能是其获得较高真实性评价的重要因素之一。

相比之下，CaMN 在头部与上身动作衔接处常出现僵硬或转向延迟的现象，
且头部朝向容易偏离听众方向，从而影响了整体自然度与同步性评分。

对于 DiffSHEG，多数参与者提到其动作丰富度较高，
并倾向于将其视为最具多样性的模型。
这可能与 DiffSHEG 在生成过程中依赖完整文本输入有关：
文本语义边界为动作变化提供了更明确的触发信号，
使其更容易生成幅度更大、变化更频繁的动作模式。
相比之下，FaceCapGes 主要基于实时语音流与感知模态驱动，
缺乏显式语义解析，因此更难实现细粒度的语义级动作响应。

不过，不少参与者也提到 DiffSHEG 在部分片段中存在短暂的手部摆动过快或突然抖动的问题，
从而降低了其真实性或同步性的评分。
该现象可能与 Axis-Angle 旋转表示在数值空间中存在非连续点或奇异性有关，
从而在训练或推理过程中更容易诱发关节角度的突变与抖动。
相比之下，本文模型采用 Rot6D~\cite{rot6d} 旋转表示，
该表示具有更好的连续性与数值稳定性，
有助于缓解由旋转表示引入的突变问题。
在本次用户研究中，FaceCapGes 未出现明显的抖动相关反馈，
生成动作在关节转动与姿态过渡上保持较为连贯自然。

\paragraph{结果讨论}
FaceCapGes 的因果式时间建模与头部姿态融合策略有效提升了局部动作的平滑性与节奏协调，
在不依赖未来输入信息的条件下完成逐帧推理，
更契合在线实时交互场景的因果性约束。
此外，平衡拉丁方版本进一步证明主观结论在不同呈现顺序下的一致性，
排除了顺序偏差对结果的显著影响。

综上，
用户研究表明 FaceCapGes 在在线实时生成条件下
仍能维持与离线模型相近的主观表现，
验证了本文提出的多模态融合与时间建模策略的有效性。

\section{定性分析}

我们强烈建议观看附录中的演示视频（附录~\ref{appendix:videos}），内容包含Ground Truth（GT）、本模型（FaceCapGes）、CaMN与DiffSHEG在不同演讲数据上生成的手势的并排展示，能够直观体现时间对齐性、手势响应性以及头部-身体协调性方面的差异。

为进一步理解模型在实时因果约束下的动态响应行为，
我们对每个模型的生成手势中进行了逐帧观察，
并选取具有代表性的片段进行案例分析。

\subsection{生成动作平滑性}

如图~\ref{fig3} 所示，FaceCapGes 在多个片段中均能平滑地响应说话人的语调变化。
当语调出现明显上升或下降趋势时，本模型生成的双手高度能够随之连续变化，
且动作幅度保持自然，整体运动轨迹连续、关节过渡平稳。
该现象与主观用户评估中参与者对 FaceCapGes “动作过渡自然、节奏感强”的反馈一致。

相比之下，CaMN 的生成动作在语调变化较快的片段中表现出一定的迟滞性：
双手高度调整通常较为缓慢，且动作幅度变化更趋于保守，
导致整体动作轨迹的动态范围较小，视觉上更容易产生“僵硬感”。
这一观察与用户反馈中提到的 CaMN “转向延迟与衔接僵硬”现象相一致。

DiffSHEG 的生成动作整体更活跃，
在部分片段中能够在更细粒度的时间尺度上产生频繁的手势变化。
然而，我们也观察到其在少数时间步出现局部关节的突然加速或短暂抖动，
表现为手部轨迹在相邻帧间产生明显跳变或方向快速反转。
该现象与主观用户反馈中提到的“偶发抖动”一致，
可能与其旋转表示在数值空间中存在非连续点有关，
从而使推理阶段更容易产生局部突变。

总体而言，FaceCapGes 通过采用 Rot6D 表示并结合帧级因果时间建模，
在保证实时性的同时维持了更高的动作连续性与稳定性，
生成结果在视觉平滑性上更接近真实动作序列。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/GeneratedPoseOverview.png}
\bicaption{生成动作效果对比}{Qualitative Comparison of Generated Gestures}
\label{fig3}
\end{figure*}

\subsection{头部朝向与手势空间指向一致性}

为分析头部姿态输入对生成手势空间指向的影响，
我们选取测试集中 GT 头部朝向发生明显偏转的片段，
并对比不同模型在该时刻生成动作的朝向一致性表现。
这里的“空间指向一致性”指生成手势的主要运动方向是否与角色的头部/身体朝向保持匹配，
从而反映模型是否能够利用非语言姿态线索生成更符合交互场景的空间表达。

值得注意的是，CaMN 与 DiffSHEG 在输入端均不显式包含头部姿态信息，
因此其生成的手势方向主要依赖语音或文本内容，难以体现真实头部转向带来的空间指向变化。

\begin{figure*}[h!t]
\centering
\includegraphics[width=0.8\textwidth]{figures/Fig_HeadHandAlignment.png}
\bicaption{头部朝向与手势指向一致性的动作对比}{Comparison of Head Orientation and Gesture Direction Consistency}
\label{fig:headhand_alignment}
\end{figure*}

图~\ref{fig:headhand_alignment} 展示了一个包含明显头部转向变化的片段截图，
从左到右依次为 GT、本模型（FaceCapGes）、CaMN 与 DiffSHEG 的生成结果。
在该片段中，GT 的身体与头部朝向呈现出明确的空间指向，
并且手势多沿着当前头部/躯干朝向展开，体现出面向不同听众时的自然交流习惯。

我们观察到，本模型在多数帧中能够保持与 GT 相近的身体朝向，
并使双手动作的空间指向与头部朝向保持一致，
例如在头部向右侧转动的阶段，生成的手势也倾向于朝向相同方向展开。
相比之下，CaMN 的动作表现更为僵硬，
身体朝向变化较弱且缺乏稳定的空间锚定，手势方向在多帧间呈现随机波动。
DiffSHEG 虽能在其可获取的语音信息下生成较连贯的手势节奏与整体动作幅度，
但由于其输入不包含头部姿态信号，
生成结果难以反映 GT 中由头部转向引起的空间指向变化。

该现象表明，在严格因果与实时输入条件下，
头部姿态作为额外的非语言模态能够为手势生成提供空间指向上的中层约束，
使动作更自然地与说话者的注意方向和交互对象匹配。
这种头-手空间一致性在元宇宙等虚拟交互场景中尤为重要：
当用户面向不同方向的听众或对象进行交流时，
头部转向与与之匹配的手势能够增强空间合理性与沉浸感，
从而提升多方位交流中的可理解性与聆听体验。

\section{客观评估指标与实现细节}
\label{sec:objective_metrics}

为客观层面评价模型在动作自然性、节奏同步性与多样性等方面的表现，
本文在客观评估中采用四项度量：Fréchet Gesture Distance（FGD）\cite{speech_gesture_generation}、语义相关动作召回率（Semantic Relevance Gesture Recall, SRGR）\cite{beatcamn}、节奏对齐度（Beat Alignment, BA）\cite{beatcamn, beatalign}以及L1范数（L1DIV）。
这些指标分别对应生成动作在分布一致性、语音同步性与变化丰富性等不同维度，
共同构成对模型质量的综合评估体系。

\subsection{Fréchet Gesture Distance（FGD）}
\label{subsec:fgd}

FGD\cite{speech_gesture_generation}用于衡量生成手势分布与真实手势分布之间的统计距离，
灵感源自图像生成领域的Fréchet Inception Distance（FID）。
不同于图像任务直接利用Inception网络特征，
在动作生成领域，特征空间需由单独训练的动作自编码器定义。
该自编码器通过重构任务学习手势的潜在表示，使潜在空间具备对运动模式的压缩与区分能力。
在该潜在空间中，假设真实分布与生成分布的高维嵌入向量分别为
$\mathcal{N}(\bm{\mu}_r, \bm{\Sigma}_r)$ 与 $\mathcal{N}(\bm{\mu}_g, \bm{\Sigma}_g)$，
其中，$\mathcal{N}(\cdot)$为高斯分布，$\bm{\mu}$与$\bm{\Sigma}$分别为嵌入特征的均值向量与协方差矩阵。

此时，FGD定义为：
\begin{equation}
\mathrm{FGD} = 
\|\bm{\mu}_r - \bm{\mu}_g\|_2^2 +
\mathrm{Tr}(\bm{\Sigma}_r + \bm{\Sigma}_g - 2(\bm{\Sigma}_r \bm{\Sigma}_g)^{1/2}).
\end{equation}

其中，$\mathrm{Tr}(\cdot)$为矩阵迹运算。

较小的FGD值表示生成动作的统计分布更接近真实数据，
可反映动作的整体自然度与风格一致性。

\paragraph{FGD的模型结构与训练配置}
本文在每位说话人的训练集上分别训练一组评估用自编码器，以避免跨说话人分布差异对指标的干扰。
自编码器输入为以Rot6D表示的上半身骨架序列，
训练目标为最小化位置与速度的重构误差：
\begin{equation}
\mathcal{L}_{AE} = 
\lambda_r \|\hat{\bm{g}} - \bm{g}\|_2^2 +
\lambda_v \|\hat{\bm{g}}' - \bm{g}'\|_2^2,
\label{eq:fgd_ae_loss}
\end{equation}
其中$\lambda_r = 1$, $\lambda_v = 0.1$，$\bm{g}'$为速度序列。

训练配置如下：
输入片段长度设为 32 帧，训练片段切割步长设为 10 帧，
批大小设为 256，编码器与解码器的隐藏层维度均为 128。
优化器采用 SGD\cite{SGDBottou2010LargeScaleML}，
基础学习率设为 $\mathrm{lr}_{base}=1.2\times 10^{-4}$，
并按批大小线性缩放为
\begin{equation}
\mathrm{lr}=\mathrm{lr}_{base}\cdot \frac{\mathrm{batch\_size}}{128}.
\end{equation}
训练过程中仅使用位置与速度重构损失（式（\ref{eq:fgd_ae_loss}）），不包含加速度或对抗项。
自编码器共训练 400 个 epoch。

\paragraph{用于FGD评估的骨架感知自编码器}

在基线模型CaMN的FGD评估中，采用了一种基于时间卷积的嵌入式自编码器（embedding-based autoencoder）进行手势特征提取。
该结构将每一帧姿态平铺为高维向量输入，并对各关节分量进行独立建模。
在实际使用中我们观察到，这类嵌入空间对旋转表示的数值尺度较为敏感。
当采用Rot6D表示时，不同关节与分量之间的不均衡方差可能在潜在空间中被进一步放大，
从而导致协方差估计条件较差，并引发FGD数值不稳定的问题。

为提高FGD评估的鲁棒性，本文采用了一种骨架拓扑感知的自编码器作为特征提取器。
该模型在编码阶段显式引入骨架邻接矩阵$A$，并通过对相邻关节进行局部卷积实现结构约束。
具体而言，第$l$层中关节$i$的特征向量$\mathbf{h}_i^{(l)} \in \mathbb{R}^{d_l}$，
其中$d_l$表示第$l$层中每个关节节点的特征维度。
通过聚合其邻域$\mathcal{N}(i)$内相邻关节$j$的特征$\mathbf{h}_j^{(l)}$进行更新，
其中$A_{ij}$表示骨架邻接矩阵中节点$j$到节点$i$的连接权重，
即
\begin{equation}
A_{ij} =
\begin{cases}
1, & \text{若关节}i\text{与}j\text{在骨架拓扑中直接相连，或}i=j; \\
0, & \text{否则}.
\end{cases}
\end{equation}
此外，设$W^{(l)} \in \mathbb{R}^{d_{l+1}\times d_l}$与$b^{(l)} \in \mathbb{R}^{d_{l+1}}$
分别为第$l$层的可学习权重矩阵与偏置项。设$\sigma(\cdot)$为非线性激活函数，在本文实现中取为双曲正切函数$\tanh(\cdot)$。
上述局部邻域聚合过程可形式化表示为：
\begin{equation}
\mathbf{h}_i^{(l+1)} =
\sigma\left(
\sum_{j=1}^{J} A_{ij} W^{(l)} \mathbf{h}_j^{(l)} + b^{(l)}
\right),
\end{equation}
这种基于局部邻域的权重共享机制有助于保持人体运动的空间结构一致性，
并在特征提取过程中缓解由旋转表示差异带来的数值尺度放大问题。

实验结果表明，该骨架感知自编码器在Rot6D下产生更加稳定的潜在分布统计量。
在本文的实验设置中，该设计有效提升了FGD评估的数值稳定性，
并有效避免了在Rot6D表示条件下使用嵌入式自编码器时出现的极端FGD数值现象。

\subsection{Semantic Relevance Gesture Recall（SRGR）}
\label{subsec:srgr}

SRGR指标（Semantic Relevance Gesture Recall）\cite{beatcamn}
用于衡量生成手势在语义相关时间段内与真实手势在数值层面的匹配程度。
该指标关注的是语音语义触发的关键手势是否被正确生成，
而非整体分布一致性或语音–手势节奏相关性。

与基于分布的评估指标（如 FGD）不同，
SRGR 属于基于阈值的逐帧召回率度量，
通过统计生成手势在允许误差范围内命中的比例，
反映模型在语义相关动作重现方面的准确性。

\paragraph{定义与原理}
在本文实现中，SRGR 作用于关节的旋转表示。
设真实手势序列与生成手势序列在第 $t$ 帧第 $j$ 个关节的旋转表示
分别为 $\mathbf{r}_{t,j}$ 与 $\hat{\mathbf{r}}_{t,j}$，
其中 $\mathbf{r}_{t,j} \in \mathbb{R}^{6}$ 为关节的 Rot6D 表示，
$T$ 为序列总帧数，$J$ 为关节数量。

在给定旋转表示误差阈值 $\delta$ 的条件下，
若生成关节旋转与真实关节旋转之间的表示差异满足
$\|\mathbf{r}_{t,j} - \hat{\mathbf{r}}_{t,j}\|_1 < \delta$，
则认为该关节在该时刻被成功召回。
在本文实验中，阈值固定设为 $\delta = 0.5$。

SRGR通过对所有时间帧与关节进行统计，
并引入语义相关性权重 $\lambda_t$，
定义为：
\begin{equation}
\mathrm{SRGR} =
\frac{1}{T J}
\sum_{t=1}^{T}
\sum_{j=1}^{J}
\lambda_t \,
\mathbb{I}
\left(
\|\mathbf{r}_{t,j} - \hat{\mathbf{r}}_{t,j}\| < \delta
\right),
\end{equation}
其中 $\mathbb{I}(\cdot)$ 为指示函数。

语义相关性权重 $\lambda_t$ 用于强调语音中语义显著时间片段
（如强调词或语义触发点）对应的手势匹配程度，由 BEAT 数据集提供的语义标注确定，
从而使 SRGR 更加关注语义相关手势的召回情况，
而非对所有时间片段进行均匀统计。

\subsection{Beat Alignment（BA）}
\label{subsec:ba}

Beat Alignment（BA）指标用于衡量语音节拍事件与手势关键动作事件在时间轴上的对齐程度，
反映模型在语音–手势时序同步性（temporal synchronization）方面的性能。
本文采用 BEAT/CaMN 中使用的 BeatAlign 指标\cite{beatcamn}，其原始形式由 Li 等人提出于 AI Choreographer 工作中\cite{beatalign}，
并可视为音频节拍与动作节拍集合之间的单向 Chamfer 相似度度量。

与 SRGR 基于阈值的逐帧数值匹配不同，
BA 关注的是离散事件层面的时间对齐关系，
即手势关键动作是否在时间上合理地响应了语音中的重读节拍。

\paragraph{定义与原理}
设语音节拍事件集合为
$\mathcal{B}_s = \{ t^s_k \}$，
通过对语音信号的能量相关特征进行起始点检测（onset detection）得到。
具体而言，首先基于谱能量变化计算 onset strength 曲线，
并在该曲线上进行峰值检测以获得候选语音事件位置。
随后，引入 Root Mean Square（RMS）特征作为短时能量幅度的描述，
并在 RMS 曲线上对检测到的起始点进行 backtracking 校正，
从而将语音节拍事件定位至能量实际开始上升的位置，
以提高时间定位的稳定性与准确性。

手势关键动作事件集合为
$\mathcal{B}_g = \{ t^g_m \}$，
定义为关节运动速度的局部极小值点，
对应动作中的停顿或方向变化等显著运动事件。

对于每一个语音节拍事件$t^s_k$，
计算其与所有手势关键动作事件之间的最小时间偏差：
\begin{equation}
\Delta t_k = \min_{m} | t^s_k - t^g_m | .
\end{equation}
随后采用高斯核函数将时间偏差映射为相似度分数，
从而得到单个语音节拍的对齐得分。
最终BA指标定义为：
\begin{equation}
\mathrm{BA} =
\frac{1}{|\mathcal{B}_s|}
\sum_{k}
\exp\left(
-\frac{(\Delta t_k)^2}{2\sigma^2}
\right),
\end{equation}
其中$\sigma$为时间尺度参数，本文中取$\sigma=0.3$，用于控制对齐容忍范围。

该定义可视为从语音节拍集合到手势节拍集合的单向 Chamfer 相似度，
当语音节拍与手势关键动作在时间上高度对齐时，BA 值接近 1；
反之，当二者时间偏差较大时，BA 值趋近于 0。

本文使用 BA 指标评估模型在语音重读节拍与手势关键动作之间的时间同步能力，
该指标与主观观察到的语音–手势同步自然度通常具有较好一致性。

\subsection{L1范数}
\label{subsec:l1div}

L1范数（L1DIV）指标\cite{beatcamn}用于衡量模型生成手势序列的多样性，
即不同生成样本之间在动作空间中的差异程度。
该指标反映模型在避免生成结果收敛到平均动作模式
（mode collapse）的同时，
是否能够保持足够丰富的动作变化。

\paragraph{定义与原理}
设模型在评估过程中生成 $N$ 个手势序列样本，
第 $i$ 个生成样本在第 $t$ 帧第 $j$ 个关节的旋转表示为
$\hat{\mathbf{r}}^{(i)}_{t,j}$，
其中 $\hat{\mathbf{r}}^{(i)}_{t,j} \in \mathbb{R}^{6}$ 为关节的 Rot6D 表示，
$T$ 为序列总帧数，$J$ 为关节数量。

L1DIV 通过计算不同生成样本之间在所有时间帧与关节上的平均 L1 距离，
来刻画生成动作分布的离散程度。
其数学形式定义为：
\begin{equation}
\mathrm{L1DIV} =
\frac{1}{N(N-1)}
\sum_{i<k}
\frac{1}{T J}
\sum_{t=1}^{T}
\sum_{j=1}^{J}
\left\|
\hat{\mathbf{r}}^{(i)}_{t,j}
-
\hat{\mathbf{r}}^{(k)}_{t,j}
\right\|_1 .
\end{equation}

较高的L1范数表明模型生成具有较强的多样性，
但过高可能意味着动作不稳定或噪声放大。
因此，L1范数通常与FGD联合分析：  
FGD 反映真实度，L1范数反映丰富度，
两者共同平衡模型在自然性—多样性维度上的表现。

\subsection{评估区域设定与公平性说明}
由于本文方法 FaceCapGes 在输入端显式引入了真实的头部姿态作为额外模态，
并在输出中生成包含头部旋转的上半身骨骼序列，
若直接将头部旋转也计入整体误差，
可能会对不使用头部姿态输入的对比方法（如 CaMN 与 DiffSHEG）造成不公平的优势。
为保证跨方法的可比性，
本文在所有定量评估指标中分别报告两种评估区域：

（1）\textbf{上半身（含头部）}：包含头部与上半身全部关节旋转；

（2）\textbf{上半身（不含头部）}：在计算指标时移除头部关节的旋转分量，仅统计身体与手臂部分。

具体而言，在计算 FGD、SRGR、BA 与 L1DIV 时，
我们将头部关节的旋转维度从指标计算中排除，
从而消除头部旋转差异对指标的直接影响。

\section{定量评估结果}
\begin{table*}[h]
\bicaption{定量评估结果}{Quantitative Evaluation Results}
\centering
\begin{tabular}{@{}lclrrrr@{}}
\hline
区域 & 方法 & FGD\textdownarrow & SRGR\textuparrow & BA\textuparrow & L1DIV\textuparrow \\
\hline
\multirow{3}{*}{上半身（含头部）}
& CaMN       & 47.732 & 0.098 & 0.845 & 7.591 \\
& DiffSHEG   & \underline{26.846} & \textbf{0.109} & \underline{0.883} & \underline{11.028} \\
& 本模型     & \textbf{23.385} & \underline{0.107} & \textbf{0.913} & \textbf{13.284} \\
\hline
\multirow{3}{*}{上半身（不含头部）}
& CaMN       & 49.437 & 0.103 & 0.845 & 7.327 \\
& DiffSHEG   & \underline{26.847} & \textbf{0.110} & \underline{0.883} & \underline{10.990} \\
& 本模型     & \textbf{23.384} & \underline{0.107} & \textbf{0.913} & \textbf{13.330} \\
\hline
\end{tabular}
\label{tab1}
\end{table*}

表~\ref{tab1}显示，FaceCapGes在所有指标上均优于CaMN。
值得注意的是，“上半身（含头部）”与“上半身（不含头部）”两种评估区域下的结果趋势基本一致，
表明本文方法的性能优势并非仅来源于头部姿态输入或头部输出的额外信息。

此外，表~\ref{tab1} 还显示，与DiffSHEG相比，本模型FGD更低，SRGR相近，且在L1范数上表现最优，
表明其生成动作具备良好多样性。
但这一结论与用户主观评分存在一定出入：DiffSHEG在多样性上主观排名更高。

这个现象可能来自于L1范数的局限性：它主要衡量空间偏离程度，
不能直接体现动作的颗粒度，或用户感知上的丰富性。
虽然我们的模型在动作结构上更丰富，但用户普遍认为DiffSHEG的动作更活跃，
在合理的时机做出了更多吸引注意的动作。

\section{消融实验分析}

我们围绕两个核心设计展开消融：

（1）头部姿态输入是否能提供有效的空间与时序线索；

（2）帧级自回归生成与滑动窗口训练是否优于片段级一次性解码。

为此，我们构造了三种变体：移除头部姿态输入、移除帧级生成策略，以及基线 CaMN。

所有消融实验均基于 BEAT 数据集的第2位说话人进行，
训练与测试划分与主实验保持一致。

表~\ref{tab2} 展示了各模块在四个指标上的结果。

\begin{table*}[h]
\bicaption{消融实验结果}{Ablation Study Results}
\centering
\begin{tabular}{@{}llrrrr@{}}
\hline
区域 & 变体 & FGD\textdownarrow & SRGR\textuparrow & BA\textuparrow & L1DIV\textuparrow \\
\hline
\multirow{4}{*}{上半身（含头部）}
& CaMN                    & 32.870  & 0.111  & 0.858  & 7.214  \\
& 移除头部姿态           & 19.591  & 0.123  & 0.916  & 10.642 \\
& 移除帧级生成           & 21.592  & \textbf{0.125}  & 0.892  & 10.456 \\
& FaceCapGes（本模型）   & \textbf{19.290}  & 0.123 & \textbf{0.918}  & \textbf{10.871} \\
\hline
\end{tabular}
\label{tab2}
\end{table*}

\paragraph{头部姿态输入的贡献}
对比“移除头部姿态”与完整模型可以发现，
尽管两者在 SRGR 上差异不大（均为 0.123），
但完整模型在 FGD、BA 与 L1DIV 上均取得更优表现，
尤其在动作分布一致性（FGD）与多样性（L1DIV）上呈现出稳定收益。
这表明头部姿态作为非语言模态提供了额外的空间方向与交互焦点线索，
能够帮助模型在生成手势时保持更一致的身体朝向与动作指向，
从而提升动作自然度与表达丰富性。

需要注意的是，该提升幅度相对温和，
原因可能在于语音与面部表情已包含较强的节奏与情绪提示，
头部姿态主要补充空间层面的约束，
因此其收益更集中地反映在分布与多样性相关指标上。

\paragraph{帧级自回归生成的优势}
值得注意的是，“移除帧级生成”版本允许利用未来上下文并采用双向 LSTM，
但其整体表现仍不及帧级版本，
尤其在 FGD 上出现较为明显的退化（FGD 从 19.290 上升至 21.592）。

一种合理解释是，该变体在训练阶段以独立窗口为单位进行优化，
而在测试阶段采用整段演讲作为长序列输入并一次性生成完整动作序列。
这种训练与推理的序列长度范围不一致，
可能导致模型在长序列推理时的隐状态传播方式偏离训练分布，
从而使生成动作在嵌入空间中的统计特性偏离真实数据分布，
表现为生成分布与真实动作分布的距离增大（FGD 上升）。

尽管本模型同样采用窗口化训练，
但在每个窗口内通过滑动窗口展开与纯自回归预测进行多步生成，
并对每步输出的误差累计取平均作为优化目标，
使模型在训练阶段即暴露于自身预测分布并学习局部动力学的稳定性。
因此训练目标与在线推理过程保持一致，
对推理阶段序列长度变化所引入的分布偏移具有更强鲁棒性。

\paragraph{与基线模型 CaMN 的差异}
相比基线 CaMN，本模型在所有指标上均显著提升，
其中 FGD 从 32.870 降至 19.290，表明生成分布更接近真实动作；
同时 BA 与 L1DIV 的提高说明动作更平衡且更具表达多样性。
这进一步验证了本文引入的因果时序建模、头部模态补充与滑动窗口训练策略
对于实时交互场景下的手势生成具有有效作用。

此外，基线 CaMN 采用欧拉角而本模型采用 Rot6D 表示，
该表示差异亦可能部分解释性能提升幅度较大的原因。

\section{性能评估}

\subsection{单帧推理性能}
\label{sec:performance_test}
FaceCapGes 作为端到端实时手势生成框架的核心计算模块，其性能评估聚焦于单帧输入，单帧输出的核心推理流程，
即模型接收当前帧的语音、面部表情与头部姿态多模态输入，实时输出对应帧的上半身手势骨骼动画。

为模拟真实应用中的实时输入流场景，性能测试基于 BEAT 数据集的测试集展开，
关闭批处理机制，确保每帧数据均独立输入模型进行推理，还原逐帧处理的实际运行状态。
测试过程中，我们将测试集中总计93015帧的多模态输入数据传入模型，记录从首帧输入到末帧输出的总推理耗时，
通过总时长与测试帧数的比值计算平均单帧处理时间。
测试时使用的硬件配置为单张 RTX4090 硬件。

测试结果如表~\ref {tab3} 所示，模型平均单帧处理时间为 6.07 毫秒，
具备良好的实时响应能力。

\begin{table}[h]
\bicaption{推理速度评估结果}{Inference Speed Evaluation Results}
\centering
\begin{tabular}{@{}lr@{}}
\hline
指标 & 时间 \\
\hline
测试帧数 & 93015 (f) \\
推理总时长 & 504 (s) \\
平均单帧时间 & 6.07E-03 (s/f) \\
\hline
\end{tabular}
\label{tab3}
\end{table}

\subsection{端到端计算链路延迟}
结合第~\ref{sec:system}节所述的端到端框架流程，
系统端到端延迟可按时间顺序拆解为数据采集、特征提取、模型推理与结果返回四个核心阶段。
各阶段性能消耗及瓶颈分析如下：

\begin{itemize}
    \item \textbf{数据采集阶段：}
    依赖设备端传感器实时捕获多模态信号，主要耗时来源于 ARKit 面部与头部姿态追踪。
    根据官方文档标注，在 iOS 设备上 ARKit 的目标追踪帧率为 $60\,\mathrm{FPS}$~\cite{AppleARKitTrackingGuide}，
    对应输入更新周期约为 $16.7\,\mathrm{ms}$。
    该阶段耗时由设备端硬件算力与系统负载决定。

    \item \textbf{特征提取阶段：}
    将原始传感器数据转换为模型可识别的结构化特征（语音 Mel 频谱、面部 BlendShape 系数、头部 Rot6D 旋转参数）。
    测试原型中采用 Librosa 对测试集音频进行离线特征提取，
    93015 帧数据的总计算耗时约 61 秒，对应的平均单帧计算成本约为 $0.66\,\mathrm{ms}$。
    需要注意的是，该统计不包含实时 I/O 与缓冲管理等系统开销，
    但可用于估计音频特征提取的计算量级。
    实际部署时可替换为 PyAudio 等实时流式提取工具，因此该阶段预计不构成主要性能瓶颈。

    \item \textbf{模型推理阶段：}
    为端到端流程的核心计算环节，结合第~\ref{sec:performance_test}节的模型推理性能测试，
    在单张 RTX4090 硬件、
    无批处理（Batch=1）配置下，
    单帧推理耗时为 $6.07\,\mathrm{ms}$。
    该阶段耗时与硬件算力强相关，是由神经网络结构设计决定的主要性能变量。

    \item \textbf{结果返回阶段：}
    将模型输出的骨骼姿态参数传输至渲染引擎，耗时可忽略（通常低于 $0.1\,\mathrm{ms}$），不构成性能瓶颈。
\end{itemize}

在本地推理设置下，若将一次响应链路定义为单帧采集完成后立即进入后续计算，
则计算链路延迟可近似表示为：
\begin{equation}
t_{\mathrm{e2e}} \approx t_{\mathrm{ARKit}} + t_{\mathrm{feat}} + t_{\mathrm{infer}} + t_{\mathrm{return}},
\end{equation}
其中 $t_{\mathrm{ARKit}}\approx16.7\,\mathrm{ms}$，
$t_{\mathrm{feat}}<1\,\mathrm{ms}$，
$t_{\mathrm{infer}}=6.07\,\mathrm{ms}$，
$t_{\mathrm{return}}<0.1\,\mathrm{ms}$。
因此在该设置下，计算链路理论延迟可视为 $25\,\mathrm{ms}$ 以下。

需要注意的是，该估计未计入音频特征提取的窗口缓存与渲染端同步可能引入的额外等待，
实际交互延迟还将受渲染刷新周期影响。

\paragraph{远程推理部署的额外开销}
在移动端等算力受限的平台上，
模型推理可部署于远程服务器并通过网络进行输入输出传输。
此时端到端延迟需进一步加上网络往返与序列化开销：
\begin{equation}
t_{\mathrm{e2e}}^{\mathrm{remote}}
\approx
t_{\mathrm{ARKit}} + t_{\mathrm{feat}} + t_{\mathrm{net}} + t_{\mathrm{infer}} + t_{\mathrm{return}},
\end{equation}
其中 $t_{\mathrm{net}}$ 表示网络传输与通信开销，
其大小由网络条件与系统实现决定，本文不在此展开测量。

\subsection{系统更新率}
在本实验设置下，模型在单张 RTX4090 上的单帧推理时间为 $6.07\,\mathrm{ms}$，
对应的理论推理吞吐约为 $165\,\mathrm{FPS}$，
说明模型推理阶段在当前硬件条件下不会成为实时交互中的性能瓶颈。
需要强调的是，该数值仅反映神经网络推理吞吐能力，并不等同于系统端到端更新率。

然而，端到端系统的实际更新率仍将受到输入采集频率与渲染刷新率的共同约束。
根据官方文档，ARKit 面部与头部追踪通常以 $60\,\mathrm{FPS}$ 为目标更新率~\cite{AppleARKitTrackingGuide}，
因此在采用 ARKit 作为面部与头部输入来源的应用场景中，
系统可获得的相关输入更新频率理论上限可视为 $60\,\mathrm{Hz}$，
相应地，系统的有效输出更新率亦不会超过 $60\,\mathrm{FPS}$。
在远程推理部署下，网络传输与同步开销可能进一步降低实际更新率。

\subsection{帧率设定的可扩展性}
本文实现中采用 $15\,\mathrm{FPS}$ 的动作时间采样率进行训练与输出，
主要原因在于与 BEAT 数据集预处理及基线模型的设置保持一致，以便进行公平对比。
该采样率选择并不构成本模型的固有限制。

但需要注意的是，本文模型的输入模态不包含帧间时间间隔 $\Delta t$ 的显式信息，
因此训练过程中隐式假设固定的离散时间步长（即 $\Delta t = 1/15\,\mathrm{s}$）。
当部署环境的输入采样率或输出刷新率发生变化（例如 ARKit 为 $60\,\mathrm{FPS}$）时，
若直接使用 $15\,\mathrm{FPS}$ 训练的模型进行逐帧推理，
模型可能会对运动速度与节奏尺度产生偏差，从而影响手势动态表现。

因此，在目标运行帧率与训练帧率不一致的情况下，
更稳妥的做法是对数据集进行对应帧率的重采样并重新训练模型，
以确保训练时域与实际系统时域一致，从而使模型学习到正确的动力学时间尺度。
未来也可进一步探索将 $\Delta t$ 作为显式条件输入，提高模型对不同采样率输入的鲁棒性。

在实际部署中，系统可依据端到端管线中的主要瓶颈选择合适的目标动作帧率：
当推理计算为瓶颈（如移动端或低算力设备）时，可维持较低帧率以确保每帧按时生成；
当输入采样率与计算资源允许（例如更高刷新率的追踪）时，
可采用更高帧率训练版本以获得更细粒度的时间响应与动作细节表达。

\section{本章小结}

本章围绕 FaceCapGes 的生成质量与实时性能开展了系统评估。

在主观评估方面，本章结合两轮用户评估对模型在真实感、同步性与多样性三个维度进行了分析，
结果显示本方法整体优于基线模型 CaMN，并在真实感维度上与离线模型表现相近。
结合用户反馈与访谈，本章进一步讨论了头部姿态信息在提升动作朝向一致性与真实感方面的潜在贡献。
此外，通过对生成结果的定性分析，我们观察到本方法能够随头部朝向变化生成方向一致的手势动作，
从而增强交互场景下的空间合理性。

在客观评估方面，本文采用 FGD、SRGR、BA 与 L1DIV 等指标，
分别从动作分布一致性、语音节奏对齐、动作平衡性与多样性等角度对比了 CaMN 与 DiffSHEG 等方法，
并通过“含头部/不含头部”两种评估区域设定保证指标对比的公平性。
在此基础上，本章进一步通过消融实验分析了头部姿态输入与帧级自回归生成策略对性能提升的贡献，
验证了各模块设计的有效性。

最后，本章对模型推理速度、端到端计算链路延迟与系统更新率进行了评估，
表明 FaceCapGes 在保持较高生成质量的同时具备实时交互所需的低延迟响应能力。

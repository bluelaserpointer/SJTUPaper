% !TEX root = ../main.tex

\chapter{评估}

为评估生成手势的质量，我们将生成的手势动画与其它研究方法做对比，展开客观指标测量与主观评测。
此外，我们还对本文模型的性能进行了分析。

\section{客观评估指标与实现细节}
\label{sec:objective_metrics}

为全面评价模型在动作自然性、节奏同步性与多样性等方面的表现，
本文在客观指标层面采用四项度量：Fréchet Gesture Distance（FGD）\cite{speech_gesture_generation}、语义相关动作召回率（Semantic Relevance Gesture Recall, SRGR）\cite{beatcamn}、节奏对齐度（Beat Alignment, BA）\cite{beatcamn, beatalign}以及L1范数（L1DIV）。
这些指标分别对应生成动作在分布一致性、语音同步性与变化丰富性等不同维度，
共同构成对模型质量的综合评估体系。

\subsection{Fréchet Gesture Distance（FGD）}
\label{subsec:fgd}

FGD\cite{speech_gesture_generation}用于衡量生成手势分布与真实手势分布之间的统计距离，
灵感源自图像生成领域的Fréchet Inception Distance（FID）。
不同于图像任务直接利用Inception网络特征，
在动作生成领域，特征空间需由单独训练的动作自编码器定义。
该自编码器通过重构任务学习手势的潜在表示，使潜在空间具备对运动模式的压缩与区分能力。
在该潜在空间中，假设真实分布与生成分布的高维嵌入向量分别为
$\mathcal{N}(\bm{\mu}_r, \bm{\Sigma}_r)$ 与 $\mathcal{N}(\bm{\mu}_g, \bm{\Sigma}_g)$，
其中，$\mathcal{N}(\cdot)$为高斯分布，$\bm{\mu}$与$\bm{\Sigma}$分别为嵌入特征的均值向量与协方差矩阵。

此时，FGD定义为：
\begin{equation}
\mathrm{FGD} = 
\|\bm{\mu}_r - \bm{\mu}_g\|_2^2 +
\mathrm{Tr}(\bm{\Sigma}_r + \bm{\Sigma}_g - 2(\bm{\Sigma}_r \bm{\Sigma}_g)^{1/2}).
\end{equation}

其中，$\mathrm{Tr}(\cdot)$为矩阵迹运算。

较小的FGD值表示生成动作的统计分布更接近真实数据，
可反映动作的整体自然度与风格一致性。

\paragraph{评估模型结构与训练配置}
本文在每位说话人的训练集上分别训练一组评估用自编码器，以避免跨说话人分布差异对指标的干扰。
自编码器输入为以Rot6d表示的上半身骨架序列，
训练目标为最小化位置与速度的重构误差：
\begin{equation}
\mathcal{L}_{AE} = 
\lambda_r \|\hat{\bm{g}} - \bm{g}\|_2^2 +
\lambda_v \|\hat{\bm{g}}' - \bm{g}'\|_2^2,
\label{eq:fgd_ae_loss}
\end{equation}
其中$\lambda_r = 1$, $\lambda_v = 0.1$，$\bm{g}'$为速度序列。

训练配置如下：
输入片段长度设为 32 帧，训练片段切割步长设为 10 帧，
批大小设为 256，编码器与解码器的隐藏层维度均为 128。
优化器采用 SGD\cite{bottou2010sgd}，
基础学习率设为 $\mathrm{lr}_{base}=1.2\times 10^{-4}$，
并按批大小线性缩放为
\begin{equation}
\mathrm{lr}=\mathrm{lr}_{base}\cdot \frac{\mathrm{batch\_size}}{128}.
\end{equation}
训练过程中仅使用位置与速度重构损失（式（\ref{eq:fgd_ae_loss}）），不包含加速度或对抗项。
自编码器共训练 400 个 epoch。

\paragraph{用于FGD评估的骨架感知自编码器}

在基线模型CaMN的FGD评估中，采用了一种基于时间卷积的嵌入式自编码器（embedding-based autoencoder）进行手势特征提取。
该结构将每一帧姿态平铺为高维向量输入，并对各关节分量进行独立建模。
在实际使用中我们观察到，这类嵌入空间对旋转表示的数值尺度较为敏感。
当采用Rot6d表示时，不同关节与分量之间的不均衡方差可能在潜在空间中被进一步放大，
从而导致协方差估计条件较差，并引发FGD数值不稳定的问题。

为提高FGD评估的鲁棒性，本文采用了一种骨架拓扑感知的自编码器作为特征提取器。
该模型在编码阶段显式引入骨架邻接矩阵$A$，并通过对相邻关节进行局部卷积实现结构约束。
具体而言，第$l$层中关节$i$的特征向量$\mathbf{h}_i^{(l)} \in \mathbb{R}^{d_l}$，
其中$d_l$表示第$l$层中每个关节节点的特征维度。
通过聚合其邻域$\mathcal{N}(i)$内相邻关节$j$的特征$\mathbf{h}_j^{(l)}$进行更新，
其中$A_{ij}$表示骨架邻接矩阵中节点$j$到节点$i$的连接权重，
即
\begin{equation}
A_{ij} =
\begin{cases}
1, & \text{若关节}i\text{与}j\text{在骨架拓扑中直接相连，或}i=j; \\
0, & \text{否则}.
\end{cases}
\end{equation}
此外，设$W^{(l)} \in \mathbb{R}^{d_{l+1}\times d_l}$与$b^{(l)} \in \mathbb{R}^{d_{l+1}}$
分别为第$l$层的可学习权重矩阵与偏置项。设$\sigma(\cdot)$为非线性激活函数，在本文实现中取为双曲正切函数$\tanh(\cdot)$。
上述局部邻域聚合过程可形式化表示为：
\begin{equation}
\mathbf{h}_i^{(l+1)} =
\sigma\left(
\sum_{j=1}^{J} A_{ij} W^{(l)} \mathbf{h}_j^{(l)} + b^{(l)}
\right),
\end{equation}
这种基于局部邻域的权重共享机制有助于保持人体运动的空间结构一致性，
并在特征提取过程中缓解由旋转表示差异带来的数值尺度放大问题。

实验结果表明，该骨架感知自编码器在Rot6d下产生更加稳定的潜在分布统计量。
在本文的实验设置中，该设计显著提升了FGD评估的数值稳定性，
并有效避免了在Rot6d表示条件下使用嵌入式自编码器时出现的极端FGD数值现象。

\subsection{Semantic Relevance Gesture Recall（SRGR）}
\label{subsec:srgr}

SRGR指标（Semantic Relevance Gesture Recall）\cite{beatcamn}
用于衡量生成手势在语义相关时间段内与真实手势在数值层面的匹配程度。
该指标关注的是语音语义触发的关键手势是否被正确生成，
而非整体分布一致性或语音–手势节奏相关性。

与基于分布的评估指标（如 FGD）不同，
SRGR 属于基于阈值的逐帧召回率度量，
通过统计生成手势在允许误差范围内命中的比例，
反映模型在语义相关动作重现方面的准确性。

\paragraph{定义与原理}
在本文实现中，SRGR 作用于关节的旋转表示。
设真实手势序列与生成手势序列在第 $t$ 帧第 $j$ 个关节的旋转表示
分别为 $\mathbf{r}_{t,j}$ 与 $\hat{\mathbf{r}}_{t,j}$，
其中 $\mathbf{r}_{t,j} \in \mathbb{R}^{6}$ 为关节的 Rot6d 表示，
$T$ 为序列总帧数，$J$ 为关节数量。

在给定旋转表示误差阈值 $\delta$ 的条件下，
若生成关节旋转与真实关节旋转之间的表示差异满足
$\|\mathbf{r}_{t,j} - \hat{\mathbf{r}}_{t,j}\|_1 < \delta$，
则认为该关节在该时刻被成功召回。
在本文实验中，阈值固定设为 $\delta = 0.5$。

SRGR通过对所有时间帧与关节进行统计，
并引入语义相关性权重 $\lambda_t$，
定义为：
\begin{equation}
\mathrm{SRGR} =
\frac{1}{T J}
\sum_{t=1}^{T}
\sum_{j=1}^{J}
\lambda_t \,
\mathbb{I}
\left(
\|\mathbf{r}_{t,j} - \hat{\mathbf{r}}_{t,j}\| < \delta
\right),
\end{equation}
其中 $\mathbb{I}(\cdot)$ 为指示函数。

语义相关性权重 $\lambda_t$ 用于强调语音中语义显著时间片段
（如强调词或语义触发点）对应的手势匹配程度，由 BEAT 数据集提供的语义标注确定，
从而使 SRGR 更加关注语义相关手势的召回情况，
而非对所有时间片段进行均匀统计。

\subsection{Beat Alignment（BA）}
\label{subsec:ba}

Beat Alignment（BA）指标用于衡量语音节拍事件与手势关键动作事件在时间轴上的对齐程度，
反映模型在语音–手势时序同步性（temporal synchronization）方面的性能。
本文采用 BEAT/CaMN 中使用的 BeatAlign 指标\cite{beatcamn}，其原始形式由 Li 等人提出于 AI Choreographer 工作中\cite{beatalign}，
并可视为音频节拍与动作节拍集合之间的单向 Chamfer 相似度度量。

与 SRGR 基于阈值的逐帧数值匹配不同，
BA 关注的是离散事件层面的时间对齐关系，
即手势关键动作是否在时间上合理地响应了语音中的重读节拍。

\paragraph{定义与原理}
设语音节拍事件集合为
$\mathcal{B}_s = \{ t^s_k \}$，
通过对语音信号的能量相关特征进行起始点检测（onset detection）得到。
具体而言，首先基于谱能量变化计算 onset strength 曲线，
并在该曲线上进行峰值检测以获得候选语音事件位置。
随后，引入 Root Mean Square（RMS）特征作为短时能量幅度的描述，
并在 RMS 曲线上对检测到的起始点进行 backtracking 校正，
从而将语音节拍事件定位至能量实际开始上升的位置，
以提高时间定位的稳定性与准确性。

手势关键动作事件集合为
$\mathcal{B}_g = \{ t^g_m \}$，
定义为关节运动速度的局部极小值点，
对应动作中的停顿或方向变化等显著运动事件。

对于每一个语音节拍事件$t^s_k$，
计算其与所有手势关键动作事件之间的最小时间偏差：
\begin{equation}
\Delta t_k = \min_{m} | t^s_k - t^g_m | .
\end{equation}
随后采用高斯核函数将时间偏差映射为相似度分数，
从而得到单个语音节拍的对齐得分。
最终BA指标定义为：
\begin{equation}
\mathrm{BA} =
\frac{1}{|\mathcal{B}_s|}
\sum_{k}
\exp\left(
-\frac{(\Delta t_k)^2}{2\sigma^2}
\right),
\end{equation}
其中$\sigma$为时间尺度参数，本文中取$\sigma=0.3$，用于控制对齐容忍范围。

该定义可视为从语音节拍集合到手势节拍集合的单向 Chamfer 相似度，
当语音节拍与手势关键动作在时间上高度对齐时，BA 值接近 1；
反之，当二者时间偏差较大时，BA 值趋近于 0。

本文使用 BA 指标评估模型在语音重读节拍与手势关键动作之间的时间同步能力，
该指标与主观观察到的语音–手势同步自然度通常具有较好一致性。

\subsection{L1范数}
\label{subsec:l1div}

L1范数（L1DIV）指标\cite{beatcamn}用于衡量模型生成手势序列的多样性，
即不同生成样本之间在动作空间中的差异程度。
该指标反映模型在避免生成结果收敛到平均动作模式
（mode collapse）的同时，
是否能够保持足够丰富的动作变化。

\paragraph{定义与原理}
设模型在评估过程中生成 $N$ 个手势序列样本，
第 $i$ 个生成样本在第 $t$ 帧第 $j$ 个关节的旋转表示为
$\hat{\mathbf{r}}^{(i)}_{t,j}$，
其中 $\hat{\mathbf{r}}^{(i)}_{t,j} \in \mathbb{R}^{6}$ 为关节的 Rot6D 表示，
$T$ 为序列总帧数，$J$ 为关节数量。

L1DIV 通过计算不同生成样本之间在所有时间帧与关节上的平均 L1 距离，
来刻画生成动作分布的离散程度。
其数学形式定义为：
\begin{equation}
\mathrm{L1DIV} =
\frac{1}{N(N-1)}
\sum_{i<k}
\frac{1}{T J}
\sum_{t=1}^{T}
\sum_{j=1}^{J}
\left\|
\hat{\mathbf{r}}^{(i)}_{t,j}
-
\hat{\mathbf{r}}^{(k)}_{t,j}
\right\|_1 .
\end{equation}


较高的L1范数表明模型生成具有较强的多样性，
但过高可能意味着动作不稳定或噪声放大。
因此，L1范数通常与FGD联合分析：  
FGD 反映真实度，L1范数反映丰富度，
两者共同平衡模型在自然性—多样性维度上的表现。

\subsection{客观评估设置}
所有评估均在BEAT数据集上进行，选用说话人编号为2、4、6和8。生成的身体姿态与真实标签均采用相同骨架拓扑的BVH格式。
FGD训练方式由EMAGE\cite{emage}提取，L1范数、BA和SRGR的实现使用CaMN\cite{beatcamn}提供的算法。
各指标结果在四位说话人上取平均，以减少个体差异带来的偏差。

\section{模型对比概览}

表~\ref{tab:modalitycomparison}总结了各模型的输入输出模态特征。值得注意的是，我们的模型是唯一同时支持“头部姿态”输入且完全不依赖未来信息的方案。

\begin{table}[h]
\caption{实验对比模型的输入输出模态对比}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lccccc|c|c@{}}
\toprule
\multirow{2}{*}{模型} & \multicolumn{5}{c|}{输入模态} & \multirow{2}{*}{未来信息} & \multirow{2}{*}{输出} \\ 
\cmidrule(lr){2-6}
 & 音频 & 面部捕捉 & 头部姿态 & 说话人ID & 情绪 &  &  \\ 
\midrule
CaMN       & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 身体 \\
DiffSHEG   & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & 身体+面部 \\
本方法     & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\ast$\footnotemark & $\times$ & $\times$ & 身体 \\ 
\bottomrule
\footnote{模型结构未显式输入说话人ID，采用每位说话人独立训练的设置，说话人ID通过模型参数指定。}
\end{tabular}%
}
\label{tab:modalitycomparison}
\end{table}

\section{用户调研}
\label{sec:userstudy}

为进一步验证模型在真实交互环境中的表现，
本文进行了用户主观评估实验，
比较FaceCapGes、DiffSHEG\cite{diffsheg}与CaMN\cite{beatcamn}三个模型在动作自然性、同步性与多样性方面的主观质量。
本节首先介绍用户研究系统与实验配置，
随后报告主观评价结果与分析。

\subsection{评估系统与实验配置}

\paragraph{实验材料与呈现方式}
用户评估所使用的手势动画均基于测试集语音片段生成，
并以Biovision Hierarchy（BVH）文件形式保存。
BVH是一种通用的动作捕捉数据格式，
通过层级化定义骨骼结构与帧级旋转参数，
可直接导入3D动画与虚拟人系统。

本文使用的BVH文件采用欧拉角旋转表示。
由于三种对比模型的输出旋转参数形式不同——
CaMN模型直接输出欧拉角，
DiffSHEG输出Axis-Angle，
而FaceCapGes输出Rot6d表示——
因此在导出BVH之前，
需将后两者统一转换为欧拉角表示，
以便在后续动画播放中使用统一渲染流程。

所有对比模型（FaceCapGes、CaMN、DiffSHEG）均使用相同的训练集与骨骼定义，
其中CaMN与DiffSHEG采用各自论文公开的原版参数。
虚拟人角色选自BEAT数据集提供的公开演讲者模型集合，
并经过Unity\cite{unity}的Mecanim\cite{unitymechanim}自动骨骼绑定系统匹配。
该系统会自动配对BVH文件中定义的骨骼层级与虚拟人模型的骨骼节点，
从而在不依赖手动权重绘制的情况下完成动作重定向。

本系统当前支持的虚拟人模型需同时具备骨骼绑定与ARKit\cite{ARKitDocumentation}兼容的BlendShape参数。
基于此约束，实验选用了CaMN论文公开的男女两名演讲者模型，二者均满足兼容要求，可实现身体与面部的联合驱动。

\paragraph{播放系统实现}
我们基于Unity自行编写播放脚本，
将各模型生成的BVH动画用于驱动虚拟人身体骨骼，
同时以面部捕捉序列驱动BlendShape表情参数，
并同步播放原始语音音频。
系统支持同时呈现三种模型生成的动画结果：
用户可在同一画面中（左、中、右）并行观察三种手势表现，
所有语音与面部表情完全一致，
唯一变量为身体动作。
该设计使参与者能够直接比较不同模型在动作风格、
节奏响应与语音同步性方面的差异。

为确保主观评价的公正性与可重复性，
系统在每次实验开始前会随机分配三种模型的位置（左、中、右），
界面上不会显示模型名称，
从而避免潜在偏向。
各测试片段的播放顺序在实验前统一设定，
以保证不同参与者之间的样本顺序均衡。
实验员在播放系统后台记录当前序列与模型对应关系，
以便后续结果统计。

\paragraph{实验界面与设备}
评估系统提供桌面端与VR端两种版本，功能完全一致。
VR版本基于PICO设备\cite{pico4}实现；
桌面版支持多窗口并行播放，方便用户同时对比。
如图~\ref{fig:userstudy_app}所示，
播放界面在两种设备上保持统一布局，
播放完成后参与者需通过交互界面对三个模型进行排序打分。
VR用户在沉浸式环境中逐一观看三段动画；
桌面端用户则可在单屏上同时观察全部模型。
因此前者注重细节感知与临场性，
后者更有利于整体风格与节奏的一致性对比。

\paragraph{实验流程与指导}
实验正式开始前，
研究人员向参与者说明了三项主观评价标准的含义，
确保所有被试对评分维度理解一致：
\begin{itemize}
    \item \textbf{真实性}：整体动作是否自然流畅，是否存在明显的违和感，如朝向异常或突然抖动；
    \item \textbf{同步性}：手势动作与语调、语音节奏是否协调一致；
    \item \textbf{多样性}：手势是否丰富多变，避免长时间静止或重复单一动作。
\end{itemize}

在实验过程中，
VR版本于线下环境进行，
桌面版通过线上远程环境执行。
两种形式均保持实时交流通道，
研究人员可在参与者提问时即时解释操作或澄清评分标准。
在正式评估阶段，
参与者可多次重播当前片段，
但不能返回查看先前内容，
以减少记忆偏差。
所有播放条件（Unity场景内的相机角度、光照参数、音量与分辨率设置）
在全部被试中保持一致，
以确保渲染输出的可比性。

需要说明的是，
对于VR实验，
所有测试均在相同的线下实验室环境中进行，
使用同一套PICO设备与照明条件；
而桌面端实验通过远程方式执行，
参与者在各自电脑上运行实验程序。
研究人员可通过实时屏幕共享观察其操作流程并保持语音沟通，
但无法严格控制其所在房间的光照或环境噪声条件。
因此，桌面端实验在“观看环境”上存在一定差异，
但由于任务内容与播放系统完全相同，
且实验员在测试中持续指导，
可认为该差异对结果的总体影响有限。

\paragraph{实验材料与任务设计}
评估样本来自BEAT数据集中四位演讲者（ID: 2、4、6、8），
其中2、4为男性，6、8为女性。
每位演讲者各选取两段平均长度约1分钟的语音片段，
演讲话题互不重复，共组成8段固定视频样本。
所有实验均使用相同的8段样本，
但其呈现顺序在不同被试间经过随机化或平衡化处理，
以避免顺序效应。
每段视频均包含三种模型生成的动作版本（FaceCapGes、CaMN、DiffSHEG），
并在播放时随机分配每个模型的动画在屏幕中的排序。
参与者在观看每一片演讲音频后，
根据三项主观标准（真实性、同步性、多样性）
对三个模型的手势动画表现进行排名评估。

图~\ref{fig:userstudy_app}中为用户调研工具的实机界面。
画面中共有3个虚拟人模型水平分布，在每一片演讲音频播放时，将3个生成模型的动画随机分配给3个虚拟人的身体骨骼。
这包括肩膀、手臂、手指、脊椎、以及脖子关节。
同时，将数据集中的面捕数据赋给所有虚拟人面部表情。
因此，三个模型的音频与面部表情完全一致，只有身体姿态不同。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudyImage.png}
\caption{用户调用工具实机界面}
\label{fig:userstudy_app}
\end{figure}

\paragraph{实验参与者}
共邀请16名参与者（12名使用VR设备，4名使用桌面端），
涵盖不同性别与学术背景。
所有参与者在实验前均接受了操作说明与校准，
并在系统指导下完成评分练习。
为避免呈现顺序对主观印象造成偏差，
另设计了采用平衡拉丁方（Balanced Latin Square）顺序的实验版本，
使不同参与者观看样本的顺序均衡分布。

\paragraph{实验环境说明}
为全面验证模型在不同交互场景下的表现稳定性，
本次主观评估设置了桌面端与VR 端两种实验环境，
确保覆盖常规屏幕交互与沉浸式交互两类典型应用场景，
具体环境配置如下：

\begin{itemize}
    \item \texbf{桌面端环境}：参与者通过个人电脑或实验室台式机进行评估，屏幕尺寸为 14-27 英寸，观看距离约 50-80cm。
实验界面采用三窗口并行布局，参与者可同时观察左、中、右三个区域的虚拟人动作，
聚焦于整体动作风格、节奏同步性的直观对比，注意力分布于整个屏幕的动作全局表现。
    \item \texbf{VR 端环境}：基于 PICO VR 设备\cite{pico4}搭建沉浸式评估场景，参与者佩戴 VR 头显后进入虚拟观测空间，
虚拟人以 1:1 比例呈现在眼前，观看距离模拟真实人际交流（约 1~1.5m）。
该环境下参与者注意力更易聚焦于虚拟人上半身细节动作，对空间一致性、动作连贯性及临场协调感的感知更敏锐。
\end{itemize}

两种环境的实验流程、评估指标定义及测试样本完全一致，仅通过设备差异构建不同的观察视角与注意力聚焦模式，以验证模型表现的跨设备适配性。

\subsection{平衡拉丁方排序设计}
\label{subsec:latin_square}

该版本实验共招募8位VR用户（4男4女），
排序顺序由HCI用户调研工具包\cite{LatinSquareToolkit}自动生成，
确保模型与演讲者组合的呈现顺序在全体被试间均匀分布。
所有条件保持一致，唯一变量为视频播放顺序。

\subsection{结果与分析}
\label{subsec:user_study_result}

本节综合分析两轮用户评估的统计结果与参与者反馈。
所有结果均基于相同的8段测试样本，
其中模型位置与播放顺序在不同被试间随机化或经平衡拉丁方控制，
以保证主观评价的公正性。

\paragraph{电脑用户测评结果}
如图~\ref{fig:userstudy}所示，
在16名参与者的总体评价中，
FaceCapGes在三个维度（真实性、同步性、多样性）上均优于基线模型CaMN，
并在“真实性”方面与离线扩散模型DiffSHEG持平。
这一结果表明，FaceCapGes虽在严格的实时因果约束下运行，
但仍能保持与非实时生成模型相近的动作自然度与流畅性。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudy_All.png}
\caption{电脑用户对三种生成模型的主观排名结果}
\label{fig:userstudy}
\end{figure}

\paragraph{平衡拉丁方实验结果}
图~\ref{fig:userstudy_latin_square}展示了平衡拉丁方实验版本中8位VR用户的独立结果，
该版本严格控制了模型与演讲者组合的呈现顺序。
结果与总体趋势一致，
FaceCapGes在“同步性”与“真实性”上显著优于CaMN，
并在“多样性”指标上与DiffSHEG接近。
这表明实验结果在不同顺序条件下保持稳定，
进一步验证了模型在多维度主观评价中的一致优势。

\begin{figure}[h!t]
\centering
\includegraphics[width=\linewidth]{figures/UserStudy_LatinSquare.png}
\caption{平衡拉丁方实验版本中虚拟现实用户对三种生成模型的主观排名结果}
\label{fig:userstudy_latin_square}
\end{figure}

\paragraph{用户反馈分析}
根据实验后访谈与自由评论汇总，
参与者普遍认为 FaceCapGes 的动作过渡自然、节奏感强，手势响应与语音重音、语调变化更加一致。
部分 VR 用户指出，在沉浸式视角下能够明显感受到手势的连贯性与时序协调，
而 CaMN 在头部与上身动作衔接处常有僵硬或转向延迟的现象，且头部朝向容易偏离听众。
%稍微弱化

约三分之一的参与者提到 DiffSHEG 的动作表现力最强，
其核心优势在于能够在单词级短间隔上生成丰富细腻的动作。
推测这一表现源于 DiffSHEG 依赖完整文本输入进行语义识别，
能够清晰捕捉单词分界线，进而为每个语义单元匹配对应的精细动作；
而 FaceCapGes 基于实时语音流与感知模态驱动，缺乏显式语义解析，
仅能在说话人出现明显声调变化或情绪波动时产生动作起伏，难以实现单词级别的细腻动作响应。
%哪里来的比例？

不过，DiffSHEG 在部分片段中存在手部摆动幅度过大或突然抖动的问题，这一现象对真实性评分造成了负面影响。
结合两种实验环境的观察差异发现：桌面端用户因观察距离较远、注意力覆盖整个屏幕，基本都能察觉到这类抖动；
而 VR 用户的注意力更聚焦于虚拟人核心交互区域，除非刻意注视手部细节，否则较难发现抖动问题。

这一差异源于两种环境下的注意力范围区别，桌面端的观察覆盖全局，更易捕捉动作异常，
而 VR 端的沉浸式聚焦的视野可能忽略局部抖动，该因素在一定程度上影响了本次主观评分结果。
进一步对比两类用户的评价倾向：桌面端用户普遍认为三种模型的差异在节奏与流畅性上最明显；
VR 用户则更易察觉动作的空间一致性与临场协调。尽管存在观察视角的差异，但两组被试对模型的整体排序趋势一致，
说明模型在自然性、同步性与多样性等核心维度的表现具有跨设备一致性。
%评价倾向更多的是真实性的差异

\paragraph{结果讨论}
CaMN 得分较低的主要原因在于其采用欧拉角表示，导致部分姿态序列在旋转空间中出现不连续，
尤其在头部动作上易产生跳变；
而 DiffSHEG 的高方差输出虽提升了动作多样性，
但偶尔表现出突然的大角度关节瞬间转动，
这一现象的潜在原因与 Axis-Angle 的表示特性直接相关。
Axis-Angle以旋转轴与角度描述姿态，其角度参数存在周期性循环特性（取值范围为 $[0, 2\pi)$），
跨越周期边界时数值发生跳动，导致模型训练过程中易学到数值跳跃的映射关系，
进而在推理时偶尔产生关节转动的突变与抖动。

相比之下，本文模型采用 Rot6d\cite{rot6d}旋转表示，其将 $3\times3$ 旋转矩阵的前两列展开为 6 维向量，
既规避了欧拉角的万向节锁问题，又解决了 Axis-Angle 的周期性不连续缺陷，实现了旋转空间的全局平滑性。
这一特性使 FaceCapGes 在整个测试过程中未收到任何关于动作抖动的用户反馈，生成的手势在关节转动、
姿态过渡上均保持连贯自然，呈现了Rot6d表示在手势生成任务中对运动平滑性的显著提升作用。

FaceCapGes 的因果式时间建模和头部姿态融合策略有效提升了局部动作的平滑性与节奏协调，
且全程不依赖任何未来输入信息，仅通过当前及历史帧的多模态数据完成推理，完全契合在线实时交互场景的因果性约束。
此外，实验的平衡拉丁方版本进一步证明结果在不同呈现顺序下的一致性，
排除了顺序偏差对主观评价的显著影响。

综上，
用户研究表明FaceCapGes在在线实时生成条件下
仍能维持与离线模型相当的主观表现，
在动作真实性、节奏同步性和长时间交互稳定性方面
均显著优于传统因果结构基线，
验证了本文提出的多模态融合与时间建模策略的有效性。

\section{定性分析}

我们强烈建议观看电子附录中的演示视频，内容包含GT、CaMN、DiffSHEG与本模型的并排展示，能够直观体现时间对齐性、手势响应性以及头-身协调性方面的差异。

如图~\ref{fig3} 所示，FaceCapGes能平滑且富有表现力地捕捉说话人动态。与CaMN相比，本模型避免了欧拉角带来的不连续问题，生成的身体动作可顺应头部运动趋势。与DiffSHEG相比，我们的模型在动作速度上略逊一筹，但能有效避免抖动或夸张姿态。头部姿态的引入也明显提升了手势方向与语义的一致性。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/GeneratedPoseOverview.png}
\caption{GT、CaMN、DiffSHEG与FaceCapGes的动作效果对比。本模型手势响应自然，方向与头部朝向一致。}
\label{fig3}
\end{figure*}

\section{定量分析}

\begin{table*}[h]
\caption{定量评估结果}
\centering
\begin{tabular}{@{}lclrrrr@{}}
\hline
区域 & 方法 & FGD\textdownarrow & SRGR\textuparrow & BA\textuparrow & L1DIV\textuparrow \\
\hline
\multirow{3}{*}{全身}
& CaMN       & 47.732 & 0.098 & 0.845 & 7.591 \\
& DiffSHEG   & \underline{26.846} & \textbf{0.109} & \underline{0.883} & \underline{11.028} \\
& 本方法     & \textbf{23.385} & \underline{0.107} & \textbf{0.913} & \textbf{13.284} \\
\hline
\multirow{3}{*}{不含头部}
& CaMN       & 49.437 & 0.103 & 0.845 & 7.327 \\
& DiffSHEG   & \underline{26.847} & \textbf{0.110} & \underline{0.883} & \underline{10.990} \\
& 本方法     & \textbf{23.384} & \underline{0.107} & \textbf{0.913} & \textbf{13.330} \\
\hline
\end{tabular}
\label{tab1}
\end{table*}

表~\ref{tab1}显示，FaceCapGes在所有指标上均优于CaMN。与DiffSHEG相比，本模型FGD更低，SRGR相近，且在L1范数上表现最优，表明其生成动作具备良好多样性。但这一结论与用户主观评分存在一定出入：DiffSHEG在多样性上主观排名更高。

这个现象可能来自于L1范数的局限性：它主要衡量空间偏离程度，不能直接体现视觉表现力，或捕捉感知上的丰富性。虽然我们的模型在结构上更丰富，但用户普遍认为DiffSHEG的动作更活跃，在合理的时机做出了许多吸引注意的动作。

\section{消融实验分析}

\begin{table*}[h]
\caption{消融实验结果}
\centering
\begin{tabular}{@{}llrrrr@{}}
\hline
区域 & 变体 & FGD\textdownarrow & SRGR\textuparrow & BA\textuparrow & L1DIV\textuparrow \\
\hline
\multirow{4}{*}{全身}
& CaMN                    & 32.870  & 0.111  & 0.858  & 7.214  \\
& 移除头部姿态           & 19.591  & 0.123  & 0.916  & 10.642 \\
& 移除帧级生成           & 21.592  & \textbf{0.125}  & 0.892  & 10.456 \\
& FaceCapGes（本方法）   & \textbf{19.290}  & 0.123 & \textbf{0.918}  & \textbf{10.871} \\
\hline
\end{tabular}
\label{tab2}
\end{table*}

表~\ref{tab2}展示了各模块对模型性能的影响。移除头部姿态会基本导致所有指标上的表现下降，证明其对表达性手势的重要性。

值得注意的是，非帧级版本使用未来上下文与双向LSTM，但其表现反而不及帧级版本。这可能是因为它采用一次性解码，对每段输入独立处理，无法保证时间连续性，与原CaMN的设计一致。而我们的方法通过滑动窗口训练和自回归推理，逐帧依赖历史信息，使动作更连贯、过渡更自然。即使不使用未来帧，也能获得更低的FGD和更自然的手势对齐。%需要更多验证

\section{性能评估}

FaceCapGes 作为端到端实时手势生成框架的核心计算模块，其性能评估聚焦于单帧输入，单帧输出的核心推理流程，
即模型接收当前帧的语音、面部表情与头部姿态多模态输入，实时输出对应帧的上半身手势骨骼动画。
为模拟真实应用中的实时输入流场景，性能测试基于 BEAT 数据集的测试集展开，
关闭批处理机制，确保每帧数据均独立输入模型进行推理，还原逐帧处理的实际运行状态。
测试过程中，我们将测试集中总计93015帧的多模态输入数据传入模型，记录从首帧输入到末帧输出的总推理耗时，
通过总时长与测试帧数的比值计算平均单帧处理时间。
测试时使用的硬件配置为单张 RTX4090 硬件。

测试结果如表～\ref {tab3} 所示，模型平均单帧处理时间为 6.07 毫秒，
具备良好的实时响应能力。

\begin{table}[h]
\caption{推理速度评估结果}
\centering
\begin{tabular}{@{}lr@{}}
\hline
指标 & 时间 \\
\hline
测试帧数 & 93015 (f) \\
推理总时长 & 504 (s) \\
平均单帧时间 & 6.07E-03 (s/f) \\
\hline
\end{tabular}
\label{tab3}
\end{table}

\paragraph{端到端总延迟分析}
结合前文所述的端到端框架流程，系统总延迟按时间顺序可拆解为数据采集 - 特征提取 - 模型推理 - 结果返回四个核心阶段，各阶段性能消耗及瓶颈分析如下：
数据采集阶段：依赖设备端传感器实时捕获多模态信号，主要耗时来源于 ARKit 面部与头部姿态追踪。根据官方文档标注，ARKit 原生追踪帧率为 60FPS\cite {AppleARKitTrackingGuide}，对应单帧追踪延迟约 16.7 毫秒，该延迟由设备硬件算力决定，属于固定消耗项。
特征提取阶段：需将原始传感器数据转换为模型可识别的结构化特征（语音 Mel 频谱、面部 BlendShape 系数、头部 Rot6D 旋转参数）。测试原型中采用 Librosa 离线提取音频特征，93015 帧数据提取总耗时 61 秒，单帧耗时不足 1 毫秒；实际部署时替换为 PyAudio 等实时提取工具后，单帧耗时可控制在 0.5-1 毫秒，属于低消耗阶段。
模型推理阶段：为端到端流程的核心计算环节，在 RTX4090 硬件、Batch=1 配置下，单帧推理延迟为 6.07 毫秒，该阶段耗时与硬件算力强相关，是跨设备部署时的主要性能变量。
结果返回阶段：将模型输出的骨骼姿态参数传输至渲染引擎，耗时可忽略（通常低于 0.1 毫秒），不构成性能瓶颈。
基于上述拆解，结合不同部署场景的硬件差异，以下为两类典型场景的端到端总延迟量化分析：
场景一：内置 RTX4090 的台式机本地部署：数据采集（16.7ms）+ 特征提取（0.8ms）+ 模型推理（6.07ms）+ 结果返回（0.1ms），总延迟约 23.67 毫秒，完全满足实时交互需求（通常要求延迟＜30ms），无明显性能瓶颈。
场景二：手机依赖远程服务器（搭载 RTX4090）部署：需额外叠加数据传输延迟（5G 网络下单帧双向传输延迟约 5-10 毫秒，Wi-Fi 网络下约 10-20 毫秒）。叠加后总延迟为：手机端数据采集（16.7ms）+ 数据上传（5-20ms）+ 服务器端特征提取（0.8ms）+ 模型推理（6.07ms）+ 结果下载（5-20ms）+ 手机端渲染返回（0.1ms），总延迟范围约 33.67-63.67 毫秒。其中，网络传输延迟是核心瓶颈，5G 网络下可接近实时需求，Wi-Fi 网络下需通过边缘计算或模型轻量化进一步优化。
综上，本地部署场景下系统总延迟可控且无明显瓶颈，远程部署场景的核心优化方向为降低网络传输延迟，整体框架的实时性在主流硬件与网络环境下均具备实用价值。
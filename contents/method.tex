% !TEX root = ../main.tex

\chapter{方法}

\section{研究定位与总体设计思路}
手势可根据语义依赖性与时间结构复杂度区分为可语义生成与可韵律生成两大类。
本文的研究聚焦于严格实时的语音驱动任务，在此条件下模型无法访问未来语音或完整语义，
因此重点生成与语音韵律同步的节奏型手势，
并通过面部与头部模态的联合输入进一步增强表达性与自然度。

基于上述定位，本文提出了一种基于音频、面部BlendShape权重和头部姿态输入的
帧级多模态级联手势生成模型FaceCapGes。
模型旨在在实时条件下实现自然、同步且具有一定指向性的上半身动作生成，
在不依赖语义理解或未来上下文的前提下，
通过多模态输入弥补语音模态预测能力的不足。

为避免与系统级说明混淆，本文模型在第~\ref{sec:system}~节首先给出端到端系统定位；
随后在第~\ref{sec:problem}~节形式化定义任务与符号体系；
在第~\ref{sec:cascade}~节详细阐述模型的级联结构与输入模态设计；
最后在第~\ref{sec:training}~节说明训练过程与优化方法；
并在第~\ref{sec:implementation}~节介绍实现细节与训练配置。

\section{系统整体框架与模块定位}
\label{sec:system}
本节介绍整个系统的端到端驱动流程及模块职责划分。如图~\ref{fig:system_architecture}所示，系统整体架构由五个层级组成：用户配置层、设备层、中间件层、手势生成模型层以及渲染与驱动层。各层之间通过多模态信号接口进行连接，实现从信号采集到虚拟人动作生成的端到端实时处理。

FaceCapGes模型位于中间层，承担多模态输入到上半身姿态输出的核心推理任务，而输入采集与渲染模块分别负责信号获取与结果展示。

为实现基于语音、面部捕捉与头部姿态的实时数字人驱动系统，本文构建了完整的信号采集、动作生成与渲染展示的处理管线。FaceCapGes模型作为该系统的核心计算模块，负责在实时约束下从多模态输入推理出当前帧的上半身骨骼姿态。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/SystemArchitecture.png}
\caption{系统整体架构与数据流示意图}
\label{fig:system_architecture}
\end{figure*}

\subsection{信号采集与系统配置层}
该层位于系统整体架构的输入端，用于从用户端设备实时获取多模态信号，并在系统初始化阶段完成运行参数的配置。整体结构可划分为设备层、中间件层与用户配置层三个部分，如图~\ref{fig:system_architecture} 所示。

\paragraph{设备层}
设备层负责采集语音与视觉模态信号。语音信号由麦克风实时录制，采样率与帧移可根据运行设备性能调整；视觉信号由前置深度相机摄像头获取面部深度图与视频流，并作为ARKit面部追踪模块的输入。

\paragraph{中间件层}
中间件层通过Apple提供的ARKit框架\cite{ARKitDocumentation}，将设备层的原始图像流与深度图转化为结构化特征。ARKit输出两类主要数据：  
(1) \textbf{面部表情特征}
ARKit提供52维BlendShape系数向量，用于描述关键肌肉群的局部形变状态。该特征能够反映用户的表情、口型与情感变化，并以帧级形式同步输出。  
(2) \textbf{头部姿态特征} 
ARKit在ARFaceAnchor中提供一个齐次变换矩阵$\mathbf{T}\in\mathbb{R}^{4\times4}$，
用于描述人脸锚点相对会话世界坐标系的位姿：
\begin{equation}
\mathbf{T} =
\begin{bmatrix}
\mathbf{R} & \mathbf{t} \\
\mathbf{0}^{\top} & 1
\end{bmatrix},
\quad
\mathbf{R}\in\mathbb{R}^{3\times3},\ 
\mathbf{t}\in\mathbb{R}^{3\times1}.
\end{equation}
其中左上角的$\mathbf{R}$为旋转矩阵，右上角的$\mathbf{t}$为平移向量。
本研究从矩阵中提取旋转部分 $\mathbf{R}$，
并将其转换为Rot6D~\cite{rot6d}表示形式，
以提升旋转空间的连续性与模型训练的稳定性。

同时，音频流在中间件层中被传入特征提取模块以生成时间序列特征。模型训练阶段使用Librosa库离线提取Mel频谱、短时能量与基频$F_0$等声学特征，以保证特征精度与一致性。系统运行阶段可由等价的实时特征提取模块（如torchaudio或TensorFlow Audio）逐帧生成对应特征，以实现端到端的低延迟运行。

\paragraph{用户配置层}
用户配置层负责系统初始化阶段的模型与参数设定。用户可在应用中选择说话风格，对应加载不同说话人ID配置下的模型权重。该配置仅在系统启动时生效，不参与实时推理过程。

本层提供的多模态信号经中间件处理后，以统一的数据接口传递至手势生成模型，实现语音、表情与头部姿态的实时融合输入。

\subsection{手势生成模型层（FaceCapGes）}

FaceCapGes模块位于系统的中间层，是本文提出的核心计算单元。该模块接收来自信号采集与系统配置层的三类输入特征：语音特征、面部BlendShape系数以及头部姿态参数，并在不依赖未来帧的条件下，逐帧预测用户当前时刻的上半身骨骼姿态。

生成的骨骼姿态采用Rot6D\cite{rot6d}连续旋转表示形式，覆盖上半身47个关节的旋转参数。模型内部通过级联多模态编码结构提取时序相关特征，并利用单向LSTM解码器完成时间依赖建模，从而在保持实时性的同时，生成与语音节奏、表情变化及头部朝向高度一致的自然手势。

FaceCapGes输出的姿态数据通过统一接口传递至渲染与驱动模块，与实时面部捕捉信号共同驱动虚拟角色的整体动作。由于模型仅依赖当前与历史帧输入，可与输入层以固定帧率并行运行，实现端到端的低延迟推理。

\subsection{渲染驱动层}

该模块位于系统输出端，负责将手势生成模型与面部捕捉结果共同转化为虚拟人的实时动作表现。
系统将FaceCapGes模型输出的上半身骨骼姿态与ARKit实时检测的52维面部BlendShape系数传递至渲染引擎，
由引擎内的模块解析并映射至目标虚拟人的骨骼与表情控制接口，从而实现多模态动作驱动。

渲染模块采用基于GPU的蒙皮计算与实时光照模型，以确保动画的平滑性和视觉一致性。
最终，系统能够在实时流式输入条件下稳定运行，
同步呈现语音、表情与身体动作，
以自然流畅的数字人形象实现从多模态信号输入到可视化输出的完整驱动流程。

\section{问题定义}
\label{sec:problem}

在整体系统中，FaceCapGes模块承担着从多模态输入信号到上半身骨骼姿态预测的核心任务。
为了明确模型的输入输出结构与学习目标，本节对该问题进行形式化定义。

\subsection{任务描述}

目标是在实时条件下，根据用户当前时刻的语音、面部表情与头部姿态信息，预测其对应的上半身骨骼姿态。模型需能够逐帧生成与语音节奏、面部动态和头部转动方向相协调的自然手势动作，而不依赖未来的输入帧或整句语音信息。

形式上，可以将该任务定义为一个多模态时序映射函数：
\begin{equation}
\hat{\bm{v}}_{t}^{B} = f_\theta\!\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big),
\end{equation}
其中$f_\theta$表示由参数$\theta$控制的预测模型，$N$为历史窗口长度。
%
各模态输入定义如下：
$\bm{v}_{t}^{A}$表示语音模态在时刻$t$的特征向量，由麦克风信号经特征提取模块得到；
$\bm{v}_{t}^{F}$表示面部模态的输入，为ARKit输出的52维标准化BlendShape系数；
$\bm{v}_{t}^{H}$表示头部模态的输入，为ARKit得出的头部旋转矩阵经Rot6D表示；
而$\hat{\bm{v}}_{t}^{B}$为模型在当前时刻预测的上半身骨骼姿态向量。
%
模型仅利用当前及过去$N$帧的输入信息估计$\hat{\bm{v}}_{t}^{B}$，
从而满足严格的实时推理约束。

\subsection{输入与输出模态}

FaceCapGes模型的输入由三种可同时实时获取的模态组成：语音特征、面部BlendShape权重及头部姿态参数；输出为当前帧的上半身骨骼旋转状态。各模态的符号与维度如表~\ref{tab:modalities}所示。

\begin{table}[h]
\centering
\caption{输入输出模态符号与维度}
\label{tab:modalities}
\begin{tabular}{llll}
\toprule
\textbf{模态} & \textbf{符号} & \textbf{维度} & \textbf{描述} \\
\midrule
语音特征 & $\bm{v}_t^{A}$ & $\mathbb{R}^{1067}$ & 由音频信号提取的时序特征（Mel频谱、短时能量、基频等） \\
面部 BlendShape & $\bm{v}_t^{F}$ & $\mathbb{R}^{52}$ & ARKit输出的标准化表情权重向量 \\
头部姿态 & $\bm{v}_t^{H}$ & $\mathbb{R}^{6}$ & 采用Rot6d表示的头部旋转参数 \\
骨骼姿态（输出） & $\hat{\bm{v}}_t^{B}$ & $\mathbb{R}^{6 \times 47}$ & 上半身47个关节的旋转状态 \\
\bottomrule
\end{tabular}
\end{table}

输入序列$\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big)$描述了用户在过去$N$帧内的语音与表情动态信息。模型通过学习其时序变化规律，逐帧生成对应的骨骼姿态输出$\hat{\bm{v}}_t^{B}$。在推理阶段，模型仅访问至时刻$t$的输入序列，无法访问任何未来帧信息，保证了生成过程的因果性与实时性。

\subsection{学习目标与优化形式}

在训练阶段，给定来自多模态语音动作数据集（如BEAT）的配对样本：
\begin{equation}
\big(\bm{v}_t^{A},\, \bm{v}_t^{F},\, \bm{v}_t^{H},\, \bm{v}_t^{B}\big)
\end{equation}
模型的学习目标是最小化预测姿态 $\hat{\bm{v}}_t^{B}$与真实姿态$\bm{v}_t^{B}$之间的差异。
综合考虑空间重构误差与时序平滑性约束，总体优化目标可表示为：
\begin{equation}
\mathcal{L}_{total} =
\lambda_r  \mathcal{L}_{rec} + \lambda_v \mathcal{L}_{vel} + \lambda_a \mathcal{L}_{acc} + \lambda_{adv} \mathcal{L}_{adv}
\end{equation}
其中：
\begin{itemize}
    \item $\mathcal{L}_{rec}$为姿态重构损失，衡量单帧旋转角度误差；
    \item $\mathcal{L}_{vel}$、$\mathcal{L}_{acc}$分别约束预测序列的速度与加速度连续性；
    \item $\mathcal{L}_{adv}$为对抗损失，用于提升生成手势的自然性；
    \item $\lambda_r, \lambda_v, \lambda_a, \lambda_{adv}$ 为对应的权重系数，实验中分别设为
    $5\times10^{2}$、$10^{3}$、$10^{3}$、$10^{-1}$。
\end{itemize}

通过最小化上述综合损失，模型能够在不依赖未来帧的条件下生成自然、流畅且与语音节奏相匹配的上半身动作序列。

\section{级联架构与输入模态设计}
\label{sec:cascade}

\subsection{级联架构的原理与理论背景}

现有语音驱动手势生成模型多采用多模态融合结构，其中以CaMN\cite{beatcamn}为代表的级联架构在设计理念上具有代表性。其核心思想是将语音、面部表情与身体动作视为语义表达的不同层级：语音模态承担语义与节奏驱动作用，面部模态反映情感与意图，身体动作则是语言与情绪的外化呈现。CaMN 采用自上而下的处理顺序，即依次对语音、面部和动作模态进行建模，从而以层次化结构保持模态间的语义依存关系。

这种设计符合人类交流中“语言、表情、动作”一体化的认知规律\cite{mcneill_1992_hand, kendon_2004_gesture}。语音先规划语义与节奏，面部表情作为情绪强化信号随后产生，最终通过身体动作完成完整的非语言表达。模型中，语音编码器输出的时间嵌入被输入至面部编码器，再与面部特征融合后驱动动作解码器，从而保持语义一致性并增强表现力。

然而，CaMN的原始设计面向离线整句生成任务，需要访问未来上下文以维持全局连贯性。在实时场景下，这种依赖将引入显著延迟并破坏因果性。FaceCapGes在继承其层次思想的同时，对输入模式、训练方式与模态选择进行了系统性重构，以满足帧级实时约束。

\subsection{基线模态继承与实时适配}
FaceCapGes保留了CaMN的语音与面部模态结构，但针对实时生成任务进行了适配性改写。

\subsubsection{说话人ID分支移除}
如图~\ref{fig:system_architecture}所示，用户配置层会设置说话人ID配置用于模型切换，但该模态在本模型中不属于网络输入。
在基线模型 CaMN 中，输入模态包含显式的说话人ID向量，用于在同一模型内区分不同演讲者的风格差异。
然而在实时交互场景下，该分支并非必要：用户身份通常固定，且说话风格的变化频率远低于帧级推理速度。
因此，FaceCapGes移除了ID输入分支，并采用“单说话人训练”策略，即针对每个说话人独立训练模型参数。
实验表明，该方式能在保持收敛稳定的同时显著提升动作的自然性与节奏一致性。
从系统使用角度看，不同模型可视为“说话风格配置文件”，用户仅在需要时切换对应参数，该操作发生频率低，不会影响实时推理性能。

\subsubsection{输入模态继承}

语音特征通过时间卷积网络（Temporal Convolutional Network, TCN）和多层感知机（Multilayer Perceptron, MLP）编码，以捕捉短时节奏模式；面部模态采用相似结构，并在中间层融合语音嵌入，从而增强语音与表情之间的语义关联。  
语音编码器$E_A$与面部编码器$E_F$的输出定义为：
\begin{equation}
\bm{z}^{A}_t = E_A(\bm{v}_{t-N:t}^{A}), \quad
\bm{z}^{F}_t = E_F(\bm{v}_{t-N:t}^{F}; \bm{z}^{A}_t)
\end{equation}
其中$\bm{z}^{A}_t \in \mathbb{R}^{128}$，$\bm{z}^{F}_t  \in \mathbb{R}^{32}$。
这两个编码器负责提取低层次语音节奏与表情动态信息，为后续模态融合提供稳定上下文表征。

此外，系统在此基础上引入头部姿态模态$\bm{v}_t^{H}$，用于补充空间方向与节奏信号。
其编码器$E_H$将Rot6D表示的头部旋转向量映射为紧凑潜在表征：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}_{t-N:t}^{H}; \bm{z}^{A}_t, \bm{z}^{F}_t),
\end{equation}
编码器结构将在第~\ref{sec:head_encoder}~节详细说明。

\subsubsection{输出模态继承（身体姿态解码）}

在输入模态经过编码与融合后，模型需将多模态特征映射至对应的身体姿态空间。
为实现层次化的动作生成与结构协调，本文将上半身的输出区域划分为两个互补分支：
躯干(Torso, T)与上肢(Upper limbs, U)。
躯干部分包含脊椎的三个主要控制关节，用于确定身体的姿态基准与运动节奏；
上肢部分包含双臂及手部关节，负责生成与语音节奏及情绪表达相呼应的细节动作。
最终的上半身姿态表示为两者的组合：
\begin{equation}
\bm{v}^B = \bm{v}^T \otimes \bm{v}^U,
\end{equation}
其中$\otimes$表示通道维度拼接操作。

该分层设计继承了CaMN的层次预测思路：
模型首先生成相对稳定的躯干姿态以确定整体方向，
再以此为条件预测上肢动作，从而在实时生成中保持整体协调性与自然度。

具体而言，来自语音、面部与头部编码器的特征
$\bm{z}_i^{A}$、$\bm{z}_i^{F}$、$\bm{z}_i^{H}$会与历史姿态序列$(\bm{v}_{i-N}^{B}, \ldots, \bm{v}_i^{B})$拼接，组成多模态隐向量：
\begin{equation}
\bm{z}_i^{M} = \bm{z}_i^{A} \otimes \bm{z}_i^{F} \otimes \bm{z}_i^{H} \otimes (\bm{v}_{i-N}^{B}, \ldots, \bm{v}_i^{B}),
\end{equation}
其中$\otimes$表示通道维度拼接操作，时间末帧采用零填充以对齐维度。

随后，$\{\bm{z}_0^{M}, \ldots, \bm{z}_N^{M}\}$经两个单向 LSTM 解码器，分别生成躯干与上肢的潜在特征：
\begin{equation}
\bm{z}^{T} = \mathrm{LSTM}_{T}(\bm{z}_0^{M}, \ldots, \bm{z}_N^{M}), \quad
\bm{z}^{U} = \mathrm{LSTM}_{U}(\bm{z}_0^{M}, \ldots, \bm{z}_N^{M}),
\end{equation}
并通过独立的 MLP 模块还原为旋转参数：
\begin{equation}
\hat{\bm{v}}^{T} = \mathrm{MLP}_{T}(\bm{z}^{T}), \quad
\hat{\bm{v}}^{U} = \mathrm{MLP}_{U}(\bm{z}^{U}).
\end{equation}
最终拼接得到当前帧的完整上半身姿态：
\begin{equation}
\hat{\bm{v}}^{B} = \hat{\bm{v}}^{T} \otimes \hat{\bm{v}}^{U}.
\end{equation}

在推理阶段，解码器隐状态在时间步之间保持连续，
与前述输入模态特征配合，使模型在保持因果性的同时具备自然的时间平滑性。
由于该部分结构沿用自基线模型，本文不再赘述。
接下来，将介绍时间建模结构的改动及其对实时性的适配。

\subsubsection{时间建模结构改动与因果性约束}

基线模型CaMN使用双向LSTM生成完整序列的骨骼姿态，输入与输出片段长度一致。
由于双向结构在每个时间步都依赖未来帧隐状态，虽然能增强整体平滑性，但不满足实时生成场景的因果约束。
为实现严格的实时性，本文将时间建模模块改为单向LSTM，
使模型在每一时间步仅依赖过去$N$帧的输入并预测当前帧的骨骼姿态。
虽然单向LSTM结构上仍会输出与输入片段等长的时间序列，但训练时仅计算其最后一帧的预测误差：

\begin{equation}
\hat{\bm{v}}_{t} = \mathrm{LSTM}(\bm{z}_{t-N:t}^{M})_{N}, \quad
\mathcal{L}_{\text{causal}} = \|\hat{\bm{v}}_t - \bm{v}_t\|_2^2,
\end{equation}

其中，下标$N$表示取LSTM输出序列的最后一帧作为当前时刻的预测结果。

该策略通过在前$N$帧内累积隐状态，于第$N+1$帧完成当前姿态预测，从而建立严格的因果时序映射。

在推理阶段，FaceCapGes采用长度为$N$的显式输入窗口，并在时间步之间保留LSTM的隐状态。
虽然单向LSTM理论上能够仅通过递推隐状态存储历史信息，
但由于隐状态为压缩形式，难以完全保留短时节奏与相位特征。
因此，显式窗口输入与隐状态记忆在模型中形成互补：
前者提供局部的高分辨率上下文，
后者维持全局的时序连贯性。
这种设计在保证因果性的前提下提高了生成的稳定性与自然性，
也是实现实时语音驱动动作生成的关键因素之一。

图~\ref{fig_lstmcompare}展示了双向与单向结构的差异：单向结构仅依赖历史帧输入，更适合在流式序列中逐帧输出预测结果。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.9\linewidth]{figures/Fig_lstmcompare.png}
\caption{双向与单向 LSTM 对比示意图。
双向结构（上）在每个时间步同时利用历史与未来帧特征进行建模；
单向结构（下）仅基于历史帧进行递推，以保持因果性并支持流式推理。}
\label{fig_lstmcompare}
\end{figure}

\subsubsection{滑动窗口式自回归训练}

训练时，每段输入为$N+M$帧，前$N$帧作为输入上下文，后$M$帧逐步预测（见图~\ref{fig2_}）。
其中，前$N$帧的历史姿态可视为模型的因果历史窗口，
其设计思路与基线模型中附加片段首部的“历史缓冲”类似，
但在意义上有所不同。
CaMN在双向时间建模下使用该缓冲以补足片段外部上下文，
而FaceCapGes则将其重新定义为实时预测所需的前序姿态帧数量，
即当前帧推理所依赖的显式时间上下文。

在训练阶段，模型采用纯自回归（pure autoregressive）方式展开，
即在每一步预测后，将自身生成的历史帧作为下一步输入，
而非采用教师强制（teacher forcing）。
这种方式使模型在优化过程中暴露于自身预测的分布，
保持训练与推理过程的一致性，
避免了教师强制常见的暴露偏差（exposure bias），
即推理阶段模型面对自身生成数据时性能下降的问题。
在每个窗口内连续预测$M$步后，
累计所有预测帧的误差并计算平均损失：
\begin{equation}
\mathcal{L}_{\text{total}} = \frac{1}{M}\sum_{i=1}^{M} \|\hat{\bm{g}}_i - \bm{g}_i\|_2^2.
\end{equation}

虽然该纯自回归方式在训练早期的收敛速度略慢，
但生成稳定性更高，可有效抑制长期序列中的误差积累。
此外，这种机制特别契合实时虚拟人等持续交互场景：
模型在此类应用中并非针对短语段离线生成，
而是与语音流持续同步、长时间运行。
在这种场景下，纯自回归训练使模型在遇到自身预测误差时能够动态修正节奏，
从而在长时间交互中保持自然的动作连续性与节奏稳定性。

每一预测步中定义一个长度为$N+1$的滑动窗口，
其中前$N$帧为输入，第$N+1$帧为预测目标。
形式化定义如下：
\begin{align}
\bm{g}^H_i &= (\bm{g}_{i-N}, ..., \bm{g}_{\min(N, i-1)}) \otimes (\hat{\bm{g}}_{\max(N+1, i-N)}, ..., \hat{\bm{g}}_{i-1}), \\
\hat{\bm{g}}_i &= FaceCapGes(\bm{v}_{i-N}, ..., \bm{v}_i; \bm{g}^H_i),
\end{align}
其中$\otimes$表示时间拼接操作。
该机制在每步仅依赖过去信息，从而保持因果性约束；
同时通过窗口内的滚动更新，在不引入未来帧的前提下实现平滑过渡。
推理阶段模型以单帧为输入流，输出当前时刻的上半身姿态，实现端到端低延迟生成。

需要指出的是，由于滑动窗口机制依赖前$N$帧的上下文信息，
模型在序列开端无法立即生成动作，即存在一个短暂的“冷启动”阶段。
然而在本文的目标应用场景——实时语音驱动虚拟人系统——中，
模型作为常驻进程持续运行，而非针对离散语句反复初始化。
因此该延迟仅在首次启动时出现$N$帧的等待，
对用户体验影响可忽略。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.75\linewidth]{figures/Fig2.png}
\caption{滑动窗口训练机制：模型通过自回归循环预测$M$帧，每步使用$N$帧上下文并预测第$N+1$帧。损失函数累计所有预测帧的误差，保证因果性与时间平滑性。}
\label{fig2_}
\end{figure}

通过以上适配，FaceCapGes在保持CaMN级联优势的同时显著降低系统延迟，
实现实时稳定的语音与面部驱动手势生成。

然而，仅依赖这两种模态仍存在动作方向与节奏响应不足的问题。
为此，下一节将在级联结构末端引入头部姿态模态，以补充时序反馈与空间方向信号。

\subsection{头部姿态模态的引入与结构位置}
\label{sec:head_encoder}

在模型结构设计中，我们考察了头部姿态特征与其他模态的多种组合方式。%
具体而言，分别尝试了：%
(1) 将头部姿态特征在编码阶段与语音或面部特征进行早期融合；%
(2) 在解码阶段以前两者的嵌入结果为条件，预测头部姿态特征作为辅助信号。%
实验结果显示，这两种交互方式均未带来显著性能提升，%
部分设置甚至出现训练收敛速度下降或动作节奏轻微错位的情况。%

这一现象与认知层面的规律相符。%
头部动作虽然与语音韵律在时间上存在同步性，%
但在认知层面并非由语音或表情直接驱动，%
也难以反向推导这些模态的动态变化。%
换言之，三者更可能属于\emph{并行协同}关系，%
共享节奏与注意机制，但不构成单向的预测链。%

基于此观察，本文在最终架构中采用弱耦合的后级输入设计：%
头部姿态特征在语音与面部特征编码完成后，%
以独立通道的形式拼接至多模态隐向量 $\bm{z}^M_t$，%
而非在编码阶段进行显式交互。%
该处理方式在保持整体结构简洁性的同时，%
仍保留头部姿态在方向、节奏及注意焦点方面的补充作用。%

实验表明，%
在此配置下模型的整体自然度与时序稳定性得到改善，%
说明头部姿态虽非语音或表情的从属模态，%
但作为\emph{空间与节奏的辅助信号}仍具有积极贡献。

\paragraph{输入特征与表示}
本文仅使用头部旋转信息，不引入平移位移特征。BEAT数据集中演讲者多为站姿，录制中存在身体移动，若直接使用位移作为输入，噪声易混入；此外，目标应用场景中的坐姿用户分布不同，直接建模位移会削弱泛化性。  
因此仅采用旋转特征，并使用连续且可微的Rot6d表示，以避免欧拉角与四元数的奇异性问题。

\paragraph{编码器结构}
图~\ref{fig_headencoder} 所示为头部姿态编码器结构。该编码器由两层前馈网络组成，输入为Rot6d表示的6维向量：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}^H_{t-N:t}; \bm{z}^A_t, \bm{z}^F_t),
\end{equation}
其中 $E_H$ 的具体形式为：
\begin{align}
\bm{h}_1 &= \mathrm{ReLU}(W_1 \bm{v}^H_t + b_1), \\
\bm{z}^{H}_t &= W_2 \bm{h}_1 + b_2,
\end{align}
网络维度设置为：输入 $6$，中间层 $36$，输出 $12$。  
在特征层面，其输出与语音、面部嵌入拼接后输入解码器，形成从语义到反应的多层信号流。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.8\linewidth]{figures/Fig_headencoder.png}
\caption{头部姿态编码器结构示意图。
输入为 Rot6d 表示的 6 维旋转向量，经两层前馈网络与 ReLU 非线性映射，
输出 12 维紧凑潜在表征。
该编码结果与语音、面部嵌入拼接后输入解码器，用于补充动作的方向与节奏信号。}
\label{fig_headencoder}
\end{figure}

\paragraph{架构位置与实验依据}
在早期实验中，我们尝试将头部姿态与语音、面部特征早期融合，但该方式导致手势方向轻微抖动、语音与动作节奏错位，收敛速度减慢。分析认为，原因在于头部姿态的反应性与非因果性：若过早参与特征交互，会破坏语音—手势因果映射。  
因此本文采用后级融合策略，在语音与面部特征编码完成后再输入头部特征。该设计使模型先形成语义骨架，再由头部姿态进行方向修正与节奏调节。

\subsection{小结：从语义驱动到反应调节的信号层级}

FaceCapGes 在 CaMN 的语音—表情级联架构基础上，通过实时适配与头部模态引入实现了信号层级的扩展。前两级模态承担语义与情感驱动，而新增头部姿态层作为反应性调节模块，在实时条件下为手势提供动态节奏与空间反馈。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/Fig1.png}
\caption{FaceCapGes 模型结构：音频、面部、头部编码器分别提取模态特征后拼接，输入至 LSTM 解码器生成躯干与手部动作，仅保留最后一帧输出作为当前时刻预测，符合帧级实时推理设定。训练阶段历史姿态序列比目标长度少一帧，需进行零填充。}
\label{fig1_}
\end{figure*}

\section{训练与损失函数设计}
\label{sec:training}

在前述滑动窗口式自回归训练框架下，每段输入序列包含 $N$ 帧上下文与 $M$ 帧预测结果。
模型在每个窗口中输出连续的动作序列 $\hat{\bm{g}} \in \mathbb{R}^{M \times 6 \times 47}$，
并以重构与对抗两类损失共同约束生成质量。

%重了？
\paragraph{手势重构损失}
重构项 $\mathcal{L}_{Gesture Rec}$ 由位置、速度与加速度误差组成，用以平衡空间准确性与时间平滑性：
\begin{equation}
\bm{g}' = \bm{g}_t - \bm{g}_{t-1}, \quad \bm{g}'' = \bm{g}'_t - \bm{g}'_{t-1},
\end{equation}
\begin{equation}
\mathcal{L}_{Gesture Rec} = \mathcal{L}_{rec}(\bm{g}, \hat{\bm{g}}) + \mathcal{L}_{vel}(\bm{g}', \hat{\bm{g}}') + \mathcal{L}_{acc}(\bm{g}'', \hat{\bm{g}}''),
\end{equation}
其中 $\mathcal{L}_{rec}$ 保证空间姿态重构精度，
$\mathcal{L}_{vel}$ 和 $\mathcal{L}_{acc}$ 强调动态平滑性与时序一致性。
该组合设计在自回归预测中能有效缓解抖动与速度漂移问题。

\paragraph{对抗损失}
为进一步提升动作的自然度，引入对抗项 $\mathcal{L}_{Adv}$：
\begin{equation}
\mathcal{L}_{Adv} = -\mathbb{E}[\log(Dis(\hat{\bm{g}}))],
\end{equation}
其中判别器 $Dis$ 以完整的动作序列为输入，
判别其是否来自真实数据分布。
该项损失约束生成序列的整体动力学分布，
促进生成动作在节奏、加速度和能量变化上与真实表演者一致。
训练中通过交替优化生成器与判别器的参数，保持两者的平衡。

\paragraph{总体损失}
综合两项目标，模型的最终训练目标为：
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{Gesture Rec} + \lambda_{adv}\mathcal{L}_{Adv},
\end{equation}
其中 $\lambda_{adv}$ 为对抗损失的权重（实验中设为 $0.1$）。
该设计在保持运动学准确性的同时，提升了时序的自然性与节奏感。

\section{实现与训练配置}
\label{sec:implementation}

在上述训练目标下，FaceCapGes 基于 PyTorch 实现，
所有实验在单张 NVIDIA RTX 4090 GPU 上进行。

本文基于 BEAT 数据集\cite{beatcamn} 进行训练与评估。
该数据集包含多模态同步的语音、面部 blendshape 与全身动作信息，
以 15\,fps 记录多位专业表演者的演讲片段，覆盖多种语义与情绪场景。
其标准骨架结构如图~\ref{fig_beatbones} 所示，
共包含 47 个关节节点。
FaceCapGes 仅预测其中的上半身部分，
包括上肢及躯干的三个主要控制点（蓝色区域所示），
以聚焦语音驱动手势中的表达性动作。
下肢关节保持静态以保证骨架一致性。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.15\textwidth]{figures/Fig_BEATBones.png}
\caption{
BEAT 数据集的骨架拓扑结构。
蓝色部分为 FaceCapGes 模型的控制区域，
涵盖上肢与三段脊椎关节，其余节点保持静态。
}
\label{fig_beatbones}
\end{figure}

本文选取表演者 ID~2、4、6、8 的数据进行训练与测试，
其中~2、4~为男性，~6、8~为女性，
确保在性别与说话风格上的分布均衡。
训练集与测试集均包含相同的表演者，
但使用不同的演讲片段，
在预处理阶段已进行严格划分以避免片段交叉。

\paragraph{训练配置}
\label{train_param_setting}
训练时输入窗口的前序帧数设为 $N=16$，预测长度为 $M=34$，训练片段的切割步长为 10 帧。
相邻片段因此存在部分重叠，从而在保证充分上下文信息的同时提升数据覆盖率与时间连续性。
优化器采用 Adam\cite{adam2017}，学习率设为 $2\times10^{-4}$，$\beta_1=0.9, \beta_2=0.999$。
批大小设为 256。
为防止早期训练阶段的不稳定，对抗项在第 10 epoch 后引入，整体训练共 374 个 epoch。
在损失计算中，前 $N$ 帧的历史窗口仅作为因果上下文输入，不参与重构与对抗项的误差回传。
损失函数采用第~\ref{sec:loss} 节所述的复合重构与对抗目标。

\paragraph{姿态表示}
所有身体动作均转换为连续可微的 Rot6d 表示，
使用 EMAGE\cite{emage} 中的实现方法，
以避免欧拉角奇异性与四元数的符号不确定性。

\paragraph{运行性能}
在实时推理阶段，FaceCapGes 能以 15\,FPS 的速度驱动虚拟角色，
满足实时语音交互应用的延迟要求。

\section{本章小结}
本章系统介绍了 FaceCapGes 模型的总体设计与关键技术细节。  
首先，从系统整体出发，阐述了实时语音驱动虚拟人生成管线的总体框架，  
说明了模型在输入采集、手势生成与渲染驱动中的定位与功能。
针对基线模型 CaMN 的结构特征，本文在保持多模态级联优势的基础上，  
对时间建模与实时适配机制进行了系统性改进：  
移除了说话人 ID 输入分支，采用独立模型对应不同说话风格；  
引入单向 LSTM 与滑动窗口式自回归训练以保证因果性与流式生成能力。  

在模型的模态设计上，本文重点分析了语音与面部模态的继承机制，  
并在级联架构末端加入头部姿态编码器，以补充方向性与节奏反馈信号。  
同时，结合对抗优化与多级重构损失，构建了兼顾空间准确性与时间自然性的训练目标。  
实验部分将进一步验证这些结构设计对实时性、平滑性与自然度的提升效果。  
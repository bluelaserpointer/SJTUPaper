% !TEX root = ../main.tex

\chapter{方法}

\section{研究定位与总体设计思路}
手势可根据语义依赖性与时间结构复杂度区分为可语义生成与可韵律生成两大类。
本文的研究聚焦于严格实时的语音驱动任务，在此条件下模型无法访问未来语音或完整语义，
因此重点生成与语音韵律同步的节奏型手势，
并通过面部与头部模态的联合输入进一步增强表达性与自然度。

基于上述定位，本文提出了一种基于音频、面部BlendShape权重和头部姿态输入的
帧级多模态级联手势生成模型FaceCapGes。
模型旨在在实时条件下实现自然、同步且具有一定指向性的上半身动作生成，
在不依赖语义理解或未来上下文的前提下，
通过多模态输入弥补语音模态预测能力的不足。

为避免与系统级说明混淆，本文在第~\ref{sec:system}~节给出端到端系统框架与各模块的功能划分；
在第~\ref{sec:problem}~节形式化定义任务目标、输入输出模态与符号体系；
在第~\ref{sec:cascade}~节详细介绍模型的级联结构、模态编码方式；
在第~\ref{sec:realtime}~节详细介绍滑动窗口、自回归训练等实时适配策略；
在第~\ref{sec:head_encoder}~节介绍头部姿态的引入方法与编码器；
在第~\ref{sec:architecture}~节展示模型的整体结构图；
最后在第~\ref{sec:implementation}~节说明实现细节与训练配置。

\section{系统整体框架与模块定位}
\label{sec:system}
本节介绍整个系统的端到端驱动流程及模块职责划分。如图~\ref{fig:system_architecture}所示，系统整体架构由五个层级组成：用户配置层、设备层、中间件层、手势生成模型层以及渲染与驱动层。各层之间通过多模态信号接口进行连接，实现从信号采集到虚拟人动作生成的端到端实时处理。

FaceCapGes模型位于中间层，承担多模态输入到上半身姿态输出的核心推理任务，而输入采集与渲染模块分别负责信号获取与结果展示。

为实现基于语音、面部捕捉与头部姿态的实时数字人驱动系统，本文构建了完整的信号采集、动作生成与渲染展示的处理管线。FaceCapGes模型作为该系统的核心计算模块，负责在实时约束下从多模态输入推理出当前帧的上半身骨骼姿态。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/SystemArchitecture.png}
\bicaption{系统整体架构与数据流示意图}{System Overview and Data Flow Diagram}
\label{fig:system_architecture}
\end{figure*}

\subsection{信号采集与系统配置层}
该层位于系统整体架构的输入端，用于从用户端设备实时获取多模态信号，并在系统初始化阶段完成运行参数的配置。整体结构可划分为设备层、中间件层与用户配置层三个部分，如图~\ref{fig:system_architecture} 所示。

\paragraph{设备层}
设备层负责采集语音与视觉模态信号。语音信号由麦克风实时录制，采样率与帧移可根据运行设备性能调整；视觉信号由前置深度相机摄像头获取面部深度图与视频流，并作为ARKit面部追踪模块的输入。

\paragraph{中间件层}
中间件层通过Apple提供的ARKit框架\cite{ARKitDocumentation}，将设备层的原始图像流与深度图转化为结构化特征。ARKit输出两类主要数据：  
(1) \textbf{面部表情特征}
ARKit提供52维BlendShape系数向量，用于描述关键肌肉群的局部形变状态。该特征能够反映用户的表情、口型与情感变化，并以帧级形式同步输出。  
(2) \textbf{头部姿态特征} 
ARKit在ARFaceAnchor中提供一个齐次变换矩阵$\mathbf{T}\in\mathbb{R}^{4\times4}$，
用于描述人脸锚点相对会话世界坐标系的位姿：
\begin{equation}
\mathbf{T} =
\begin{bmatrix}
\mathbf{R} & \mathbf{t} \\
\mathbf{0}^{\top} & 1
\end{bmatrix},
\quad
\mathbf{R}\in\mathbb{R}^{3\times3},\ 
\mathbf{t}\in\mathbb{R}^{3\times1}.
\end{equation}
其中左上角的$\mathbf{R}$为旋转矩阵，右上角的$\mathbf{t}$为平移向量。
本研究从矩阵中提取旋转部分 $\mathbf{R}$，
并将其转换为Rot6D~\cite{rot6d}表示形式，
以提升旋转空间的连续性与模型训练的稳定性。

同时，音频流在中间件层中被传入特征提取模块以生成时间序列特征。模型训练阶段使用Librosa库离线提取Mel频谱、短时能量与基频$F_0$等声学特征，以保证特征精度与一致性。系统运行阶段可由等价的实时特征提取模块（如torchaudio或TensorFlow Audio）逐帧生成对应特征，以实现端到端的低延迟运行。

\paragraph{用户配置层}
用户配置层负责系统初始化阶段的模型与参数设定。用户可在应用中选择说话风格，对应加载不同说话人ID配置下的模型权重。该配置仅在系统启动时生效，不参与实时推理过程。

本层提供的多模态信号经中间件处理后，以统一的数据接口传递至手势生成模型，实现语音、表情与头部姿态的实时融合输入。

\subsection{手势生成模型层（FaceCapGes）}

FaceCapGes模块位于系统的中间层，是本文提出的核心计算单元。该模块接收来自信号采集与系统配置层的三类输入特征：语音特征、面部BlendShape系数以及头部姿态参数，并在不依赖未来帧的条件下，逐帧预测用户当前时刻的上半身骨骼姿态。

生成的骨骼姿态采用Rot6D\cite{rot6d}连续旋转表示形式，覆盖上半身47个关节的旋转参数。模型内部通过级联多模态编码结构提取时序相关特征，并利用单向LSTM解码器完成时间依赖建模，从而在保持实时性的同时，生成与语音节奏、表情变化及头部朝向高度一致的自然手势。

FaceCapGes输出的姿态数据通过统一接口传递至渲染与驱动模块，与实时面部捕捉信号共同驱动虚拟角色的整体动作。由于模型仅依赖当前与历史帧输入，可与输入层以固定帧率并行运行，实现端到端的低延迟推理。

\subsection{渲染驱动层}

该模块位于系统输出端，负责将手势生成模型与面部捕捉结果共同转化为虚拟人的实时动作表现。
系统将FaceCapGes模型输出的上半身骨骼姿态与ARKit实时检测的52维面部BlendShape系数传递至渲染引擎，
由引擎内的模块解析并映射至目标虚拟人的骨骼与表情控制接口，从而实现多模态动作驱动。

渲染模块采用基于GPU的蒙皮计算与实时光照模型，以确保动画的平滑性和视觉一致性。
最终，系统能够在实时流式输入条件下稳定运行，
同步呈现语音、表情与身体动作，
以自然流畅的数字人形象实现从多模态信号输入到可视化输出的完整驱动流程。

\section{问题定义}
\label{sec:problem}

在整体系统中，FaceCapGes模块承担着从多模态输入信号到上半身骨骼姿态预测的核心任务。
为了明确模型的输入输出结构与学习目标，本节对该问题进行形式化定义。

\subsection{任务描述}

目标是在实时条件下，根据用户当前时刻的语音、面部表情与头部姿态信息，预测其对应的上半身骨骼姿态。模型需能够逐帧生成与语音节奏、面部动态和头部转动方向相协调的自然手势动作，而不依赖未来的输入帧或整句语音信息。

形式上，可以将该任务定义为一个多模态时序映射函数：
\begin{equation}
\hat{\bm{v}}_{t}^{B} = f_\theta\!\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big),
\end{equation}
其中$f_\theta$表示由参数$\theta$控制的生成模型，$N$为历史窗口长度。
%
各模态输入定义如下：
$\bm{v}_{t}^{A}$表示语音模态在时刻$t$的特征向量，由麦克风信号经特征提取模块得到；
$\bm{v}_{t}^{F}$表示面部模态的输入，为ARKit输出的52维标准化BlendShape系数；
$\bm{v}_{t}^{H}$表示头部模态的输入，为ARKit得出的头部旋转矩阵经Rot6D表示；
而$\hat{\bm{v}}_{t}^{B}$为生成模型在当前时刻预测的上半身骨骼姿态向量。
%
模型仅利用当前及过去$N$帧的输入信息估计$\hat{\bm{v}}_{t}^{B}$，
从而满足严格的实时推理约束。

\subsection{输入与输出模态}

FaceCapGes模型的输入由三种可同时实时获取的模态组成：语音特征、面部BlendShape权重及头部姿态参数；输出为当前帧的上半身骨骼旋转状态。各模态的符号与维度如表~\ref{tab:modalities}所示。

\begin{table}[h]
\bicaption{输入输出模态符号与维度}{Notations and Dimensions of Input/Output Modalities}
\centering
\label{tab:modalities}
\begin{tabular}{llll}
\toprule
\textbf{模态} & \textbf{符号} & \textbf{维度} & \textbf{描述} \\
\midrule
语音特征 & $\bm{v}_t^{A}$ & $\mathbb{R}^{1067}$ & 由音频信号提取的时序特征（Mel频谱、短时能量、基频等） \\
面部 BlendShape & $\bm{v}_t^{F}$ & $\mathbb{R}^{52}$ & ARKit输出的标准化表情权重向量 \\
头部姿态 & $\bm{v}_t^{H}$ & $\mathbb{R}^{6}$ & 采用Rot6D表示的头部旋转参数 \\
骨骼姿态（输出） & $\hat{\bm{v}}_t^{B}$ & $\mathbb{R}^{6 \times 47}$ & 上半身47个关节的旋转状态 \\
\bottomrule
\end{tabular}
\end{table}

输入序列$\big(\bm{v}_{t-N:t}^{A},\, \bm{v}_{t-N:t}^{F},\, \bm{v}_{t-N:t}^{H}\big)$描述了用户在过去$N$帧内的语音与表情动态信息。模型通过学习其时序变化规律，逐帧生成对应的骨骼姿态输出$\hat{\bm{v}}_t^{B}$。在推理阶段，模型仅访问至时刻$t$的输入序列，无法访问任何未来帧信息，保证了生成过程的因果性与实时性。

\subsection{学习目标与优化形式}
\label{sec:loss}
在训练阶段，给定来自多模态语音动作数据集（如 BEAT）的配对样本：
\begin{equation}
\big(\bm{v}_t^{A},\, \bm{v}_t^{F},\, \bm{v}_t^{H},\, \bm{v}_t^{B}\big),
\end{equation}
模型的学习目标是在不依赖未来帧的条件下，
最小化预测姿态 $\hat{\bm{v}}_t^{B}$ 与真实姿态 $\bm{v}_t^{B}$ 之间的差异，
从而生成自然、流畅且与语音节奏相匹配的上半身动作序列。

假设模型在每个时间窗口中输出连续的 $M$ 帧预测结果，
得到预测动作序列
$\hat{\bm{g}} \in \mathbb{R}^{M \times 6 \times 47}$。
综合考虑空间重构精度、时序平滑性以及动作分布一致性，
总体优化目标定义为：
\begin{equation}
\mathcal{L}_{total} =
\lambda_r  \mathcal{L}_{rec}
+ \lambda_v \mathcal{L}_{vel}
+ \lambda_a \mathcal{L}_{acc}
+ \lambda_{adv} \mathcal{L}_{adv}.
\label{eq:loss_total}
\end{equation}

其中 $\mathcal{L}_{rec}$ 衡量单帧姿态重构误差，
$\mathcal{L}_{vel}$ 与 $\mathcal{L}_{acc}$ 分别约束速度与加速度的连续性,
$\mathcal{L}_{adv}$ 表示对抗训练损失，将在第~\ref{sec:adv}中介绍。
该组合设计在自回归预测过程中能够有效缓解抖动与速度漂移问题。

\paragraph{姿态重构与时序平滑损失}
为同时保证空间重构精度与时间连续性，
我们采用基于 Huber 误差的重构损失形式，
并分别作用于姿态、速度与加速度信号。

给定任意预测序列 $\hat{\bm{x}}$ 及其对应的真实序列 $\bm{x}$，
基础误差项定义为：
\begin{equation}
\mathcal{L}_{Huber}(\bm{x}, \hat{\bm{x}})
=
\beta \cdot
\mathrm{SmoothL1}
\left(
\frac{\bm{x}}{\beta},
\frac{\hat{\bm{x}}}{\beta}
\right),
\end{equation}
其中 $\mathrm{SmoothL1}(\cdot)$ 表示平滑L1误差，$\beta$为平滑系数，本文中设为 $0.1$。

在此基础上，
姿态、速度与加速度重构损失分别定义为：
\begin{align}
\mathcal{L}_{rec} &= \mathcal{L}_{Huber}(\bm{g}, \hat{\bm{g}}), \\
\mathcal{L}_{vel} &= \mathcal{L}_{Huber}(\bm{g}', \hat{\bm{g}}'), \\
\mathcal{L}_{acc} &= \mathcal{L}_{Huber}(\bm{g}'', \hat{\bm{g}}''),
\end{align}
其中一阶与二阶时间差分 $\bm{g}'$、$\bm{g}''$ 定义为：
\begin{equation}
\bm{g}'_t = \bm{g}_t - \bm{g}_{t-1}, \quad
\bm{g}''_t = \bm{g}'_t - \bm{g}'_{t-1}.
\end{equation}

该多尺度重构约束在自回归预测过程中
能够有效缓解高频抖动与速度漂移问题，
在保证运动学精度的同时提升生成序列的时间稳定性。

\paragraph{对抗损失}
\label{sec:adv}
为进一步提升生成动作的自然度，
引入基于判别器的对抗损失：
\begin{equation}
\mathcal{L}_{adv} = -\mathbb{E}[\log(Dis(\hat{\bm{g}}))],
\end{equation}
其中判别器 $Dis$ 以完整动作序列为输入，
判断其是否来自真实数据分布。
该损失从整体动力学分布层面约束生成结果，
促进生成动作在节奏、加速度与能量变化等统计特性上
与真实表演者保持一致。
训练过程中通过交替优化生成器与判别器参数以维持稳定性。

各损失项的权重系数在实验中设定为
$\lambda_r = 5\times10^{2}$，
$\lambda_v = 10^{3}$，
$\lambda_a = 10^{3}$，
$\lambda_{adv} = 10^{-1}$。

\section{级联架构设计的继承}
\label{sec:cascade}

\subsection{级联架构的原理与理论背景}

现有语音驱动手势生成模型多采用多模态融合结构，其中以CaMN\cite{beatcamn}为代表的级联架构在设计理念上具有代表性。其核心思想是将语音、面部表情与身体动作视为语义表达的不同层级：语音模态承担语义与节奏驱动作用，面部模态反映情感与意图，身体动作则是语言与情绪的外化呈现。CaMN 采用自上而下的处理顺序，即依次对语音、面部和动作模态进行建模，从而以层次化结构保持模态间的语义依存关系。

这种设计符合人类交流中“语言、表情、动作”一体化的认知规律\cite{mcneill_1992_hand, kendon_2004_gesture}。语音先规划语义与节奏，面部表情作为情绪强化信号随后产生，最终通过身体动作完成完整的非语言表达。模型中，语音编码器输出的时间嵌入被输入至面部编码器，再与面部特征融合后驱动动作解码器，从而保持语义一致性并增强表现力。

然而，CaMN的原始设计面向离线整句生成任务，需要访问未来上下文以维持全局连贯性。在实时场景下，这种依赖将引入显著延迟并破坏因果性。FaceCapGes在继承其层次思想的同时，对输入模式、训练方式与模态选择进行了系统性重构，以满足帧级实时约束。

\subsection{说话人ID分支移除}
如图~\ref{fig:system_architecture}所示，用户配置层会设置说话人ID配置用于模型切换，但该模态在本模型中不属于网络输入。
在基线模型 CaMN 中，输入模态包含显式的说话人ID向量，用于在同一模型内区分不同演讲者的风格差异。
然而在实时交互场景下，该分支并非必要：用户身份通常固定，且说话风格的变化频率远低于帧级推理速度。
因此，FaceCapGes移除了ID输入分支，采用针对每个说话人独立训练模型参数的方法。
实验表明，该方式能在保持收敛稳定的同时提升动作的自然性与节奏一致性。
从系统使用角度看，不同模型可视为说话风格配置，用户仅在需要时切换对应参数，该操作发生频率低，不会影响实时推理性能。

\subsection{输入模态继承}
语音特征通过时间卷积网络（Temporal Convolutional Network, TCN）和多层感知机（Multilayer Perceptron, MLP）编码，以捕捉短时节奏模式；面部模态采用相似结构，并在中间层融合语音嵌入，从而增强语音与表情之间的语义关联。  
语音编码器$E_A$与面部编码器$E_F$的输出定义为：
\begin{equation}
\bm{z}^{A}_t = E_A(\bm{v}_{t-N:t}^{A}), \quad
\bm{z}^{F}_t = E_F(\bm{v}_{t-N:t}^{F}; \bm{z}^{A}_t)
\end{equation}
其中$\bm{z}^{A}_t \in \mathbb{R}^{128}$，$\bm{z}^{F}_t  \in \mathbb{R}^{32}$。
这两个编码器负责提取低层次语音节奏与表情动态信息，为后续模态融合提供稳定上下文表征。

此外，系统在此基础上引入头部姿态模态$\bm{v}_t^{H}$，用于补充空间方向与节奏信号。
其编码器$E_H$将Rot6D表示的头部旋转向量映射为紧凑潜在表征：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}_{t-N:t}^{H}),
\end{equation}
编码器结构将在第~\ref{sec:head_encoder}~节详细说明。

\subsection{输出模态继承（身体姿态解码）}

在输入模态经过编码与融合后，模型需将多模态特征映射至对应的身体姿态空间。
为实现层次化的动作生成与结构协调，本文将上半身的输出区域划分为两个互补分支：
躯干（Torso, T）与上肢（Upper limbs, U）。
躯干部分包含脊椎的三个主要控制关节，用于确定身体的姿态基准与运动节奏；
上肢部分包含双臂及手部关节，负责生成与语音节奏及情绪表达相呼应的细节动作。
最终的上半身姿态表示为两者的组合：
\begin{equation}
\bm{v}^B = \bm{v}^T \otimes \bm{v}^U,
\end{equation}
其中$\otimes$表示通道维度拼接操作。

该分层设计继承了CaMN的层次预测思路：
模型首先生成相对稳定的躯干姿态以确定整体方向，
再以此为条件预测上肢动作，从而在实时生成中保持整体协调性与自然度。

具体而言，来自语音、面部与头部编码器的特征
$\bm{z}_t^{A}$、$\bm{z}_t^{F}$、$\bm{z}_t^{H}$会与历史姿态序列$(\bm{v}_{t-N}^{B}, \ldots, \bm{v}_t^{B})$拼接，组成多模态隐向量：
\begin{equation}
\bm{z}_t^{fuse} = \bm{z}_t^{A} \otimes \bm{z}_t^{F} \otimes \bm{z}_t^{H} \otimes (\bm{v}_{t-N}^{B}, \ldots, \bm{v}_t^{B}),
\end{equation}
其中$\otimes$表示通道维度拼接操作，时间末帧采用零填充以对齐维度。

随后，$\{\bm{z}_0^{fuse}, \ldots, \bm{z}_N^{fuse}\}$经两个单向 LSTM 解码器，分别生成躯干与上肢的潜在特征：
\begin{equation}
\bm{z}^{T} = \mathrm{LSTM}_{T}(\bm{z}_0^{fuse}, \ldots, \bm{z}_N^{fuse}), \quad
\bm{z}^{U} = \mathrm{LSTM}_{U}(\bm{z}_0^{fuse}, \ldots, \bm{z}_N^{fuse}),
\end{equation}
并通过独立的 MLP 模块还原为旋转参数：
\begin{equation}
\hat{\bm{v}}^{T} = \mathrm{MLP}_{T}(\bm{z}^{T}), \quad
\hat{\bm{v}}^{U} = \mathrm{MLP}_{U}(\bm{z}^{U}).
\end{equation}
最终拼接得到当前帧的完整上半身姿态：
\begin{equation}
\hat{\bm{v}}^{B} = \hat{\bm{v}}^{T} \otimes \hat{\bm{v}}^{U}.
\end{equation}

在推理阶段，解码器隐状态在时间步之间保持连续，
与前述输入模态特征配合，使模型在保持因果性的同时具备自然的时间平滑性。
由于该部分结构沿用自基线模型，本文不再赘述。

\section{因果时序建模与训练策略}
\label{sec:realtime}
\subsection{时间建模结构改动与因果性约束}

基线模型CaMN使用双向LSTM生成完整序列的骨骼姿态，输入与输出片段长度一致。
由于双向结构在每个时间步都依赖未来帧隐状态，虽然能增强整体平滑性，但不满足实时生成场景的因果约束。
为实现严格的实时性，本文将时间建模模块改为单向LSTM，
使模型在每一时间步仅依赖过去$N$帧的输入并预测当前帧的骨骼姿态。
虽然单向LSTM结构上仍会输出与输入片段等长的时间序列，
但有效输出为输出序列的最后一帧（即当前时刻$t$），
我们对此计算重构与平滑损失。
形式化地，令
\begin{equation}
\hat{\bm{v}}_{t} = \mathrm{LSTM}(\bm{z}_{t-N:t}^{fuse})_{N},
\end{equation}
其中，下标$N$表示取LSTM输出序列的最后一帧作为当前时刻的预测结果。

则前述损失函数中的姿态、速度与加速度项均以该目标帧为中心计算，
即
\begin{equation}
\mathcal{L}_{rec} = \mathcal{L}_{Huber}(\bm{v}_t,\hat{\bm{v}}_t),
\end{equation}

其余项 $\mathcal{L}_{vel},\mathcal{L}_{acc}$ 亦同理，由 $\hat{\bm{v}}_t$ 与历史帧差分得到。
最终仍采用式\ref{eq:loss_total}定义的总体目标进行优化。

该策略通过在前$N$帧内累积隐状态，完成当前姿态预测，从而建立严格的因果时序映射。

在推理阶段，FaceCapGes采用长度为$N$的显式输入窗口，并在时间步之间保留LSTM的隐状态。
虽然单向LSTM理论上能够仅通过递推隐状态存储历史信息，
但由于隐状态为压缩形式，难以完全保留短时节奏与相位特征。
因此，显式窗口输入与隐状态记忆在模型中形成互补：
前者提供局部的高分辨率上下文，
后者维持全局的时序连贯性。
这种设计在保证因果性的前提下提高了生成的稳定性与自然性，
也是实现实时语音驱动动作生成的关键因素之一。

图~\ref{fig_lstmcompare}展示了双向与单向结构的差异。
双向结构在每个时间步同时利用历史与未来帧特征进行建模；
而单向结构仅基于历史帧进行递推，以保持因果性并支持流式推理，
因此仅依赖历史帧输入，更适合在流式序列中逐帧输出预测结果。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.9\linewidth]{figures/Fig_lstmcompare.png}
\bicaption{双向与单向 LSTM 的因果性对比示意图}{Illustration of Bidirectional vs. Unidirectional LSTM under Causality Constraint}
\label{fig_lstmcompare}
\end{figure}

\subsection{滑动窗口式自回归训练}

训练时，每段输入为$N+M$帧，前$N$帧作为输入上下文，后$M$帧逐步预测（见图~\ref{fig_slideWindow}）。
其中，前$N$帧的历史姿态可视为模型的因果历史窗口，
其设计思路与基线模型中附加片段首部的“历史缓冲”类似，
但在意义上有所不同。
CaMN在双向时间建模下使用该缓冲以补足片段外部上下文，
而FaceCapGes则将其重新定义为实时预测所需的前序姿态帧数量，
即当前帧推理所依赖的显式时间上下文。

在训练阶段，模型采用纯自回归（pure autoregressive）方式展开，
即在每一步预测后，将自身生成的历史帧作为下一步输入，
而非采用教师强制（teacher forcing）。
这种方式使模型在优化过程中暴露于自身预测的分布，
保持训练与推理过程的一致性，
避免了教师强制常见的暴露偏差（exposure bias），
即推理阶段模型面对自身生成数据时性能下降的问题。
在每个窗口内连续预测 $M$ 步后，
对所有预测帧计算式\ref{eq:loss_total}定义的损失，
并在时间维度上取平均作为该窗口的优化目标：
\begin{equation}
\mathcal{L}_{window}
=
\frac{1}{M}\sum_{i=1}^{M}
\mathcal{L}_{total}^{(i)},
\end{equation}
其中 $\mathcal{L}_{total}^{(i)}$ 表示在第 $i$ 帧处依据式\ref{eq:loss_total}计算得到的总损失。

纯自回归训练使模型在遇到自身预测误差时能够动态修正节奏，
从而在长时间交互中保持自然的动作连续性与节奏稳定性。

每一预测步中定义一个长度为$N+1$的滑动窗口，
其中前$N$帧为输入，第$N+1$帧为预测目标。
形式化定义如下：
\begin{align}
\bm{g}^H_i &= (\bm{g}_{i-N}, ..., \bm{g}_{\min(N, i-1)}) \otimes (\hat{\bm{g}}_{\max(N+1, i-N)}, ..., \hat{\bm{g}}_{i-1}), \\
\hat{\bm{g}}_i &= FaceCapGes(\bm{v}_{i-N}, ..., \bm{v}_i; \bm{g}^H_i),
\end{align}
其中$\otimes$表示时间拼接操作。
该机制在每步仅依赖过去信息，从而保持因果性约束；
同时通过窗口内的滚动更新，在不引入未来帧的前提下实现平滑过渡。
推理阶段模型以单帧为输入流，输出当前时刻的上半身姿态，实现端到端低延迟生成。

需要指出的是，由于滑动窗口机制依赖前$N$帧的上下文信息，
模型在序列开端无法立即生成动作，即存在一个短暂的冷启动阶段。
然而，在本文的目标应用场景中，
模型作为常驻进程伴随用户的虚拟人交互持续运行，无需在每一句发话时切换启动状态。
因此该延迟仅在首次启动程序时出现$N$帧的等待，
对用户体验影响可基本忽略。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.75\linewidth]{figures/Fig2.png}
\bicaption{滑动窗口训练示意图}{Sliding-window Training Illustration}
\label{fig_slideWindow}
\end{figure}

通过以上适配，FaceCapGes在保持CaMN级联优势的同时显著降低系统延迟，
实现实时稳定的语音与面部驱动手势生成。

\section{头部姿态模态的引入}
\label{sec:head_encoder}

\subsection{输入特征与表示}
本文仅使用头部旋转信息作为输入特征，不引入头部平移位移。
在 BEAT 数据集中，演讲者多为站姿录制，过程中存在一定幅度的身体移动与位移变化；
而目标应用场景下的用户运动模式可能不同，例如在坐姿交互中身体位移通常较小。
由于平移分量更易受到拍摄坐标系、相机距离与场景布置等因素影响，
直接建模头部位移可能引入额外的场景依赖性，从而削弱跨场景泛化能力。
因此，本文仅采用头部旋转作为输入，并使用 Rot6D~\cite{rot6d} 进行表示，
以获得连续且无奇异性的旋转特征表达。

\subsection{特征获取方法}
如第~\ref{sec:system} 所述，头部姿态可与面部表情共同由面部捕捉工具（如 ARKit）实时提取。
对于 BEAT 数据集，其原始数据中未单独提供头部姿态信号。
因此，我们在训练预处理中利用骨架层级关系，
沿骨架链路复合从根节点到头部关节的相对旋转，
从而得到头部在全局坐标系下的绝对旋转表示，并进一步转换为 Rot6D 形式作为模型输入。

\subsection{级联结构中的位置}
在模型结构设计中，我们考察了头部姿态特征与其他模态的多种组合方式。%
具体而言，分别尝试了：%
(1) 将头部姿态特征在编码阶段与语音或面部特征进行早期融合；%
(2) 在解码阶段以前两者的嵌入结果为条件，预测头部姿态特征作为辅助信号。%
实验结果显示，这两种交互方式均未带来显著性能提升，%
部分设置甚至出现训练收敛速度下降或动作节奏轻微错位的情况。%

这一现象与认知层面的规律相符。%
头部动作虽然与语音韵律在时间上存在同步性，%
但在认知层面并非由语音或表情直接驱动，%
也难以反向推导这些模态的动态变化。%
换言之，三者更可能属于并行协同关系，%
共享节奏与注意机制，但不构成单向的预测链。%

基于此观察，本文在最终架构中采用弱耦合的后级输入设计：%
头部姿态特征在语音与面部特征编码完成后，%
以独立通道的形式拼接至多模态隐向量 $\bm{z}^{fuse}_t$，%
而非在编码阶段进行显式交互。%
该处理方式在保持整体结构简洁性的同时，%
仍保留头部姿态在方向、节奏及注意焦点方面的补充作用。%

实验表明，%
在此配置下模型的整体自然度与时序稳定性得到改善，%
说明头部姿态虽非语音或表情的从属模态，%
但作为空间与节奏的辅助信号仍具有积极贡献。

\paragraph{编码器结构}
图~\ref{fig_headencoder} 所示为头部姿态编码器结构。该编码器由两层前馈网络组成，输入为Rot6D表示的6维向量：
\begin{equation}
\bm{z}^{H}_t = E_H(\bm{v}^H_{t-N:t}),
\end{equation}
其中 $E_H$ 的具体形式为：
\begin{align}
\bm{h}_1 &= \mathrm{ReLU}(W_1 \bm{v}^H_t + b_1), \\
\bm{z}^{H}_t &= W_2 \bm{h}_1 + b_2,
\end{align}
网络维度设置为：输入 $6$，中间层 $36$，输出 $12$。  
在特征层面，其输出与语音、面部嵌入拼接后输入解码器，形成从语义到反应的多层信号流。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.8\linewidth]{figures/Fig_headencoder.png}
\bicaption{头部姿态编码器结构示意图}{Architecture of the Head Pose Encoder}
\label{fig_headencoder}
\end{figure}

\section{模型整体结构}
\label{sec:architecture}
图~\ref{fig_architecture}展示了FaceCapGes从音频、面部、头部编码器分别提取模态特征后拼接，输入至 LSTM 解码器生成躯干与手部动作的过程。
其中，LSTM的输出仅保留最后一帧作为当前时刻预测，符合帧级实时推理设定。
实际训练中，训练阶段历史姿态序列比目标长度少一帧，需进行零填充进行对齐。

\begin{figure*}[h!t]
\centering
\includegraphics[width=\textwidth]{figures/Fig_NeuroArchitecture.png}
\bicaption{FaceCapGes 模型整体结构}{Overall Architecture of FaceCapGes}
\label{fig_architecture}
\end{figure*}

\section{实现与训练配置}
\label{sec:implementation}

本文模型FaceCapGes基于PyTorch实现，
所有实验在单张 NVIDIA RTX 4090 GPU 上进行。

本文基于 BEAT 数据集\cite{beatcamn} 进行训练与评估。
该数据集包含多模态同步的语音、面部 blendshape 与全身动作信息，
以 15\,fps 记录多位专业表演者的演讲片段，覆盖多种语义与情绪场景。
其标准骨架结构如图~\ref{fig_beatbones} 所示，
BEAT 数据集共包含人体中的 47 个关节节点，
包括上肢及躯干的三个主要控制点（蓝色区域所示）。
下肢关节则保持静态。

\begin{figure}[h!t]
\centering
\includegraphics[width=0.15\textwidth]{figures/Fig_BEATBones.png}
\bicaption{BEAT 数据集的骨架拓扑结构与驱动范围}{Skeleton Topology and Actuated Joint Range in the BEAT Dataset}
\label{fig_beatbones}
\end{figure}

本文选取表演者 ID~2、4、6、8 的数据进行训练与测试，
其中~2、4~为男性，~6、8~为女性，
确保在性别与说话风格上的分布均衡。
训练集与测试集均包含相同的表演者，
但使用不同的演讲片段，
在预处理阶段已进行严格划分以避免片段交叉。

\paragraph{训练配置}
\label{train_param_setting}
训练时输入窗口的前序帧数设为 $N=16$，预测长度为 $M=34$，训练片段的切割步长为 10 帧。
相邻片段因此存在部分重叠，从而在保证充分上下文信息的同时提升数据覆盖率与时间连续性。
批大小设为 256。

优化器采用随机梯度下降（Stochastic Gradient Descent, SGD）\cite{SGDBottou2010LargeScaleML}。
基础学习率设置为 $\mathrm{lr}_{base}=2.5\times10^{-4}$，
并根据批大小按线性规则缩放为
\begin{equation}
\mathrm{lr}_{g} = \mathrm{lr}_{base}\cdot \frac{\mathrm{batch\_size}}{128},
\end{equation}
其中 $\mathrm{lr}_{g}$ 为生成器（手势生成网络）的实际学习率。

在对抗训练阶段，判别器使用相同类型的 SGD 优化器，
其学习率按权重系数 $w_d$ 缩放为
\begin{equation}
\mathrm{lr}_{d} = w_d \cdot \mathrm{lr}_{g},
\end{equation}
本文中设定 $w_d = 0.2$。

为防止早期训练阶段的不稳定，对抗项在第 10 个 epoch 后引入，
即前 10 个 epoch 仅优化重构与时序平滑损失，
从第 11 个 epoch 起加入判别器并交替优化生成器与判别器参数。
整体训练共 374 个 epoch。

在损失计算中，前 $N$ 帧的历史窗口仅作为因果上下文输入，不参与重构与对抗项的误差回传。
损失函数采用第~\ref{sec:loss} 节所述的复合重构与对抗目标。

\paragraph{姿态表示}
所有身体动作均转换为连续可微的 Rot6D 表示，
使用 EMAGE\cite{emage} 中的实现方法，
以避免欧拉角奇异性与四元数的符号不确定性。

\paragraph{运行性能}
在实时推理阶段，FaceCapGes 能以 15\,FPS 的速度驱动虚拟角色，
满足实时语音交互应用的延迟要求。

\section{本章小结}

本章围绕 FaceCapGes 的模型结构与训练策略，系统介绍了面向实时交互场景的语音驱动手势生成方法设计。
首先，本文在任务设定上明确了严格的因果约束与逐帧在线生成要求，
区别于依赖未来上下文的一次性离线生成方法，
为后续模型结构与训练策略的选择奠定了基础。

在模型设计方面，本文基于 CaMN 的级联框架，引入语音、面部表情与头部姿态三种模态的协同建模方式，
并详细说明了各模态在网络中的编码位置与作用。
其中，本文将头部姿态作为独立输入模态系统性地引入实时手势生成任务，
仅使用旋转信息，以提升跨场景泛化能力与时序连续性。
同时，针对不同数据来源，给出了从面捕系统与骨架数据中获取头部姿态特征的统一处理方法。

在时序建模与训练策略上，
本文采用滑动窗口的帧级自回归训练方式，
使模型在训练阶段即暴露于自身历史预测分布，
从而保证训练目标与在线推理过程的一致性。
该设计在不依赖未来信息的前提下，
增强了生成动作在时间维度上的连贯性与稳定性，
为实时交互场景中的持续动作生成提供了有效支持。

通过上述模型结构与训练策略的设计，
FaceCapGes 在满足低延迟与因果约束的同时，
为后续章节中的主观评估、定量分析与性能测试提供了清晰的方法基础。